<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js ayu">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Polars - User Guide</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="tabbed-code-blocks.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="quickstart/intro.html"><strong aria-hidden="true">2.</strong> Getting started</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="quickstart/quick-exploration-guide.html"><strong aria-hidden="true">2.1.</strong> Polars quick exploration guide</a></li></ol></li><li class="chapter-item expanded "><a href="dsl/intro.html"><strong aria-hidden="true">3.</strong> Polars expressions</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="dsl/expressions.html"><strong aria-hidden="true">3.1.</strong> Expressions</a></li><li class="chapter-item "><a href="dsl/contexts.html"><strong aria-hidden="true">3.2.</strong> Contexts</a></li><li class="chapter-item "><a href="dsl/groupby.html"><strong aria-hidden="true">3.3.</strong> GroupBy</a></li><li class="chapter-item "><a href="dsl/folds.html"><strong aria-hidden="true">3.4.</strong> Folds</a></li><li class="chapter-item "><a href="dsl/window_functions.html"><strong aria-hidden="true">3.5.</strong> Window functions</a></li><li class="chapter-item "><a href="dsl/list_context.html"><strong aria-hidden="true">3.6.</strong> List context and row-wise compute</a></li><li class="chapter-item "><a href="dsl/numpy.html"><strong aria-hidden="true">3.7.</strong> Numpy universal functions</a></li><li class="chapter-item "><a href="dsl/custom_functions.html"><strong aria-hidden="true">3.8.</strong> Custom functions</a></li><li class="chapter-item "><a href="notebooks/introduction_polars-py.html"><strong aria-hidden="true">3.9.</strong> Python Examples</a></li><li class="chapter-item "><a href="notebooks/introduction_polars-rs.html"><strong aria-hidden="true">3.10.</strong> Rust Examples</a></li><li class="chapter-item "><a href="dsl/api.html"><strong aria-hidden="true">3.11.</strong> API</a></li><li class="chapter-item "><a href="dsl/video_intro.html"><strong aria-hidden="true">3.12.</strong> Video introduction</a></li></ol></li><li class="chapter-item expanded "><a href="lazy-api/intro.html"><strong aria-hidden="true">4.</strong> Lazy API</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="lazy-api/lazy-query-create.html"><strong aria-hidden="true">4.1.</strong> Using the lazy API</a></li><li class="chapter-item "><a href="lazy-api/lazy-schema.html"><strong aria-hidden="true">4.2.</strong> Schema in the lazy API</a></li><li class="chapter-item "><a href="lazy-api/lazy-query-plan.html"><strong aria-hidden="true">4.3.</strong> Understanding the query plan</a></li><li class="chapter-item "><a href="lazy-api/lazy-query-execution.html"><strong aria-hidden="true">4.4.</strong> Executing lazy queries</a></li><li class="chapter-item "><a href="lazy-api/streaming.html"><strong aria-hidden="true">4.5.</strong> Streaming larger-than-memory datasets</a></li></ol></li><li class="chapter-item expanded "><a href="datatypes.html"><strong aria-hidden="true">5.</strong> Data Types</a></li><li class="chapter-item expanded "><a href="coming_from_pandas.html"><strong aria-hidden="true">6.</strong> Coming from Pandas</a></li><li class="chapter-item expanded "><a href="coming_from_spark.html"><strong aria-hidden="true">7.</strong> Coming from Apache Spark</a></li><li class="chapter-item expanded "><a href="sql.html"><strong aria-hidden="true">8.</strong> Polars SQL</a></li><li class="chapter-item expanded "><a href="howcani/intro.html"><strong aria-hidden="true">9.</strong> How can I?</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/io/intro.html"><strong aria-hidden="true">9.1.</strong> IO</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/io/csv.html"><strong aria-hidden="true">9.1.1.</strong> CSV files</a></li><li class="chapter-item "><a href="howcani/io/parquet.html"><strong aria-hidden="true">9.1.2.</strong> Parquet files</a></li><li class="chapter-item "><a href="howcani/io/json.html"><strong aria-hidden="true">9.1.3.</strong> JSON files</a></li><li class="chapter-item "><a href="multiple_files/intro.html"><strong aria-hidden="true">9.1.4.</strong> Multiple files</a></li><li class="chapter-item "><a href="howcani/io/read_db.html"><strong aria-hidden="true">9.1.5.</strong> Read from a database</a></li><li class="chapter-item "><a href="howcani/io/aws.html"><strong aria-hidden="true">9.1.6.</strong> Interact with AWS</a></li><li class="chapter-item "><a href="howcani/io/google-big-query.html"><strong aria-hidden="true">9.1.7.</strong> Interact with Google BigQuery</a></li><li class="chapter-item "><a href="howcani/io/postgres.html"><strong aria-hidden="true">9.1.8.</strong> Interact with Postgres</a></li><li class="chapter-item "><a href="howcani/interop/intro.html"><strong aria-hidden="true">9.1.9.</strong> Interoperability</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/interop/arrow.html"><strong aria-hidden="true">9.1.9.1.</strong> Arrow</a></li><li class="chapter-item "><a href="howcani/interop/numpy.html"><strong aria-hidden="true">9.1.9.2.</strong> NumPy</a></li></ol></li></ol></li><li class="chapter-item "><a href="howcani/selecting_data/selecting_data_intro.html"><strong aria-hidden="true">9.2.</strong> Selecting data</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/selecting_data/selecting_data_expressions.html"><strong aria-hidden="true">9.2.1.</strong> Selecting with expressions</a></li><li class="chapter-item "><a href="howcani/selecting_data/selecting_data_indexing.html"><strong aria-hidden="true">9.2.2.</strong> Selecting with indexing</a></li></ol></li><li class="chapter-item "><a href="howcani/data/intro.html"><strong aria-hidden="true">9.3.</strong> Data handling</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/data/strings.html"><strong aria-hidden="true">9.3.1.</strong> Process strings</a></li><li class="chapter-item "><a href="howcani/data/timestamps.html"><strong aria-hidden="true">9.3.2.</strong> Process timestamps</a></li><li class="chapter-item "><a href="howcani/missing_data.html"><strong aria-hidden="true">9.3.3.</strong> Process missing data</a></li></ol></li><li class="chapter-item "><a href="howcani/timeseries/intro.html"><strong aria-hidden="true">9.4.</strong> Time-series</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/timeseries/parsing_dates_times.html"><strong aria-hidden="true">9.4.1.</strong> Parsing dates and times</a></li><li class="chapter-item "><a href="howcani/timeseries/selecting_dates.html"><strong aria-hidden="true">9.4.2.</strong> Filtering by dates</a></li><li class="chapter-item "><a href="howcani/timeseries/temporal_groupby.html"><strong aria-hidden="true">9.4.3.</strong> Fixed and rolling temporal groupby</a></li><li class="chapter-item "><a href="howcani/timeseries/resampling.html"><strong aria-hidden="true">9.4.4.</strong> Resampling</a></li><li class="chapter-item "><a href="howcani/timeseries/time_zones.html"><strong aria-hidden="true">9.4.5.</strong> Time zones</a></li></ol></li><li class="chapter-item "><a href="howcani/combining_data/intro.html"><strong aria-hidden="true">9.5.</strong> Combining data</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="howcani/combining_data/concatenating.html"><strong aria-hidden="true">9.5.1.</strong> Concatenation</a></li><li class="chapter-item "><a href="howcani/combining_data/joining.html"><strong aria-hidden="true">9.5.2.</strong> Joining</a></li></ol></li><li class="chapter-item "><a href="howcani/multiprocessing.html"><strong aria-hidden="true">9.6.</strong> Multiprocessing</a></li></ol></li><li class="chapter-item expanded "><a href="performance/intro.html"><strong aria-hidden="true">10.</strong> Performance</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="performance/strings.html"><strong aria-hidden="true">10.1.</strong> Strings</a></li></ol></li><li class="chapter-item expanded "><a href="optimizations/intro.html"><strong aria-hidden="true">11.</strong> Optimizations</a></li><li class="chapter-item expanded "><a href="testing/schema.html"><strong aria-hidden="true">12.</strong> Testing</a></li><li class="chapter-item expanded "><a href="references.html"><strong aria-hidden="true">13.</strong> Reference guides</a></li><li class="chapter-item expanded "><a href="contributing.html"><strong aria-hidden="true">14.</strong> Contributing</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu (default)</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Polars - User Guide</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="margin: 30px auto; background-color: white; border-radius: 50%; width: 200px; height: 200px;"><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars-logo-dark.svg" alt="Polars logo" style="width: 168px; height: 168px; padding: 10px 20px;"></div>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This book is an introduction to the
<a href="https://github.com/pola-rs/polars"><code>Polars</code> DataFrame library</a>. Its goal is to
introduce you to <code>Polars</code> by going through examples and comparing it to other
solutions. Some design choices are introduced here. The guide will also introduce you to
optimal usage of <code>Polars</code>.</p>
<p>Even though <code>Polars</code> is completely written in <a href="https://www.rust-lang.org/"><code>Rust</code></a> (no
runtime overhead!) and uses <a href="https://arrow.apache.org/"><code>Arrow</code></a> -- the
<a href="https://github.com/jorgecarleitao/arrow2">native arrow2 <code>Rust</code> implementation</a> -- as its foundation, the
examples presented in this guide will be mostly using its higher-level language
bindings. Higher-level bindings only serve as a thin wrapper for functionality implemented in the core library.</p>
<p>For <a href="https://pandas.pydata.org/"><code>Pandas</code></a> users, our
<a href="https://pypi.org/project/polars/">Python package</a> will offer the easiest way to get started with
<code>Polars</code>.</p>
<h2 id="goals-and-non-goals"><a class="header" href="#goals-and-non-goals">Goals and non-goals</a></h2>
<p>The goal of <code>Polars</code> is to provide a lightning fast <code>DataFrame</code> library that:</p>
<ul>
<li>Utilizes all available cores on your machine.</li>
<li>Optimizes queries to reduce unneeded work/memory allocations.</li>
<li>Handles datasets much larger than your available RAM.</li>
<li>Has an API that is consistent and predictable.</li>
<li>Has a strict schema (data-types should be known before running the query).</li>
</ul>
<p>Polars is written in Rust which gives it C/C++ performance and allows it to fully control performance critical parts
in a query engine.</p>
<p>As such <code>Polars</code> goes to great lengths to:</p>
<ul>
<li>Reduce redundant copies.</li>
<li>Traverse memory cache efficiently.</li>
<li>Minimize contention in parallelism.</li>
<li>Process data in chunks.</li>
<li>Reuse memory allocations.</li>
</ul>
<p>Polars also has control over IO, allowing it to save redundant copies and to push down projections and predicates to
the scan level.</p>
<p>Unlike tools such as dask -- which tries to parallelize existing single-threaded libraries
like <code>NumPy</code> and <code>Pandas</code> --<code>Polars</code> is written from the ground up, designed for parallelization of queries on <code>DataFrame</code>s.</p>
<p><code>Polars</code> is lazy and semi-lazy. It allows you to do most of your work eagerly, similar to <code>Pandas</code>, but
it also provides a powerful expression syntax that will be optimized and executed on within the query engine.</p>
<p>In lazy <code>Polars</code> we are able to do query optimization on the entire query, further improving performance and memory pressure.</p>
<p><code>Polars</code> keeps track of your query in a <em>logical plan</em>. This
plan is optimized and reordered before running it. When a result is requested, <code>Polars</code>
distributes the available work to different <em>executors</em> that use the algorithms available
in the eager API to produce a result. Because the whole query context is known to
the optimizer and executors of the logical plan, processes dependent on separate data
sources can be parallelized on the fly.</p>
<h3 id="performance-"><a class="header" href="#performance-">Performance 🚀🚀</a></h3>
<p><code>Polars</code> is very fast, and in fact is one of the best performing solutions available.
See the results in h2oai's db-benchmark. The image below shows the biggest datasets yielding a result.</p>
<p><img src="https://www.ritchievink.com/img/post-35-polars-0.15/db-benchmark.png" alt="" /></p>
<p><code>Polars</code> <a href="https://www.pola.rs/benchmarks.html">TPCH Benchmark results</a> are now available on the official website.</p>
<h3 id="current-status"><a class="header" href="#current-status">Current status</a></h3>
<p>Below a concise list of the features allowing <code>Polars</code> to meet its goals:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Copy-on-write">Copy-on-write</a> (COW) semantics
<ul>
<li>&quot;Free&quot; clones</li>
<li>Cheap appends</li>
</ul>
</li>
<li>Appending without clones</li>
<li>Column oriented data storage
<ul>
<li>No block manager (i.e. predictable performance)</li>
</ul>
</li>
<li>Missing values indicated with bitmask
<ul>
<li>NaN are different from missing</li>
<li>Bitmask optimizations</li>
</ul>
</li>
<li>Efficient algorithms</li>
<li>Very fast IO
<ul>
<li>Its csv and parquet readers are among the fastest in existence</li>
</ul>
</li>
<li>Out of Core
<ul>
<li>Many queries can be executed completely out of core
(meaning that we can process datasets that are larger than RAM)</li>
<li>Arrow/IPC files can be memory mapped (this is the strategy vaex uses)</li>
</ul>
</li>
<li><a href="optimizations/lazy/intro.html">Query optimizations</a>
<ul>
<li>Predicate pushdown
<ul>
<li>Filtering at scan level</li>
</ul>
</li>
<li>Projection pushdown
<ul>
<li>Projection at scan level</li>
</ul>
</li>
<li>Aggregate pushdown
<ul>
<li>Aggregations at scan level</li>
</ul>
</li>
<li>Simplify expressions</li>
<li>Scan sharing</li>
<li>Common subplan elimination</li>
<li>Parallel execution of physical plan</li>
<li>Cardinality based groupby dispatch
<ul>
<li>Different groupby strategies based on data cardinality</li>
</ul>
</li>
</ul>
</li>
<li>SIMD vectorization</li>
<li><a href="https://numpy.org/doc/stable/reference/ufuncs.html"><code>NumPy</code> universal functions</a></li>
</ul>
<h3 id="comparison-with-other-tools"><a class="header" href="#comparison-with-other-tools">Comparison with other tools</a></h3>
<p>These are some tools that share similar functionality to what polars does.</p>
<ul>
<li>
<p>Pandas</p>
<ul>
<li>A very versatile tool for small data. Read <a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">10 things I hate about pandas</a>
written by the author himself. Polars has solved all those 10 things.
Polars is a versatile tool for small and large data with a more predictable API, less ambiguous and stricter API.</li>
</ul>
</li>
<li>
<p>Pandas the API</p>
<ul>
<li>The API of pandas was designed for in memory data. This makes it a poor fit for performant analysis on large data
(read anything that does not fit into RAM). Any tool that tries to distribute that API will likely have a
suboptimal query plan compared to plans that follow from a declarative API like SQL or polars' API.</li>
</ul>
</li>
<li>
<p>Dask</p>
<ul>
<li>Parallelizes existing single-threaded libraries like <code>NumPy</code> and <code>Pandas</code>. As a consumer of those libraries Dask
therefore has less control over low level performance and semantics.
Those libraries are treated like a black box.
On a single machine the parallelization effort can also be seriously stalled by pandas strings.
Pandas strings, by default, are stored as python objects in
numpy arrays meaning that any operation on them is GIL bound and therefore single threaded. This can be circumvented
by multi-processing but has a non-trivial cost.</li>
</ul>
</li>
<li>
<p>Modin</p>
<ul>
<li>Similar to Dask</li>
</ul>
</li>
<li>
<p>Vaex</p>
<ul>
<li>Vaexs method of out-of-core analysis is memory mapping files. This works until it doesn't. For instance parquet
or csv files first need to be read and converted to a file format that can be memory mapped. Another downside is
that the OS determines when pages will be swapped. Operations that need a full data shuffle, such as
sorts cannot benefit from memory mapping. At the moment of writing vaex relies on pyarrow for sorts, meaning that the data must fit into memory.</li>
<li>Polars' out of core processing is not based on memory mapping, but on streaming data in batches (and spilling to disk
if needed), we control which data must be hold in memory, not the OS, meaning that we don't have unexpected IO stalls.</li>
</ul>
</li>
<li>
<p>DuckDB</p>
<ul>
<li>Polars and DuckDB have many similarities. DuckDB is focussed on providing an in-process OLAP Sqlite alternative,
polars is focussed on providing a scalable <code>DataFrame</code> interface to many languages. Those different front-ends lead to
different optimization strategies and different algorithm prioritization. The interop between both is zero-copy.
See more: https://duckdb.org/docs/guides/python/polars</li>
</ul>
</li>
<li>
<p>Spark</p>
<ul>
<li>Spark is designed for distributed workloads and uses the JVM. The setup for spark is complicated and the startup-time
is slow. Polars has much better performance characteristics on a single machine. The API's are somewhat similar.</li>
</ul>
</li>
<li>
<p>CuDF</p>
<ul>
<li>GPU's are fast, but not readily available and expensive in production. The amount of memory available on GPU often
is a fraction of available RAM. Next to that Polars is close in <a href="https://zakopilo.hatenablog.jp/entry/2023/02/04/220552">performance to CuDF</a> and on some operations even faster.
CuDF also doesn't optimize your query, so it is likely that on ETL jobs polars will be faster because it can elide
unneeded work and materialization's.</li>
</ul>
</li>
<li>
<p>Any</p>
<ul>
<li>Polars is written in Rust. This gives it strong safety, performance and concurrency guarantees.
Polars is written in a modular manner. Parts of polars can be used in other query program and can be added as a library.</li>
</ul>
</li>
</ul>
<h2 id="acknowledgements"><a class="header" href="#acknowledgements">Acknowledgements</a></h2>
<p>Development of <code>Polars</code> is proudly powered by</p>
<p><a href="https://www.xomnia.com"><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/sponsors/xomnia.png" alt="Xomnia" /></a></p>
<h1 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h1>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>Installing and using <code>Polars</code> is just a simple <code>pip install</code>, <code>cargo add</code>, or <code>yarn add</code> away.</p>
<div class="tabbed-blocks">
<pre><code class="language-python"># Installing for python
$ pip install polars
</code></pre>
<pre><code class="language-rust noplayground">// Installing into a Rust project
$ cargo add polars
</code></pre>
<pre><code class="language-js">// Installing for Node
$ yarn add nodejs-polars
</code></pre>
</div>
<p>All binaries are pre-built for <code>Python</code> v3.7+.</p>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick start</a></h2>
<p>Below we show a simple snippet that parses a CSV file, filters it, and finishes with a
groupby operation.  This example is presented in python only, as the &quot;eager&quot; API is not the preferred model in Rust.</p>
<pre><code class="language-python">import polars as pl

df = pl.read_csv(&quot;https://j.mp/iriscsv&quot;)
print(df.filter(pl.col(&quot;sepal_length&quot;) &gt; 5)
      .groupby(&quot;species&quot;, maintain_order=True)
      .agg(pl.all().sum())
)
</code></pre>
<p>The snippet above will output:</p>
<pre><code class="language-text">shape: (3, 5)
┌────────────┬──────────────┬─────────────┬──────────────┬─────────────┐
│ species    ┆ sepal_length ┆ sepal_width ┆ petal_length ┆ petal_width │
│ ---        ┆ ---          ┆ ---         ┆ ---          ┆ ---         │
│ str        ┆ f64          ┆ f64         ┆ f64          ┆ f64         │
╞════════════╪══════════════╪═════════════╪══════════════╪═════════════╡
│ setosa     ┆ 116.9        ┆ 81.7        ┆ 33.2         ┆ 6.1         │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ versicolor ┆ 281.9        ┆ 131.8       ┆ 202.9        ┆ 63.3        │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ virginica  ┆ 324.5        ┆ 146.2       ┆ 273.1        ┆ 99.6        │
└────────────┴──────────────┴─────────────┴──────────────┴─────────────┘
</code></pre>
<p>As we can see, <code>Polars</code> pretty-prints the output object, including the column name and
datatype as headers.</p>
<h2 id="lazy-quick-start"><a class="header" href="#lazy-quick-start">Lazy quick start</a></h2>
<p>If we want to run this query in <code>lazy Polars</code> we'd write:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

print(
    pl.read_csv(&quot;https://j.mp/iriscsv&quot;)
    .lazy()
    .filter(pl.col(&quot;sepal_length&quot;) &gt; 5)
    .groupby(&quot;species&quot;, maintain_order=True)
    .agg(pl.all().sum())
    .collect()
)
</code></pre>
<pre><code class="language-rust noplayground">use color_eyre::{Result};
use polars::prelude::*;
use reqwest::blocking::Client;
use std::io::Cursor;

fn main() -&gt; Result&lt;()&gt; { 
    let data: Vec&lt;u8&gt; = Client::new()
        .get(&quot;https://j.mp/iriscsv&quot;)
        .send()?
        .text()?
        .bytes()
        .collect();

    let df = CsvReader::new(Cursor::new(data))
        .has_header(true)
        .finish()?
        .lazy()
        .filter(col(&quot;sepal_length&quot;).gt(5))
        .groupby([col(&quot;species&quot;)])
        .agg([col(&quot;*&quot;).sum()])
        .collect()?;

    println!(&quot;{:?}&quot;, df);

    Ok(())
}
</code></pre>
</div>
<p>When the data is stored locally, we can also use <code>scan_csv</code> in Python, or <code>LazyCsvReader</code> in Rust to run the query in lazy polars.</p>
<h3 id="note-about-rust-usage"><a class="header" href="#note-about-rust-usage">Note about Rust usage</a></h3>
<p>Some functionality is not enabled by default. It must be added as an additional feature. This can be enabled by directly adding it to your <code>Cargo.toml</code></p>
<pre><code class="language-toml">[dependencies]
polars = { version = &quot;0.24.3&quot;, features = [&quot;lazy&quot;] }
reqwest =  { version = &quot;0.11.12&quot;, features = [&quot;blocking&quot;] }
color-eyre = &quot;0.6&quot;
</code></pre>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>If you want to dive right into the <code>Python</code> API docs, check the <a href="https://pola-rs.github.io/polars/py-polars/html/reference">the reference docs</a>.  Alternatively, the <code>Rust</code> API docs are available on <a href="https://docs.rs/polars/latest/polars/">docs.rs</a>.</p>
<h3 id="lazy-api"><a class="header" href="#lazy-api">Lazy API</a></h3>
<p>The lazy API builds a query plan. Nothing is executed until you explicitly ask <code>Polars</code>
to execute the query (via <code>LazyFrame.collect()</code>, or <code>LazyFrame.fetch()</code>). This provides
<code>Polars</code> with the entire context of the query, allowing optimizations and choosing the
fastest algorithm given that context.</p>
<p>Going from eager to lazy is often as simple as starting your query with <code>.lazy()</code> and ending with <code>.collect()</code>.</p>
<p>So the eager snippet above would become:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">(
    df.lazy()
    .filter(pl.col(&quot;sepal_length&quot;) &gt; 5)
    .groupby(&quot;species&quot;, maintain_order=True)
    .agg(pl.all().sum())
    .collect()
)
</code></pre>
<pre><code class="language-rust noplayground">let df = df
    .lazy()
    .filter(col(&quot;sepal_length&quot;).gt(5))
    .groupby([col(&quot;species&quot;)])
    .agg([col(&quot;*&quot; ).sum()])
    .collect()?;
</code></pre>
</div>
<h1 id="polars-quick-exploration-guide"><a class="header" href="#polars-quick-exploration-guide">Polars quick exploration guide</a></h1>
<p>This quick exploration guide is written for new users of Polars. The goal is to provide an overview of the most common functions and capabilities of the package. In this guide we will provide several examples. At the end of every part there is a link to relevant parts of the Polars Book with more information and the API reference guide.</p>
<p>In this exploration guide we will go through the follow topics:</p>
<ul>
<li><a href="quickstart/quick-exploration-guide.html#installation-and-import">Installation and Import</a></li>
<li><a href="quickstart/quick-exploration-guide.html#object-creation">Object creation</a>
<ul>
<li><a href="quickstart/quick-exploration-guide.html#from-scratch">From scratch</a></li>
<li><a href="quickstart/quick-exploration-guide.html#from-files">From files</a></li>
</ul>
</li>
<li><a href="quickstart/quick-exploration-guide.html#viewing-data">Viewing data</a></li>
<li><a href="quickstart/quick-exploration-guide.html#expressions">Expressions</a>
<ul>
<li><a href="quickstart/quick-exploration-guide.html#select-statement">Select</a></li>
<li><a href="quickstart/quick-exploration-guide.html#filter">Filter</a></li>
<li><a href="quickstart/quick-exploration-guide.html#with_columns">With_columns</a></li>
<li><a href="quickstart/quick-exploration-guide.html#groupby">Groupby</a></li>
<li><a href="quickstart/quick-exploration-guide.html#combining-operations">Combining operations</a></li>
</ul>
</li>
<li><a href="quickstart/quick-exploration-guide.html#combining-dataframes">Combining dataframes</a>
<ul>
<li><a href="quickstart/quick-exploration-guide.html#join">Join</a></li>
<li><a href="quickstart/quick-exploration-guide.html#concat">Concat</a></li>
</ul>
</li>
<li><a href="quickstart/quick-exploration-guide.html#remaining-topics">Remaining topics</a></li>
</ul>
<h2 id="installation-and-import"><a class="header" href="#installation-and-import">Installation and Import</a></h2>
<p>Install <code>Polars</code> in your (virtual) environment with the following command:</p>
<pre><code class="language-shell">pip install -U polars
</code></pre>
<p>Import <code>Polars</code> as follows:</p>
<pre><code class="language-python">import polars as pl

# to enrich the examples in this quickstart with dates
from datetime import datetime, timedelta 
# to generate data for the examples
import numpy as np 
</code></pre>
<h2 id="object-creation"><a class="header" href="#object-creation">Object creation</a></h2>
<h3 id="from-scratch"><a class="header" href="#from-scratch">From scratch</a></h3>
<p>Creating a simple <code>Series</code> or <code>Dataframe</code> is easy and very familiar to other packages.</p>
<p>You can create a <code>Series</code> in Polars by providing a <code>list</code> or a <code>tuple</code>.</p>
<pre><code class="language-python"># with a tuple
series = pl.Series(&quot;a&quot;, [1, 2, 3, 4, 5])

print(series)
</code></pre>
<pre><code>shape: (5,)
Series: 'a' [i64]
[
    1
    2
    3
    4
    5
]
</code></pre>
<pre><code class="language-python"># with a list
series = pl.Series([1, 2, 3, 4, 5])

print(series)
</code></pre>
<pre><code>shape: (5,)
Series: '' [i64]
[
    1
    2
    3
    4
    5
]
</code></pre>
<p>A <code>DataFrame</code> is created from a <code>dict</code> or a collection of <code>dicts</code>.</p>
<pre><code class="language-python">dataframe = pl.DataFrame({&quot;integer&quot;: [1, 2, 3], 
                          &quot;date&quot;: [
                              (datetime(2022, 1, 1)), 
                              (datetime(2022, 1, 2)), 
                              (datetime(2022, 1, 3))
                          ], 
                          &quot;float&quot;:[4.0, 5.0, 6.0]})

print(dataframe)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬─────────────────────┬───────┐
│ integer ┆ date                ┆ float │
│ ---     ┆ ---                 ┆ ---   │
│ i64     ┆ datetime[μs]        ┆ f64   │
╞═════════╪═════════════════════╪═══════╡
│ 1       ┆ 2022-01-01 00:00:00 ┆ 4.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2       ┆ 2022-01-02 00:00:00 ┆ 5.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3       ┆ 2022-01-03 00:00:00 ┆ 6.0   │
└─────────┴─────────────────────┴───────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to <code>Series</code> in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/series/index.html">link</a></li>
<li>Link to <code>DataFrames</code> in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/index.html">link</a></li>
</ul>
<h3 id="from-files"><a class="header" href="#from-files">From files</a></h3>
<p>In Polars we can also read files and create a <code>DataFrame</code>. In the following examples we write the output of the <code>DataFrame</code> from the previous part to a specific file type (<code>csv</code>, <code>json</code> and <code>parquet</code>). After that we will read it and print the output for inspection.</p>
<h4 id="csv"><a class="header" href="#csv">csv</a></h4>
<pre><code class="language-python">dataframe.write_csv('output.csv')
</code></pre>
<pre><code class="language-python">df_csv = pl.read_csv('output.csv')

print(df_csv)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬────────────────────────────┬───────┐
│ integer ┆ date                       ┆ float │
│ ---     ┆ ---                        ┆ ---   │
│ i64     ┆ str                        ┆ f64   │
╞═════════╪════════════════════════════╪═══════╡
│ 1       ┆ 2022-01-01T00:00:00.000000 ┆ 4.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2       ┆ 2022-01-02T00:00:00.000000 ┆ 5.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3       ┆ 2022-01-03T00:00:00.000000 ┆ 6.0   │
└─────────┴────────────────────────────┴───────┘
</code></pre>
<p>As we can see above, Polars made the datetimes a <code>string</code>. We can tell Polars to parse dates, when reading the csv, to ensure the date becomes a datetime. The example can be found below:</p>
<pre><code class="language-python">df_csv_with_dates = pl.read_csv('output.csv', parse_dates=True)

print(df_csv_with_dates)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬─────────────────────┬───────┐
│ integer ┆ date                ┆ float │
│ ---     ┆ ---                 ┆ ---   │
│ i64     ┆ datetime[μs]        ┆ f64   │
╞═════════╪═════════════════════╪═══════╡
│ 1       ┆ 2022-01-01 00:00:00 ┆ 4.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2       ┆ 2022-01-02 00:00:00 ┆ 5.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3       ┆ 2022-01-03 00:00:00 ┆ 6.0   │
└─────────┴─────────────────────┴───────┘
</code></pre>
<h4 id="json"><a class="header" href="#json">json</a></h4>
<pre><code class="language-python">dataframe.write_json('output.json')
</code></pre>
<pre><code class="language-python">df_json = pl.read_json('output.json')

print(df_json)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬─────────────────────┬───────┐
│ integer ┆ date                ┆ float │
│ ---     ┆ ---                 ┆ ---   │
│ i64     ┆ datetime[μs]        ┆ f64   │
╞═════════╪═════════════════════╪═══════╡
│ 1       ┆ 2022-01-01 00:00:00 ┆ 4.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2       ┆ 2022-01-02 00:00:00 ┆ 5.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3       ┆ 2022-01-03 00:00:00 ┆ 6.0   │
└─────────┴─────────────────────┴───────┘
</code></pre>
<h4 id="parquet"><a class="header" href="#parquet">parquet</a></h4>
<pre><code class="language-python">dataframe.write_parquet('output.parquet')
</code></pre>
<pre><code class="language-python">df_parquet = pl.read_parquet('output.parquet')

print(df_parquet)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬─────────────────────┬───────┐
│ integer ┆ date                ┆ float │
│ ---     ┆ ---                 ┆ ---   │
│ i64     ┆ datetime[μs]        ┆ f64   │
╞═════════╪═════════════════════╪═══════╡
│ 1       ┆ 2022-01-01 00:00:00 ┆ 4.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2       ┆ 2022-01-02 00:00:00 ┆ 5.0   │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3       ┆ 2022-01-03 00:00:00 ┆ 6.0   │
└─────────┴─────────────────────┴───────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Read more about <code>IO</code> in the Polars Book: <a href="quickstart/../howcani/io/intro.html">link</a></li>
<li>Link to <code>IO</code> in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/io.html">link</a></li>
</ul>
<h2 id="viewing-data"><a class="header" href="#viewing-data">Viewing data</a></h2>
<p>This part focuses on viewing data in a <code>DataFrame</code>. We first create a <code>DataFrame</code> to work with.</p>
<pre><code class="language-python">df = pl.DataFrame({&quot;a&quot;: np.arange(0, 8), 
                   &quot;b&quot;: np.random.rand(8), 
                   &quot;c&quot;: [datetime(2022, 12, 1) + timedelta(days=idx) for idx in range(8)],
                   &quot;d&quot;: [1, 2.0, np.NaN, np.NaN, 0, -5, -42, None]
                  })

print(df)
</code></pre>
<pre><code>shape: (8, 4)
┌─────┬──────────┬─────────────────────┬───────┐
│ a   ┆ b        ┆ c                   ┆ d     │
│ --- ┆ ---      ┆ ---                 ┆ ---   │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   │
╞═════╪══════════╪═════════════════════╪═══════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null  │
└─────┴──────────┴─────────────────────┴───────┘
</code></pre>
<p>The <code>head</code> function shows by default the first 5 rows of a <code>DataFrame</code>. You can specify the number of rows you want to see (e.g. <code>df.head(10)</code>).</p>
<pre><code class="language-python">df.head(5)
</code></pre>
<pre><code>shape: (5, 4)
┌─────┬──────────┬─────────────────────┬─────┐
│ a   ┆ b        ┆ c                   ┆ d   │
│ --- ┆ ---      ┆ ---                 ┆ --- │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64 │
╞═════╪══════════╪═════════════════════╪═════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0 │
└─────┴──────────┴─────────────────────┴─────┘
</code></pre>
<p>The <code>tail</code> function shows the last 5 rows of a <code>DataFrame</code>. You can also specify the number of rows you want to see, similar to <code>head</code>.</p>
<pre><code class="language-python">df.tail(5)
</code></pre>
<pre><code>shape: (5, 4)
┌─────┬──────────┬─────────────────────┬───────┐
│ a   ┆ b        ┆ c                   ┆ d     │
│ --- ┆ ---      ┆ ---                 ┆ ---   │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   │
╞═════╪══════════╪═════════════════════╪═══════╡
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null  │
└─────┴──────────┴─────────────────────┴───────┘
</code></pre>
<p>If you want to get an impression of the data of your <code>DataFrame</code>, you can also use <code>sample</code>. With <code>sample</code> you get an <em>n</em> number of random rows from the <code>DataFrame</code>.</p>
<pre><code class="language-python">df.sample(n=3)
</code></pre>
<pre><code>shape: (3, 4)
┌─────┬──────────┬─────────────────────┬──────┐
│ a   ┆ b        ┆ c                   ┆ d    │
│ --- ┆ ---      ┆ ---                 ┆ ---  │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64  │
╞═════╪══════════╪═════════════════════╪══════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null │
└─────┴──────────┴─────────────────────┴──────┘
</code></pre>
<p><code>Describe</code> returns summary statistics of your <code>DataFrame</code>. It will provide several quick statistics if possible.</p>
<pre><code class="language-python">df.describe()
</code></pre>
<pre><code>shape: (7, 5)
┌────────────┬─────────┬──────────┬────────────────────────────┬───────┐
│ describe   ┆ a       ┆ b        ┆ c                          ┆ d     │
│ ---        ┆ ---     ┆ ---      ┆ ---                        ┆ ---   │
│ str        ┆ f64     ┆ f64      ┆ str                        ┆ f64   │
╞════════════╪═════════╪══════════╪════════════════════════════╪═══════╡
│ count      ┆ 8.0     ┆ 8.0      ┆ 8                          ┆ 8.0   │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ null_count ┆ 0.0     ┆ 0.0      ┆ 0                          ┆ 1.0   │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ mean       ┆ 3.5     ┆ 0.431245 ┆ null                       ┆ NaN   │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ std        ┆ 2.44949 ┆ 0.340445 ┆ null                       ┆ NaN   │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ min        ┆ 0.0     ┆ 0.062943 ┆ 2022-12-01 00:00:00.000000 ┆ -42.0 │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ max        ┆ 7.0     ┆ 0.896408 ┆ 2022-12-08 00:00:00.000000 ┆ 2.0   │
├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ median     ┆ 3.5     ┆ 0.42741  ┆ null                       ┆ 1.0   │
└────────────┴─────────┴──────────┴────────────────────────────┴───────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to aggregations on <code>DataFrames</code> in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/aggregation.html">link</a></li>
<li>Link to descriptive <code>DataFrame</code> functions in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/descriptive.html">link</a></li>
<li>Link to <code>DataFrame</code> attributes in the Reference guide: <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/attributes.html">link</a></li>
</ul>
<h2 id="expressions"><a class="header" href="#expressions">Expressions</a></h2>
<p><code>Expressions</code> are the core strength of <code>Polars</code>. The <code>expressions</code> offer a versatile structure that solves easy queries, but is easily extended to complex analyses. Below we will cover the basic components that serve as building block for all your queries.</p>
<ul>
<li><code>select</code></li>
<li><code>filter</code></li>
<li><code>with_columns</code></li>
</ul>
<h3 id="select-statement"><a class="header" href="#select-statement">Select statement</a></h3>
<p>To select a column we need to do two things. Define the <code>DataFrame</code> we want the data from. And second, select the data that we need. In the example below you see that we select <code>col('*')</code>. The asterisk stands for all columns.</p>
<pre><code class="language-python">df.select(
    pl.col('*')
)
</code></pre>
<pre><code>shape: (8, 4)
┌─────┬──────────┬─────────────────────┬───────┐
│ a   ┆ b        ┆ c                   ┆ d     │
│ --- ┆ ---      ┆ ---                 ┆ ---   │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   │
╞═════╪══════════╪═════════════════════╪═══════╡
│ 0   ┆ 0.164545 ┆ 2022-12-01 00:00:00 ┆ 1.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 1   ┆ 0.747291 ┆ 2022-12-02 00:00:00 ┆ 2.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 2   ┆ 0.889227 ┆ 2022-12-03 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3   ┆ 0.736651 ┆ 2022-12-04 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 4   ┆ 0.099687 ┆ 2022-12-05 00:00:00 ┆ 0.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 5   ┆ 0.965809 ┆ 2022-12-06 00:00:00 ┆ -5.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 6   ┆ 0.93697  ┆ 2022-12-07 00:00:00 ┆ -42.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 7   ┆ 0.848925 ┆ 2022-12-08 00:00:00 ┆ null  │
└─────┴──────────┴─────────────────────┴───────┘
</code></pre>
<p>You can also specify the specific columns that you want to return. There are two ways to do this. The first option is to create a <code>list</code> of column names, as seen below.</p>
<pre><code class="language-python">df.select(
    pl.col(['a', 'b'])
)
</code></pre>
<pre><code>shape: (8, 2)
┌─────┬──────────┐
│ a   ┆ b        │
│ --- ┆ ---      │
│ i64 ┆ f64      │
╞═════╪══════════╡
│ 0   ┆ 0.164545 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 0.747291 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 0.889227 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 0.736651 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 0.099687 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ 0.965809 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 6   ┆ 0.93697  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 7   ┆ 0.848925 │
└─────┴──────────┘
</code></pre>
<p>The second option is to specify each column within a <code>list</code> in the <code>select</code> statement. This option is shown below.</p>
<pre><code class="language-python"># in this example we limit the number of rows returned to 3, as the comparison is clear.
# this also shows how easy we can extend our expression to what we need. 
df.select([
    pl.col('a'),
    pl.col('b')
]).limit(3)
</code></pre>
<pre><code>shape: (3, 2)
┌─────┬──────────┐
│ a   ┆ b        │
│ --- ┆ ---      │
│ i64 ┆ f64      │
╞═════╪══════════╡
│ 0   ┆ 0.164545 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 0.747291 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 0.889227 │
└─────┴──────────┘
</code></pre>
<p>If you want to exclude an entire column from your view, you can simply use <code>exclude</code> in your <code>select</code> statement.</p>
<pre><code class="language-python">df.select([
    pl.exclude('a')
])
</code></pre>
<pre><code>shape: (8, 3)
┌──────────┬─────────────────────┬───────┐
│ b        ┆ c                   ┆ d     │
│ ---      ┆ ---                 ┆ ---   │
│ f64      ┆ datetime[μs]        ┆ f64   │
╞══════════╪═════════════════════╪═══════╡
│ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0   │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0   │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN   │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 │
├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null  │
└──────────┴─────────────────────┴───────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to <code>select</code> with <code>expressions</code> in the Polars Book: <a href="quickstart/../dsl/expressions.html">link</a></li>
</ul>
<h3 id="filter"><a class="header" href="#filter">Filter</a></h3>
<p>The <code>filter</code> option allows us to create a subset of the <code>DataFrame</code>. We use the same <code>DataFrame</code> as earlier and we filter between two specified dates.</p>
<pre><code class="language-python">df.filter(
    pl.col(&quot;c&quot;).is_between(datetime(2022, 12, 2), datetime(2022, 12, 8)),
)
</code></pre>
<pre><code>shape: (5, 4)
┌─────┬──────────┬─────────────────────┬───────┐
│ a   ┆ b        ┆ c                   ┆ d     │
│ --- ┆ ---      ┆ ---                 ┆ ---   │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   │
╞═════╪══════════╪═════════════════════╪═══════╡
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 │
└─────┴──────────┴─────────────────────┴───────┘
</code></pre>
<p>With <code>filter</code> you can also create more complex filters that include multiple columns.</p>
<pre><code class="language-python">df.filter(
    (pl.col('a') &lt;= 3) &amp; (pl.col('d').is_not_nan())
)
</code></pre>
<pre><code>shape: (2, 4)
┌─────┬──────────┬─────────────────────┬─────┐
│ a   ┆ b        ┆ c                   ┆ d   │
│ --- ┆ ---      ┆ ---                 ┆ --- │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64 │
╞═════╪══════════╪═════════════════════╪═════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0 │
└─────┴──────────┴─────────────────────┴─────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to filtering in <code>expressions</code> in the Polars Book: <a href="quickstart/../dsl/expressions.html#filter-and-conditionals">link</a></li>
</ul>
<h3 id="with_columns"><a class="header" href="#with_columns">With_columns</a></h3>
<p><code>with_columns</code> allows you to create new columns for you analyses. We create two new columns <code>e</code> and <code>b+42</code>. First we sum all values from column <code>b</code> and store the results in column <code>e</code>. After that we add <code>42</code> to the values of <code>b</code>. Creating a new column <code>b+42</code> to store these results.</p>
<pre><code class="language-python">df.with_columns([
    pl.col('b').sum().alias('e'),
    (pl.col('b') + 42).alias('b+42')
])
</code></pre>
<pre><code>shape: (8, 6)
┌─────┬──────────┬─────────────────────┬───────┬──────────┬───────────┐
│ a   ┆ b        ┆ c                   ┆ d     ┆ e        ┆ b+42      │
│ --- ┆ ---      ┆ ---                 ┆ ---   ┆ ---      ┆ ---       │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   ┆ f64      ┆ f64       │
╞═════╪══════════╪═════════════════════╪═══════╪══════════╪═══════════╡
│ 0   ┆ 0.606396 ┆ 2022-12-01 00:00:00 ┆ 1.0   ┆ 4.126554 ┆ 42.606396 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 0.404966 ┆ 2022-12-02 00:00:00 ┆ 2.0   ┆ 4.126554 ┆ 42.404966 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 0.619193 ┆ 2022-12-03 00:00:00 ┆ NaN   ┆ 4.126554 ┆ 42.619193 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 0.41586  ┆ 2022-12-04 00:00:00 ┆ NaN   ┆ 4.126554 ┆ 42.41586  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 0.35721  ┆ 2022-12-05 00:00:00 ┆ 0.0   ┆ 4.126554 ┆ 42.35721  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ 0.726861 ┆ 2022-12-06 00:00:00 ┆ -5.0  ┆ 4.126554 ┆ 42.726861 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 6   ┆ 0.201782 ┆ 2022-12-07 00:00:00 ┆ -42.0 ┆ 4.126554 ┆ 42.201782 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 7   ┆ 0.794286 ┆ 2022-12-08 00:00:00 ┆ null  ┆ 4.126554 ┆ 42.794286 │
└─────┴──────────┴─────────────────────┴───────┴──────────┴───────────┘
</code></pre>
<h3 id="groupby"><a class="header" href="#groupby">Groupby</a></h3>
<p>We will create a new <code>DataFrame</code> for the Groupby functionality. This new <code>DataFrame</code> will include several 'groups' that we want to groupby.</p>
<pre><code class="language-python">df2 = pl.DataFrame({
                    &quot;x&quot;: np.arange(0, 8), 
                    &quot;y&quot;: ['A', 'A', 'A', 'B', 'B', 'C', 'X', 'X'],
})

print(df2)
</code></pre>
<pre><code>shape: (8, 2)
┌─────┬─────┐
│ x   ┆ y   │
│ --- ┆ --- │
│ i64 ┆ str │
╞═════╪═════╡
│ 0   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 3   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 4   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 5   ┆ C   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 6   ┆ X   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ 7   ┆ X   │
└─────┴─────┘
</code></pre>
<pre><code class="language-python"># without maintain_order you will get a random order back.
df2.groupby(&quot;y&quot;, maintain_order=True).count()
</code></pre>
<pre><code>shape: (4, 2)
┌─────┬───────┐
│ y   ┆ count │
│ --- ┆ ---   │
│ str ┆ u32   │
╞═════╪═══════╡
│ A   ┆ 3     │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ B   ┆ 2     │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ C   ┆ 1     │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┤
│ X   ┆ 2     │
└─────┴───────┘
</code></pre>
<pre><code class="language-python">df2.groupby(&quot;y&quot;, maintain_order=True).agg([
    pl.col(&quot;*&quot;).count().alias(&quot;count&quot;),
    pl.col(&quot;*&quot;).sum().alias(&quot;sum&quot;)
])
</code></pre>
<pre><code>shape: (4, 3)
┌─────┬───────┬─────┐
│ y   ┆ count ┆ sum │
│ --- ┆ ---   ┆ --- │
│ str ┆ u32   ┆ i64 │
╞═════╪═══════╪═════╡
│ A   ┆ 3     ┆ 3   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ B   ┆ 2     ┆ 7   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ C   ┆ 1     ┆ 5   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ X   ┆ 2     ┆ 13  │
└─────┴───────┴─────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to <code>groupby</code> with <code>expressions</code> in the Polars Book: <a href="quickstart/../dsl/groupby.html">link</a></li>
</ul>
<h3 id="combining-operations"><a class="header" href="#combining-operations">Combining operations</a></h3>
<p>Below are some examples on how to combine operations to create the <code>DataFrame</code> you require.</p>
<pre><code class="language-python"># create a new column that multiplies column `a` and `b` from our DataFrame
# select all the columns, but exclude column `c` and `d` from the final DataFrame

df_x = df.with_columns(
    (pl.col(&quot;a&quot;) * pl.col(&quot;b&quot;)).alias(&quot;a * b&quot;)
).select([
    pl.all().exclude(['c', 'd'])
])

print(df_x)
</code></pre>
<pre><code>shape: (8, 3)
┌─────┬──────────┬──────────┐
│ a   ┆ b        ┆ a * b    │
│ --- ┆ ---      ┆ ---      │
│ i64 ┆ f64      ┆ f64      │
╞═════╪══════════╪══════════╡
│ 0   ┆ 0.220182 ┆ 0.0      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 0.750839 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 1.269277 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2.022121 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 0.41127  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 4.482038 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 0.377657 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 0.756653 │
└─────┴──────────┴──────────┘
</code></pre>
<pre><code class="language-python"># only excluding column `d` in this example

df_y = df.with_columns([
    (pl.col(&quot;a&quot;) * pl.col(&quot;b&quot;)).alias(&quot;a * b&quot;)
]).select([
    pl.all().exclude('d')
])

print(df_y)
</code></pre>
<pre><code>shape: (8, 4)
┌─────┬──────────┬─────────────────────┬──────────┐
│ a   ┆ b        ┆ c                   ┆ a * b    │
│ --- ┆ ---      ┆ ---                 ┆ ---      │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64      │
╞═════╪══════════╪═════════════════════╪══════════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 0.0      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 0.750839 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ 1.269277 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ 2.022121 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.41127  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ 4.482038 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ 0.377657 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ 0.756653 │
└─────┴──────────┴─────────────────────┴──────────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to contexts in <code>expressions</code> in the Polars Book: <a href="quickstart/../dsl/contexts.html">link</a></li>
</ul>
<h2 id="combining-dataframes"><a class="header" href="#combining-dataframes">Combining dataframes</a></h2>
<h3 id="join"><a class="header" href="#join">Join</a></h3>
<p>Let's have a closer look on how to <code>join</code> two <code>DataFrames</code> to a single <code>DataFrame</code>.</p>
<pre><code class="language-python">df = pl.DataFrame({&quot;a&quot;: np.arange(0, 8), 
                   &quot;b&quot;: np.random.rand(8), 
                   &quot;c&quot;: [datetime(2022, 12, 1) + timedelta(days=idx) for idx in range(8)],
                   &quot;d&quot;: [1, 2.0, np.NaN, np.NaN, 0, -5, -42, None]
                  })

df2 = pl.DataFrame({
                    &quot;x&quot;: np.arange(0, 8), 
                    &quot;y&quot;: ['A', 'A', 'A', 'B', 'B', 'C', 'X', 'X'],
})
</code></pre>
<p>Our two <code>DataFrames</code> both have an 'id'-like column: <code>a</code> and <code>x</code>. We can use those columns to <code>join</code> the <code>DataFrames</code> in this example.</p>
<pre><code class="language-python">df.join(df2, left_on=&quot;a&quot;, right_on=&quot;x&quot;)
</code></pre>
<pre><code>shape: (8, 5)
┌─────┬──────────┬─────────────────────┬───────┬─────┐
│ a   ┆ b        ┆ c                   ┆ d     ┆ y   │
│ --- ┆ ---      ┆ ---                 ┆ ---   ┆ --- │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   ┆ str │
╞═════╪══════════╪═════════════════════╪═══════╪═════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  ┆ C   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 ┆ X   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null  ┆ X   │
└─────┴──────────┴─────────────────────┴───────┴─────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to <code>joins</code> in the Polars Book: <a href="quickstart/../howcani/combining_data/joining.html">link</a></li>
<li>More information about <code>joins</code> in the Reference guide <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.join.html#polars.DataFrame.join">link</a></li>
</ul>
<h3 id="concat"><a class="header" href="#concat">Concat</a></h3>
<p>We can also <code>concatenate</code> two <code>DataFrames</code>. Vertical concatenation will make the <code>DataFrame</code> longer. Horizontal concatenation will make the <code>DataFrame</code> wider. Below you can see the result of an horizontal concatenation of our two <code>DataFrames</code>.</p>
<pre><code class="language-python">pl.concat([df,df2], how=&quot;horizontal&quot;)
</code></pre>
<pre><code>shape: (8, 6)
┌─────┬──────────┬─────────────────────┬───────┬─────┬─────┐
│ a   ┆ b        ┆ c                   ┆ d     ┆ x   ┆ y   │
│ --- ┆ ---      ┆ ---                 ┆ ---   ┆ --- ┆ --- │
│ i64 ┆ f64      ┆ datetime[μs]        ┆ f64   ┆ i64 ┆ str │
╞═════╪══════════╪═════════════════════╪═══════╪═════╪═════╡
│ 0   ┆ 0.220182 ┆ 2022-12-01 00:00:00 ┆ 1.0   ┆ 0   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ 0.750839 ┆ 2022-12-02 00:00:00 ┆ 2.0   ┆ 1   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ 0.634639 ┆ 2022-12-03 00:00:00 ┆ NaN   ┆ 2   ┆ A   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 3   ┆ 0.67404  ┆ 2022-12-04 00:00:00 ┆ NaN   ┆ 3   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 4   ┆ 0.102818 ┆ 2022-12-05 00:00:00 ┆ 0.0   ┆ 4   ┆ B   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 5   ┆ 0.896408 ┆ 2022-12-06 00:00:00 ┆ -5.0  ┆ 5   ┆ C   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 6   ┆ 0.062943 ┆ 2022-12-07 00:00:00 ┆ -42.0 ┆ 6   ┆ X   │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 7   ┆ 0.108093 ┆ 2022-12-08 00:00:00 ┆ null  ┆ 7   ┆ X   │
└─────┴──────────┴─────────────────────┴───────┴─────┴─────┘
</code></pre>
<p>Additional information</p>
<ul>
<li>Link to <code>concatenation</code> in the Polars Book: <a href="quickstart/../howcani/combining_data/concatenating.html">link</a></li>
<li>More information about <code>concatenation</code> in the Reference guide <a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.concat.html#polars.concat">link</a></li>
</ul>
<h2 id="remaining-topics"><a class="header" href="#remaining-topics">Remaining topics</a></h2>
<p>This guide was a quick introduction to some of the most used functions within <code>Polars</code>. There is a lot more to explore, both in the Polars Book as in the Reference guide. Below are other common topics including a link to find more information about them.</p>
<ul>
<li>Dealing with timeseries <a href="quickstart/../howcani/timeseries/intro.html">link</a></li>
<li>Processing missing data <a href="quickstart/../howcani/missing_data.html">link</a></li>
<li>Reading data from Pandas DataFrame or Numpy array <a href="https://pola-rs.github.io/polars/py-polars/html/reference/functions.html#conversion">link</a></li>
<li>Working with the Lazy API <a href="quickstart/../optimizations/">link</a></li>
</ul>
<h1 id="polars-expressions"><a class="header" href="#polars-expressions">Polars Expressions</a></h1>
<p><code>Polars</code> has a powerful concept called expressions that is central to its very fast performance.</p>
<p>Expressions are at the core of many data science operations:</p>
<ul>
<li>taking a sample of rows from a column</li>
<li>multiplying values in a column</li>
<li>extracting a column of years from dates</li>
<li>convert a column of strings to lowercase</li>
<li>and so on!</li>
</ul>
<p>However, expressions are also used within other operations:</p>
<ul>
<li>taking the mean of a group in a <code>groupby</code> operation</li>
<li>calculating the size of groups in a <code>groupby</code> operation</li>
<li>taking the sum horizontally across columns</li>
</ul>
<p><code>Polars</code> performs these core data transformations very quickly by:</p>
<ul>
<li>automatic query optimization on each expression</li>
<li>automatic parallelization of expressions on many columns</li>
</ul>
<p>Polars expressions are a mapping from a series to a series (or mathematically <code>Fn(Series) -&gt; Series</code>). As expressions have a <code>Series</code> as an input and a <code>Series</code> as an output then it is straightforward to do a sequence of expressions (similar to method chaining in <code>Pandas</code>).</p>
<p>This has all been a bit abstract, so let's start with some examples.</p>
<h1 id="polars-expressions-1"><a class="header" href="#polars-expressions-1">Polars Expressions</a></h1>
<p>The following is an expression:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">pl.col(&quot;foo&quot;).sort().head(2)
</code></pre>
<pre><code class="language-rust noplayground">df.column(&quot;foo&quot;)?.sort(false).head(Some(2));
</code></pre>
</div>
<p>The snippet above says:</p>
<ol>
<li>Select column &quot;foo&quot;</li>
<li>Then sort the column (not in reversed order)</li>
<li>Then take the first two values of the sorted output</li>
</ol>
<p>The power of expressions is that every expression produces a new expression, and that they
can be <em>piped</em> together. You can run an expression by passing them to one of <code>Polars</code> execution contexts.</p>
<p>Here we run two expressions by running <code>df.select</code>:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df.select([
    pl.col(&quot;foo&quot;).sort().head(2),
    pl.col(&quot;bar&quot;).filter(pl.col(&quot;foo&quot;) == 1).sum()
])
</code></pre>
<pre><code class="language-rust noplayground">df.select([
   col(&quot;foo&quot;).sort(Default::default()).head(Some(2)),
   col(&quot;bar&quot;).filter(col(&quot;foo&quot;).eq(lit(1))).sum(),
]).collect()?;
</code></pre>
</div>
<p>All expressions are run in parallel, meaning that separate <code>Polars</code> expressions are <strong>embarrassingly parallel</strong>. Note that within an expression there may be more parallelization going on.</p>
<h2 id="expression-examples"><a class="header" href="#expression-examples">Expression examples</a></h2>
<p>In this section we will go through some examples, but first let's create a dataset:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl
import numpy as np

np.random.seed(12)

df = pl.DataFrame(
    {
        &quot;nrs&quot;: [1, 2, 3, None, 5],
        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],
        &quot;random&quot;: np.random.rand(5),
        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],
    }
)
print(df)
</code></pre>
<pre><code class="language-rust noplayground">use color_eyre::Result;
use polars::prelude::*;
use rand::{thread_rng, Rng};

fn main() -&gt; Result&lt;()&gt; {
    let mut arr = [0f64; 5];
    thread_rng().fill(&amp;mut arr);

    let df = df! (
        &quot;nrs&quot; =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],
        &quot;names&quot; =&gt; &amp;[Some(&quot;foo&quot;), Some(&quot;ham&quot;), Some(&quot;spam&quot;), Some(&quot;eggs&quot;), None],
        &quot;random&quot; =&gt; &amp;arr,
        &quot;groups&quot; =&gt; &amp;[&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],
    )?;

    println!(&quot;{}&quot;, &amp;df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 4)
┌──────┬───────┬──────────┬────────┐
│ nrs  ┆ names ┆ random   ┆ groups │
│ ---  ┆ ---   ┆ ---      ┆ ---    │
│ i64  ┆ str   ┆ f64      ┆ str    │
╞══════╪═══════╪══════════╪════════╡
│ 1    ┆ foo   ┆ 0.154163 ┆ A      │
│ 2    ┆ ham   ┆ 0.74005  ┆ A      │
│ 3    ┆ spam  ┆ 0.263315 ┆ B      │
│ null ┆ egg   ┆ 0.533739 ┆ C      │
│ 5    ┆ null  ┆ 0.014575 ┆ B      │
└──────┴───────┴──────────┴────────┘
</code></pre>
<p>You can do a lot with expressions. They are so expressive that you sometimes have
multiple ways to get the same results. To get a better feel for them let's go through some more examples.</p>
<blockquote>
<p>A note for the Rust examples:  Each of these examples use the same dataset.  So, due to Rust's ownership rules, and the fact that all the examples run in the same context, we'll <code>clone()</code> the dataset for each example to ensure that no prior example affects the behavior of later examples.  This is the case for all Rust examples for the remainder of this book.  It's worth mentioning, that clones in Polars are very efficient, and don't result in a &quot;deep copy&quot; of the data.  They're implemented using the Rust <code>Arc</code> type (Atomically Reference Counted).</p>
</blockquote>
<h3 id="count-unique-values"><a class="header" href="#count-unique-values">Count unique values</a></h3>
<p>We can count the unique values in a column. Note that we are creating the same result in
different ways. To avoid duplicate column names in the <code>DataFrame</code>, we could use an
<code>alias</code> expression that can rename the expression.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.col(&quot;names&quot;).n_unique().alias(&quot;unique_names_1&quot;),
        pl.col(&quot;names&quot;).unique().count().alias(&quot;unique_names_2&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([
            col(&quot;names&quot;).n_unique().alias(&quot;unique_names_1&quot;),
            col(&quot;names&quot;).unique().count().alias(&quot;unique_names_2&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (1, 2)
┌────────────────┬────────────────┐
│ unique_names_1 ┆ unique_names_2 │
│ ---            ┆ ---            │
│ u32            ┆ u32            │
╞════════════════╪════════════════╡
│ 5              ┆ 5              │
└────────────────┴────────────────┘
</code></pre>
<h3 id="various-aggregations"><a class="header" href="#various-aggregations">Various aggregations</a></h3>
<p>We can do various aggregations. Below are examples of some of them, but there are more such as
<code>median</code>, <code>mean</code>, <code>first</code>, etc.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.sum(&quot;random&quot;).alias(&quot;sum&quot;),
        pl.min(&quot;random&quot;).alias(&quot;min&quot;),
        pl.max(&quot;random&quot;).alias(&quot;max&quot;),
        pl.col(&quot;random&quot;).max().alias(&quot;other_max&quot;),
        pl.std(&quot;random&quot;).alias(&quot;std dev&quot;),
        pl.var(&quot;random&quot;).alias(&quot;variance&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([
            sum(&quot;random&quot;).alias(&quot;sum&quot;),
            min(&quot;random&quot;).alias(&quot;min&quot;),
            max(&quot;random&quot;).alias(&quot;max&quot;),
            col(&quot;random&quot;).max().alias(&quot;other_max&quot;),
            col(&quot;random&quot;).std(1).alias(&quot;std dev&quot;),
            col(&quot;random&quot;).var(1).alias(&quot;variance&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (1, 6)
┌──────────┬──────────┬─────────┬───────────┬──────────┬──────────┐
│ sum      ┆ min      ┆ max     ┆ other_max ┆ std dev  ┆ variance │
│ ---      ┆ ---      ┆ ---     ┆ ---       ┆ ---      ┆ ---      │
│ f64      ┆ f64      ┆ f64     ┆ f64       ┆ f64      ┆ f64      │
╞══════════╪══════════╪═════════╪═══════════╪══════════╪══════════╡
│ 1.705842 ┆ 0.014575 ┆ 0.74005 ┆ 0.74005   ┆ 0.293209 ┆ 0.085971 │
└──────────┴──────────┴─────────┴───────────┴──────────┴──────────┘
</code></pre>
<h3 id="filter-and-conditionals"><a class="header" href="#filter-and-conditionals">Filter and conditionals</a></h3>
<p>We can also do some pretty complex things. In the next snippet we count all names ending with the string <code>&quot;am&quot;</code>.</p>
<blockquote>
<p>Note that in <code>Rust</code>, the <code>strings</code> feature must be enabled for <code>str</code> expression to be available.</p>
</blockquote>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.col(&quot;names&quot;).filter(pl.col(&quot;names&quot;).str.contains(r&quot;am$&quot;)).count(),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([col(&quot;names&quot;)
            .filter(col(&quot;names&quot;).str().contains(&quot;am$&quot;))
            .count()])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (1, 1)
┌───────┐
│ names │
│ ---   │
│ u32   │
╞═══════╡
│ 2     │
└───────┘
</code></pre>
<h3 id="binary-functions-and-modification"><a class="header" href="#binary-functions-and-modification">Binary functions and modification</a></h3>
<p>In the example below we use a conditional to create a new expression in the following
<code>when -&gt; then -&gt; otherwise</code> construct. The <code>when</code> function requires a predicate
expression (and thus leads to a boolean <code>Series</code>). The <code>then</code> function expects an
expression that will be used in case the predicate evaluates to <code>true</code>, and the <code>otherwise</code>
function expects an expression that will be used in case the predicate evaluates to <code>false</code>.</p>
<p>Note that you can pass any expression, or just base expressions like <code>pl.col(&quot;foo&quot;)</code>,
<code>pl.lit(3)</code>, <code>pl.lit(&quot;bar&quot;)</code>, etc.</p>
<p>Finally, we multiply this with the result of a <code>sum</code> expression:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.when(pl.col(&quot;random&quot;) &gt; 0.5).then(0).otherwise(pl.col(&quot;random&quot;)) * pl.sum(&quot;nrs&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([when(col(&quot;random&quot;).gt(0.5)).then(0).otherwise(col(&quot;random&quot;)) * sum(&quot;nrs&quot;)])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 1)
┌──────────┐
│ literal  │
│ ---      │
│ f64      │
╞══════════╡
│ 1.695791 │
│ 0.0      │
│ 2.896465 │
│ 0.0      │
│ 0.160325 │
└──────────┘
</code></pre>
<h3 id="window-expressions"><a class="header" href="#window-expressions">Window expressions</a></h3>
<p>A polars expression can also do an implicit GROUPBY, AGGREGATION, and JOIN in a single expression.
In the examples below we do a GROUPBY OVER <code>&quot;groups&quot;</code> and AGGREGATE SUM of <code>&quot;random&quot;</code>, and in the next expression
we GROUPBY OVER <code>&quot;names&quot;</code> and AGGREGATE a LIST of <code>&quot;random&quot;</code>. These window functions can be combined with other expressions
and are an efficient way to determine group statistics. See more on those group statistics <a href="https://pola-rs.github.io/polars/py-polars/html/reference/expressions/aggregation.html">here</a>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = df.select(
    [
        pl.col(&quot;*&quot;),  # select all
        pl.col(&quot;random&quot;).sum().over(&quot;groups&quot;).alias(&quot;sum[random]/groups&quot;),
        pl.col(&quot;random&quot;).list().over(&quot;names&quot;).alias(&quot;random/name&quot;),
    ]
)
print(df)
</code></pre>
<pre><code class="language-rust noplayground">    let df = df
        .lazy()
        .select([
            col(&quot;*&quot;), // Select all
            col(&quot;random&quot;)
                .sum()
                .over([col(&quot;groups&quot;)])
                .alias(&quot;sum[random]/groups&quot;),
            col(&quot;random&quot;)
                .list()
                .over([col(&quot;names&quot;)])
                .alias(&quot;random/name&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 6)
┌──────┬───────┬──────────┬────────┬────────────────────┬─────────────┐
│ nrs  ┆ names ┆ random   ┆ groups ┆ sum[random]/groups ┆ random/name │
│ ---  ┆ ---   ┆ ---      ┆ ---    ┆ ---                ┆ ---         │
│ i64  ┆ str   ┆ f64      ┆ str    ┆ f64                ┆ list[f64]   │
╞══════╪═══════╪══════════╪════════╪════════════════════╪═════════════╡
│ 1    ┆ foo   ┆ 0.154163 ┆ A      ┆ 0.894213           ┆ [0.154163]  │
│ 2    ┆ ham   ┆ 0.74005  ┆ A      ┆ 0.894213           ┆ [0.74005]   │
│ 3    ┆ spam  ┆ 0.263315 ┆ B      ┆ 0.27789            ┆ [0.263315]  │
│ null ┆ egg   ┆ 0.533739 ┆ C      ┆ 0.533739           ┆ [0.533739]  │
│ 5    ┆ null  ┆ 0.014575 ┆ B      ┆ 0.27789            ┆ [0.014575]  │
└──────┴───────┴──────────┴────────┴────────────────────┴─────────────┘
</code></pre>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>This is the tip of the iceberg in terms of possible expressions. There are a ton more, and they
can be combined in a variety ways.</p>
<p>This page was an introduction to <code>Polars</code> expressions, and gave a glimpse of what's
possible with them. In the next page we'll discuss in which contexts expressions can be used. Later in the guide we'll go through expressions in various groupby contexts, all while keeping <code>Polars</code> execution parallel.</p>
<h1 id="expression-contexts"><a class="header" href="#expression-contexts">Expression contexts</a></h1>
<p>You cannot use an expression anywhere. An expression needs a context, the available contexts are:</p>
<ul>
<li>selection: <code>df.select([..])</code></li>
<li>groupby aggregation: <code>df.groupby(..).agg([..])</code></li>
<li>hstack/ add columns: <code>df.with_columns([..])</code></li>
</ul>
<h2 id="syntactic-sugar"><a class="header" href="#syntactic-sugar">Syntactic sugar</a></h2>
<p>The reason for such a context, is that you actually are using the Polars lazy API, even if you use it in eager.
For instance this snippet:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df.groupby(&quot;foo&quot;).agg([pl.col(&quot;bar&quot;).sum()])
</code></pre>
<pre><code class="language-rust noplayground">eager.groupby([&quot;foo&quot;])?.agg(&amp;[(&quot;bar&quot;, &amp;[&quot;sum&quot;])])?;
</code></pre>
</div>
<p>actually desugars to:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">(df.lazy().groupby(&quot;foo&quot;).agg([pl.col(&quot;bar&quot;).sum()])).collect()
</code></pre>
<pre><code class="language-rust noplayground">eager.lazy().groupby([&quot;foo&quot;]).agg([col(&quot;bar&quot;).sum()]).collect()?;
</code></pre>
</div>
<p>This allows Polars to push the expression into the query engine, do optimizations, and cache intermediate results.</p>
<p>Rust differs from Python somewhat in this respect.  Where Python's eager mode is little more than a thin veneer over the lazy API, Rust's eager mode is closer to an implementation detail, and isn't really recommended for end-user use.  It is possible that the eager API in Rust will be scoped private sometime in the future.  Therefore, for the remainder of this document, assume that the Rust examples are using the lazy API.</p>
<h2 id="select-context"><a class="header" href="#select-context">Select context</a></h2>
<p>In the <code>select</code> context the selection applies expressions over columns. The expressions in this context must produce <code>Series</code> that are all
the same length or have a length of <code>1</code>.</p>
<p>A <code>Series</code> of a length of <code>1</code> will be broadcasted to match the height of the <code>DataFrame</code>.
Note that a <code>select</code> may produce new columns that are aggregations, combinations of expressions, or literals.</p>
<h4 id="selection-context"><a class="header" href="#selection-context">Selection context</a></h4>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.sum(&quot;nrs&quot;),
        pl.col(&quot;names&quot;).sort(),
        pl.col(&quot;names&quot;).first().alias(&quot;first name&quot;),
        (pl.mean(&quot;nrs&quot;) * 10).alias(&quot;10xnrs&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([
            sum(&quot;nrs&quot;),
            col(&quot;names&quot;).sort(false),
            col(&quot;names&quot;).first().alias(&quot;first name&quot;),
            mean(&quot;nrs&quot;).mul(lit(10)).alias(&quot;10xnrs&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 4)
┌─────┬───────┬────────────┬────────┐
│ nrs ┆ names ┆ first name ┆ 10xnrs │
│ --- ┆ ---   ┆ ---        ┆ ---    │
│ i64 ┆ str   ┆ str        ┆ f64    │
╞═════╪═══════╪════════════╪════════╡
│ 11  ┆ null  ┆ foo        ┆ 27.5   │
│ 11  ┆ egg   ┆ foo        ┆ 27.5   │
│ 11  ┆ foo   ┆ foo        ┆ 27.5   │
│ 11  ┆ ham   ┆ foo        ┆ 27.5   │
│ 11  ┆ spam  ┆ foo        ┆ 27.5   │
└─────┴───────┴────────────┴────────┘
</code></pre>
<p><strong>Add columns</strong></p>
<p>Adding columns to a <code>DataFrame</code> using <code>with_columns</code> is also the <code>selection</code> context.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = df.with_columns(
    [
        pl.sum(&quot;nrs&quot;).alias(&quot;nrs_sum&quot;),
        pl.col(&quot;random&quot;).count().alias(&quot;count&quot;),
    ]
)
print(df)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .with_columns([
            sum(&quot;nrs&quot;).alias(&quot;nrs_sum&quot;),
            col(&quot;random&quot;).count().alias(&quot;count&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 6)
┌──────┬───────┬──────────┬────────┬─────────┬───────┐
│ nrs  ┆ names ┆ random   ┆ groups ┆ nrs_sum ┆ count │
│ ---  ┆ ---   ┆ ---      ┆ ---    ┆ ---     ┆ ---   │
│ i64  ┆ str   ┆ f64      ┆ str    ┆ i64     ┆ u32   │
╞══════╪═══════╪══════════╪════════╪═════════╪═══════╡
│ 1    ┆ foo   ┆ 0.154163 ┆ A      ┆ 11      ┆ 5     │
│ 2    ┆ ham   ┆ 0.74005  ┆ A      ┆ 11      ┆ 5     │
│ 3    ┆ spam  ┆ 0.263315 ┆ B      ┆ 11      ┆ 5     │
│ null ┆ egg   ┆ 0.533739 ┆ C      ┆ 11      ┆ 5     │
│ 5    ┆ null  ┆ 0.014575 ┆ B      ┆ 11      ┆ 5     │
└──────┴───────┴──────────┴────────┴─────────┴───────┘
</code></pre>
<h2 id="groupby-context"><a class="header" href="#groupby-context">Groupby context</a></h2>
<p>In the <code>groupby</code> context expressions work on groups and thus may yield results of any length (a group may have many members).</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.groupby(&quot;groups&quot;).agg(
    [
        pl.sum(&quot;nrs&quot;),  # sum nrs by groups
        pl.col(&quot;random&quot;).count().alias(&quot;count&quot;),  # count group members
        # sum random where name != null
        pl.col(&quot;random&quot;).filter(pl.col(&quot;names&quot;).is_not_null()).sum().suffix(&quot;_sum&quot;),
        pl.col(&quot;names&quot;).reverse().alias((&quot;reversed names&quot;)),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .lazy()
        .groupby([col(&quot;groups&quot;)])
        .agg([
            sum(&quot;nrs&quot;),                           // sum nrs by groups
            col(&quot;random&quot;).count().alias(&quot;count&quot;), // count group members
            // sum random where name != null
            col(&quot;random&quot;)
                .filter(col(&quot;names&quot;).is_not_null())
                .sum()
                .suffix(&quot;_sum&quot;),
            col(&quot;names&quot;).reverse().alias(&quot;reversed names&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (3, 5)
┌────────┬──────┬───────┬────────────┬────────────────┐
│ groups ┆ nrs  ┆ count ┆ random_sum ┆ reversed names │
│ ---    ┆ ---  ┆ ---   ┆ ---        ┆ ---            │
│ str    ┆ i64  ┆ u32   ┆ f64        ┆ list[str]      │
╞════════╪══════╪═══════╪════════════╪════════════════╡
│ A      ┆ 3    ┆ 2     ┆ 0.894213   ┆ [&quot;ham&quot;, &quot;foo&quot;] │
│ B      ┆ 8    ┆ 2     ┆ 0.263315   ┆ [null, &quot;spam&quot;] │
│ C      ┆ null ┆ 1     ┆ 0.533739   ┆ [&quot;egg&quot;]        │
└────────┴──────┴───────┴────────────┴────────────────┘
</code></pre>
<p>Besides the standard <code>groupby</code>, <code>groupby_dynamic</code>, and <code>groupby_rolling</code> are also entrances to the <code>groupby context</code>.</p>
<h1 id="groupby-1"><a class="header" href="#groupby-1">GroupBy</a></h1>
<blockquote>
<p>The GroupBy page is under construction.</p>
</blockquote>
<h2 id="a-multithreaded-approach"><a class="header" href="#a-multithreaded-approach">A multithreaded approach</a></h2>
<p>One of the most efficient ways to process tabular data is to parallelize its processing
via the &quot;split-apply-combine&quot; approach. This operation is at the core of the <code>Polars</code>
grouping implementation, allowing it to attain lightning-fast operations. Specifically, both the &quot;split&quot; and &quot;apply&quot; phases are executed in a multi-threaded
fashion.</p>
<p>A simple grouping operation is taken below as an example to illustrate this approach:</p>
<p><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/docs/split-apply-combine.svg" alt="" /></p>
<p>For the hashing operations performed during the &quot;split&quot; phase, <code>Polars</code> uses a
multithreaded lock-free approach that is illustrated on the following schema:</p>
<p><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/docs/lock-free-hash.svg" alt="" /></p>
<p>This parallelization allows the grouping and joining operations (for instance) to be
blazingly fast!</p>
<blockquote>
<p>Check out <a href="https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/">this blog post</a>
for more details.</p>
</blockquote>
<h2 id="do-not-kill-the-parallelization"><a class="header" href="#do-not-kill-the-parallelization">Do not kill the parallelization!</a></h2>
<blockquote>
<p>The following is specific to <code>Python</code>, and doesn't apply to <code>Rust</code>.  Within <code>Rust</code>, blocks and closures (<em>lambdas</em>) can, and will, be executed concurrently.</p>
</blockquote>
<p>We have all heard that <code>Python</code> is slow, and does &quot;not scale.&quot; Besides the overhead of
running &quot;slow&quot; bytecode, <code>Python</code> has to remain within the constraints of the Global
Interpreter Lock (GIL). This means that if you were to use a <code>lambda</code> or a custom <code>Python</code>
function to apply during a parallelized phase, <code>Polars</code> speed is capped running <code>Python</code>
code preventing any multiple threads from executing the function.</p>
<p>This all feels terribly limiting, especially because we often need those <code>lambda</code> functions in a
<code>.groupby()</code> step, for example. This approach is still supported by <code>Polars</code>, but
keeping in mind bytecode <strong>and</strong> the GIL costs have to be paid.</p>
<p>To mitigate this, <code>Polars</code> implements a powerful syntax defined not only in its lazy API,
but also in its eager API.  Let's take a look at what that means.</p>
<p>We can start with the simple
<a href="https://github.com/unitedstates/congress-legislators">US congress <code>dataset</code></a>.</p>
<blockquote>
<p>Note to Rust users, the <code>dtype-categorical</code> feature must be enabled for the examples in this section.</p>
</blockquote>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

url = &quot;https://theunitedstates.io/congress-legislators/legislators-historical.csv&quot;

dtypes = {
    &quot;first_name&quot;: pl.Categorical,
    &quot;gender&quot;: pl.Categorical,
    &quot;type&quot;: pl.Categorical,
    &quot;state&quot;: pl.Categorical,
    &quot;party&quot;: pl.Categorical,
}

dataset = pl.read_csv(url, dtypes=dtypes).with_columns(pl.col(&quot;birthday&quot;).str.strptime(pl.Date, strict=False))
</code></pre>
<pre><code class="language-rust noplayground">    let url = &quot;https://theunitedstates.io/congress-legislators/legislators-historical.csv&quot;;

    let mut schema = Schema::new();
    schema.with_columns(&quot;first_name&quot;.to_string(), DataType::Categorical(None));
    schema.with_columns(&quot;gender&quot;.to_string(), DataType::Categorical(None));
    schema.with_columns(&quot;type&quot;.to_string(), DataType::Categorical(None));
    schema.with_columns(&quot;state&quot;.to_string(), DataType::Categorical(None));
    schema.with_columns(&quot;party&quot;.to_string(), DataType::Categorical(None));
    schema.with_columns(&quot;birthday&quot;.to_string(), DataType::Date);

    let data: Vec&lt;u8&gt; = Client::new().get(url).send()?.text()?.bytes().collect();

    let dataset = CsvReader::new(Cursor::new(data))
        .has_header(true)
        .with_dtypes(Some(&amp;schema))
        .with_parse_dates(true)
        .finish()?;

    println!(&quot;{}&quot;, &amp;dataset);
</code></pre>
</div>
<h4 id="basic-aggregations"><a class="header" href="#basic-aggregations">Basic aggregations</a></h4>
<p>You can easily combine different aggregations by adding multiple expressions in a
<code>list</code>. There is no upper bound on the number of aggregations you can do, and you can
make any combination you want. In the snippet below we do the following aggregations:</p>
<p>Per GROUP <code>&quot;first_name&quot;</code> we</p>
<ul>
<li>count the number of rows in the group:
<ul>
<li>short form: <code>pl.count(&quot;party&quot;)</code></li>
<li>full form: <code>pl.col(&quot;party&quot;).count()</code></li>
</ul>
</li>
<li>aggregate the gender values groups:
<ul>
<li>full form: <code>pl.col(&quot;gender&quot;)</code></li>
</ul>
</li>
<li>get the first value of column <code>&quot;last_name&quot;</code> in the group:
<ul>
<li>short form: <code>pl.first(&quot;last_name&quot;)</code> (not available in Rust)</li>
<li>full form: <code>pl.col(&quot;last_name&quot;).first()</code></li>
</ul>
</li>
</ul>
<p>Besides the aggregation, we immediately sort the result and limit to the top <code>5</code> so that
we have a nice summary overview.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset

q = (
    dataset.lazy()
    .groupby(&quot;first_name&quot;)
    .agg(
        [
            pl.count(),
            pl.col(&quot;gender&quot;),
            pl.first(&quot;last_name&quot;),
        ]
    )
    .sort(&quot;count&quot;, descending=True)
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    let df = dataset
        .clone()
        .lazy()
        .groupby([&quot;first_name&quot;])
        .agg([count(), col(&quot;gender&quot;).list(), col(&quot;last_name&quot;).first()])
        .sort(
            &quot;count&quot;,
            SortOptions {
                descending: true,
                nulls_last: true,
            },
        )
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 4)
┌────────────┬───────┬───────────────────┬───────────┐
│ first_name ┆ count ┆ gender            ┆ last_name │
│ ---        ┆ ---   ┆ ---               ┆ ---       │
│ cat        ┆ u32   ┆ list[cat]         ┆ str       │
╞════════════╪═══════╪═══════════════════╪═══════════╡
│ John       ┆ 1256  ┆ [&quot;M&quot;, &quot;M&quot;, … &quot;M&quot;] ┆ Walker    │
│ William    ┆ 1022  ┆ [&quot;M&quot;, &quot;M&quot;, … &quot;M&quot;] ┆ Few       │
│ James      ┆ 714   ┆ [&quot;M&quot;, &quot;M&quot;, … &quot;M&quot;] ┆ Armstrong │
│ Thomas     ┆ 454   ┆ [&quot;M&quot;, &quot;M&quot;, … &quot;M&quot;] ┆ Tucker    │
│ Charles    ┆ 439   ┆ [&quot;M&quot;, &quot;M&quot;, … &quot;M&quot;] ┆ Carroll   │
└────────────┴───────┴───────────────────┴───────────┘
</code></pre>
<h4 id="conditionals"><a class="header" href="#conditionals">Conditionals</a></h4>
<p>It's that easy! Let's turn it up a notch. Let's say we want to know how
many delegates of a &quot;state&quot; are &quot;Pro&quot; or &quot;Anti&quot; administration. We could directly query
that in the aggregation without the need of <code>lambda</code> or grooming the <code>DataFrame</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset

q = (
    dataset.lazy()
    .groupby(&quot;state&quot;)
    .agg(
        [
            (pl.col(&quot;party&quot;) == &quot;Anti-Administration&quot;).sum().alias(&quot;anti&quot;),
            (pl.col(&quot;party&quot;) == &quot;Pro-Administration&quot;).sum().alias(&quot;pro&quot;),
        ]
    )
    .sort(&quot;pro&quot;, descending=True)
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    let df = dataset
        .clone()
        .lazy()
        .groupby([&quot;state&quot;])
        .agg([
            (col(&quot;party&quot;).eq(lit(&quot;Anti-Administration&quot;)))
                .sum()
                .alias(&quot;anti&quot;),
            (col(&quot;party&quot;).eq(lit(&quot;Pro-Administration&quot;)))
                .sum()
                .alias(&quot;pro&quot;),
        ])
        .sort(
            &quot;pro&quot;,
            SortOptions {
                descending: true,
                nulls_last: false,
            },
        )
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 3)
┌───────┬──────┬─────┐
│ state ┆ anti ┆ pro │
│ ---   ┆ ---  ┆ --- │
│ cat   ┆ u32  ┆ u32 │
╞═══════╪══════╪═════╡
│ CT    ┆ 0    ┆ 3   │
│ NJ    ┆ 0    ┆ 3   │
│ NC    ┆ 1    ┆ 2   │
│ SC    ┆ 0    ┆ 1   │
│ PA    ┆ 1    ┆ 1   │
└───────┴──────┴─────┘
</code></pre>
<p>Similarly,  this could also be done with a nested GROUPBY, but that doesn't help show off some of these nice features. 😉</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset

q = (
    dataset.lazy()
    .groupby([&quot;state&quot;, &quot;party&quot;])
    .agg([pl.count(&quot;party&quot;).alias(&quot;count&quot;)])
    .filter((pl.col(&quot;party&quot;) == &quot;Anti-Administration&quot;) | (pl.col(&quot;party&quot;) == &quot;Pro-Administration&quot;))
    .sort(&quot;count&quot;, descending=True)
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    let df = dataset
        .clone()
        .lazy()
        .groupby([&quot;state&quot;, &quot;party&quot;])
        .agg([col(&quot;party&quot;).count().alias(&quot;count&quot;)])
        .filter(
            col(&quot;party&quot;)
                .eq(lit(&quot;Anti-Administration&quot;))
                .or(col(&quot;party&quot;).eq(lit(&quot;Pro-Administration&quot;))),
        )
        .sort(
            &quot;count&quot;,
            SortOptions {
                descending: true,
                nulls_last: true,
            },
        )
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 3)
┌───────┬─────────────────────┬───────┐
│ state ┆ party               ┆ count │
│ ---   ┆ ---                 ┆ ---   │
│ cat   ┆ cat                 ┆ u32   │
╞═══════╪═════════════════════╪═══════╡
│ CT    ┆ Pro-Administration  ┆ 3     │
│ NJ    ┆ Pro-Administration  ┆ 3     │
│ VA    ┆ Anti-Administration ┆ 3     │
│ NC    ┆ Pro-Administration  ┆ 2     │
│ SC    ┆ Pro-Administration  ┆ 1     │
└───────┴─────────────────────┴───────┘
</code></pre>
<h4 id="filtering"><a class="header" href="#filtering">Filtering</a></h4>
<p>We can also filter the groups. Let's say we want to compute a mean per group, but we
don't want to include all values from that group, and we also don't want to filter the
rows from the <code>DataFrame</code> (because we need those rows for another aggregation).</p>
<p>In the example below we show how that can be done.</p>
<blockquote>
<p>Note that we can make <code>Python</code> functions for clarity. These functions don't cost us anything. That is because we only create <code>Polars</code> expressions, we don't apply a custom function over a <code>Series</code> during runtime of the query.  Of course, you can make functions that return expressions in Rust, too.</p>
</blockquote>
<div class="tabbed-blocks">
<pre><code class="language-python">from datetime import date

import polars as pl

from .dataset import dataset


def compute_age() -&gt; pl.Expr:
    return date(2021, 1, 1).year - pl.col(&quot;birthday&quot;).dt.year()


def avg_birthday(gender: str) -&gt; pl.Expr:
    return compute_age().filter(pl.col(&quot;gender&quot;) == gender).mean().alias(f&quot;avg {gender} birthday&quot;)


q = (
    dataset.lazy()
    .groupby([&quot;state&quot;])
    .agg(
        [
            avg_birthday(&quot;M&quot;),
            avg_birthday(&quot;F&quot;),
            (pl.col(&quot;gender&quot;) == &quot;M&quot;).sum().alias(&quot;# male&quot;),
            (pl.col(&quot;gender&quot;) == &quot;F&quot;).sum().alias(&quot;# female&quot;),
        ]
    )
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    fn compute_age() -&gt; Expr {
        lit(2022) - col(&quot;birthday&quot;).dt().year()
    }

    fn avg_birthday(gender: &amp;str) -&gt; Expr {
        compute_age()
            .filter(col(&quot;gender&quot;).eq(lit(gender)))
            .mean()
            .alias(&amp;format!(&quot;avg {} birthday&quot;, gender))
    }

    let df = dataset
        .clone()
        .lazy()
        .groupby([&quot;state&quot;])
        .agg([
            avg_birthday(&quot;M&quot;),
            avg_birthday(&quot;F&quot;),
            (col(&quot;gender&quot;).eq(lit(&quot;M&quot;))).sum().alias(&quot;# male&quot;),
            (col(&quot;gender&quot;).eq(lit(&quot;F&quot;))).sum().alias(&quot;# female&quot;),
        ])
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 5)
┌───────┬────────────────┬────────────────┬────────┬──────────┐
│ state ┆ avg M birthday ┆ avg F birthday ┆ # male ┆ # female │
│ ---   ┆ ---            ┆ ---            ┆ ---    ┆ ---      │
│ cat   ┆ f64            ┆ f64            ┆ u32    ┆ u32      │
╞═══════╪════════════════╪════════════════╪════════╪══════════╡
│ AL    ┆ 163.772727     ┆ 97.5           ┆ 207    ┆ 4        │
│ IL    ┆ 153.710638     ┆ 97.4           ┆ 478    ┆ 15       │
│ AZ    ┆ 114.586957     ┆ 76.8           ┆ 46     ┆ 5        │
│ PI    ┆ 145.0          ┆ null           ┆ 13     ┆ 0        │
│ OK    ┆ 119.934066     ┆ 93.0           ┆ 91     ┆ 3        │
└───────┴────────────────┴────────────────┴────────┴──────────┘
</code></pre>
<h4 id="sorting"><a class="header" href="#sorting">Sorting</a></h4>
<p>It's common to see a <code>DataFrame</code> being sorted for the sole purpose of managing the ordering during a GROUPBY operation. Let's say that we want to get the names of the oldest and youngest politicians per state. We could SORT and GROUPBY.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset


def get_person() -&gt; pl.Expr:
    return pl.col(&quot;first_name&quot;) + pl.lit(&quot; &quot;) + pl.col(&quot;last_name&quot;)


q = (
    dataset.lazy()
    .sort(&quot;birthday&quot;, descending=True)
    .groupby([&quot;state&quot;])
    .agg(
        [
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
        ]
    )
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    fn get_person() -&gt; Expr {
        col(&quot;first_name&quot;) + lit(&quot; &quot;) + col(&quot;last_name&quot;)
    }

    let df = dataset
        .clone()
        .lazy()
        .sort(
            &quot;birthday&quot;,
            SortOptions {
                descending: true,
                nulls_last: true,
            },
        )
        .groupby([&quot;state&quot;])
        .agg([
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
        ])
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 3)
┌───────┬──────────────────┬─────────────────┐
│ state ┆ youngest         ┆ oldest          │
│ ---   ┆ ---              ┆ ---             │
│ cat   ┆ str              ┆ str             │
╞═══════╪══════════════════╪═════════════════╡
│ MO    ┆ Vicky Hartzler   ┆ Spencer Pettis  │
│ NH    ┆ Frank Guinta     ┆ John Sherburne  │
│ NE    ┆ Benjamin Sasse   ┆ Samuel Daily    │
│ NC    ┆ Madison Cawthorn ┆ John Ashe       │
│ ID    ┆ Raúl Labrador    ┆ William Wallace │
└───────┴──────────────────┴─────────────────┘
</code></pre>
<p>However, <strong>if</strong> we also want to sort the names alphabetically, this breaks. Luckily we can sort in a <code>groupby</code> context separate from the <code>DataFrame</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset


def get_person() -&gt; pl.Expr:
    return pl.col(&quot;first_name&quot;) + pl.lit(&quot; &quot;) + pl.col(&quot;last_name&quot;)


q = (
    dataset.lazy()
    .sort(&quot;birthday&quot;, descending=True)
    .groupby([&quot;state&quot;])
    .agg(
        [
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
            get_person().sort().first().alias(&quot;alphabetical_first&quot;),
        ]
    )
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    let df = dataset
        .clone()
        .lazy()
        .sort(
            &quot;birthday&quot;,
            SortOptions {
                descending: true,
                nulls_last: true,
            },
        )
        .groupby([&quot;state&quot;])
        .agg([
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
            get_person().sort(false).first().alias(&quot;alphabetical_first&quot;),
        ])
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 4)
┌───────┬────────────────┬──────────────────┬────────────────────┐
│ state ┆ youngest       ┆ oldest           ┆ alphabetical_first │
│ ---   ┆ ---            ┆ ---              ┆ ---                │
│ cat   ┆ str            ┆ str              ┆ str                │
╞═══════╪════════════════╪══════════════════╪════════════════════╡
│ CT    ┆ Elizabeth Esty ┆ Henry Edwards    ┆ Abner Sibal        │
│ VT    ┆ Peter Smith    ┆ Samuel Shaw      ┆ Ahiman Miner       │
│ WI    ┆ Sean Duffy     ┆ Henry Dodge      ┆ Adolphus Nelson    │
│ ID    ┆ Raúl Labrador  ┆ William Wallace  ┆ Abe Goff           │
│ WV    ┆ Carte Goodwin  ┆ Edward Rohrbough ┆ Adam Littlepage    │
└───────┴────────────────┴──────────────────┴────────────────────┘
</code></pre>
<p>We can even sort by another column in the <code>groupby</code> context. If we want to know if the alphabetically sorted name is male or female we could add: <code>pl.col(&quot;gender&quot;).sort_by(&quot;first_name&quot;).first().alias(&quot;gender&quot;)</code></p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

from .dataset import dataset


def get_person() -&gt; pl.Expr:
    return pl.col(&quot;first_name&quot;) + pl.lit(&quot; &quot;) + pl.col(&quot;last_name&quot;)


q = (
    dataset.lazy()
    .sort(&quot;birthday&quot;, descending=True)
    .groupby([&quot;state&quot;])
    .agg(
        [
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
            get_person().sort().first().alias(&quot;alphabetical_first&quot;),
            pl.col(&quot;gender&quot;).sort_by(&quot;first_name&quot;).first().alias(&quot;gender&quot;),
        ]
    )
    .sort(&quot;state&quot;)
    .limit(5)
)

df = q.collect()
</code></pre>
<pre><code class="language-rust noplayground">    let df = dataset
        .clone()
        .lazy()
        .sort(
            &quot;birthday&quot;,
            SortOptions {
                descending: true,
                nulls_last: true,
            },
        )
        .groupby([&quot;state&quot;])
        .agg([
            get_person().first().alias(&quot;youngest&quot;),
            get_person().last().alias(&quot;oldest&quot;),
            get_person().sort(false).first().alias(&quot;alphabetical_first&quot;),
            col(&quot;gender&quot;)
                .sort_by([&quot;first_name&quot;], [false])
                .first()
                .alias(&quot;gender&quot;),
        ])
        .sort(&quot;state&quot;, SortOptions::default())
        .limit(5)
        .collect()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (5, 5)
┌───────┬──────────────────┬───────────────────┬────────────────────┬────────┐
│ state ┆ youngest         ┆ oldest            ┆ alphabetical_first ┆ gender │
│ ---   ┆ ---              ┆ ---               ┆ ---                ┆ ---    │
│ cat   ┆ str              ┆ str               ┆ str                ┆ cat    │
╞═══════╪══════════════════╪═══════════════════╪════════════════════╪════════╡
│ PA    ┆ Conor Lamb       ┆ Thomas Fitzsimons ┆ Aaron Kreider      ┆ M      │
│ KY    ┆ Ben Chandler     ┆ John Edwards      ┆ Aaron Harding      ┆ M      │
│ MD    ┆ Frank Kratovil   ┆ Benjamin Contee   ┆ Albert Blakeney    ┆ M      │
│ OH    ┆ Anthony Gonzalez ┆ John Smith        ┆ Aaron Harlan       ┆ M      │
│ VA    ┆ Scott Taylor     ┆ William Grayson   ┆ A. McEachin        ┆ M      │
└───────┴──────────────────┴───────────────────┴────────────────────┴────────┘
</code></pre>
<h3 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h3>
<p>In the examples above we've seen that we can do a lot by combining expressions. By doing so we delay the use of custom <code>Python</code> functions that slow down the queries (by the slow nature of Python AND the GIL).</p>
<p>If we are missing a type expression let us know by opening a
<a href="https://github.com/pola-rs/polars/issues/new/choose">feature request</a>!</p>
<h1 id="folds"><a class="header" href="#folds">Folds</a></h1>
<p><code>Polars</code> provides expressions/methods for horizontal aggregations like <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.sum.html"><code>sum</code></a>,
<a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.min.html"><code>min</code></a>, <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.mean.html"><code>mean</code></a>,
etc. by setting the argument <code>axis=1</code>. However, when you need a more complex aggregation the default methods provided by the
<code>Polars</code> library may not be sufficient. That's when <code>folds</code> come in handy.</p>
<p>The <code>Polars</code> <code>fold</code> expression operates on columns for maximum speed. It utilizes the data layout very efficiently and often has vectorized execution.</p>
<p>Let's start with an example by implementing the <code>sum</code> operation ourselves, with a <code>fold</code>.</p>
<h2 id="manual-sum"><a class="header" href="#manual-sum">Manual Sum</a></h2>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;a&quot;: [1, 2, 3],
        &quot;b&quot;: [10, 20, 30],
    }
)

out = df.select(
    pl.fold(acc=pl.lit(0), f=lambda acc, x: acc + x, exprs=pl.col(&quot;*&quot;)).alias(&quot;sum&quot;),
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let df = df!(
        &quot;a&quot; =&gt; &amp;[1, 2, 3],
        &quot;b&quot; =&gt; &amp;[10, 20, 30],
    )?;

    let out = df
        .lazy()
        .select([fold_exprs(lit(0), |acc, x| Ok(acc + x), [col(&quot;*&quot;)]).alias(&quot;sum&quot;)])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (3, 1)
┌─────┐
│ sum │
│ --- │
│ i64 │
╞═════╡
│ 11  │
│ 22  │
│ 33  │
└─────┘
</code></pre>
<p>The snippet above recursively applies the function <code>f(acc, x) -&gt; acc</code> to an accumulator <code>acc</code> and a new column <code>x</code>.
The function operates on columns individually and can take advantage of cache efficiency and vectorization.</p>
<h2 id="conditional"><a class="header" href="#conditional">Conditional</a></h2>
<p>In the case where you'd want to apply a condition/predicate on all columns in a <code>DataFrame</code> a <code>fold</code> operation can be
a very concise way to express this.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;a&quot;: [1, 2, 3],
        &quot;b&quot;: [0, 1, 2],
    }
)

out = df.filter(
    pl.fold(
        acc=pl.lit(True),
        f=lambda acc, x: acc &amp; x,
        exprs=pl.col(&quot;*&quot;) &gt; 1,
    )
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let df = df!(
        &quot;a&quot; =&gt; &amp;[1, 2, 3],
        &quot;b&quot; =&gt; &amp;[0, 1, 2],
    )?;

    let out = df
        .lazy()
        .filter(fold_exprs(
            lit(true),
            |acc, x| acc.bitand(&amp;x),
            [col(&quot;*&quot;).gt(1)],
        ))
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (1, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 3   ┆ 2   │
└─────┴─────┘
</code></pre>
<p>In the snippet we filter all rows where <strong>each</strong> column value is <code>&gt;</code> <code>1</code>.</p>
<h2 id="folds-and-string-data"><a class="header" href="#folds-and-string-data">Folds and string data</a></h2>
<p>Folds could be used to concatenate string data. However, due to the materialization of intermediate columns, this
operation will have squared complexity.</p>
<p>Therefore, we recommend using the <code>concat_str</code> expression for this.</p>
<blockquote>
<p>Note that, in <code>Rust</code>, the <code>concat_str</code> feature must be enabled to use the <code>concat_str</code> expression.</p>
</blockquote>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;a&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
        &quot;b&quot;: [1, 2, 3],
    }
)

out = df.select(
    [
        pl.concat_str([&quot;a&quot;, &quot;b&quot;]),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let df = df!(
        &quot;a&quot; =&gt; &amp;[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
        &quot;b&quot; =&gt; &amp;[1, 2, 3],
    )?;

    let out = df
        .lazy()
        .select([concat_str([col(&quot;a&quot;), col(&quot;b&quot;)], &quot;&quot;)])
        .collect()?;
    println!(&quot;{:?}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (3, 1)
┌─────┐
│ a   │
│ --- │
│ str │
╞═════╡
│ a1  │
│ b2  │
│ c3  │
└─────┘
</code></pre>
<h1 id="window-functions-"><a class="header" href="#window-functions-">Window functions 🚀🚀</a></h1>
<p>Window functions are expressions with superpowers. They allow you to perform aggregations on groups in the
<code>select</code> context. Let's get a feel of what that means. First we create a dataset. The dataset loaded in the
snippet below contains information about pokemon and has the following columns:</p>
<p><code>['#',  'Name',  'Type 1',  'Type 2',  'Total',  'HP',  'Attack',  'Defense',  'Sp. Atk',  'Sp. Def',  'Speed',  'Generation',  'Legendary']</code></p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

# then let's load some csv data with information about pokemon
df = pl.read_csv(
    &quot;https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv&quot;
)
</code></pre>
<pre><code class="language-rust noplayground">use color_eyre::Result;
use polars::prelude::*;
use reqwest::blocking::Client;

fn main() -&gt; Result&lt;()&gt; {
    let data: Vec&lt;u8&gt; = Client::new()
        .get(&quot;https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv&quot;)
        .send()?
        .text()?
        .bytes()
        .collect();

    let df = CsvReader::new(std::io::Cursor::new(data))
        .has_header(true)
        .finish()?;

    println!(&quot;{}&quot;, df);
</code></pre>
</div>
<pre><code class="language-text">shape: (163, 13)
┌─────┬───────────────────────┬─────────┬────────┬───┬─────────┬───────┬────────────┬───────────┐
│ #   ┆ Name                  ┆ Type 1  ┆ Type 2 ┆ … ┆ Sp. Def ┆ Speed ┆ Generation ┆ Legendary │
│ --- ┆ ---                   ┆ ---     ┆ ---    ┆   ┆ ---     ┆ ---   ┆ ---        ┆ ---       │
│ i64 ┆ str                   ┆ str     ┆ str    ┆   ┆ i64     ┆ i64   ┆ i64        ┆ bool      │
╞═════╪═══════════════════════╪═════════╪════════╪═══╪═════════╪═══════╪════════════╪═══════════╡
│ 1   ┆ Bulbasaur             ┆ Grass   ┆ Poison ┆ … ┆ 65      ┆ 45    ┆ 1          ┆ false     │
│ 2   ┆ Ivysaur               ┆ Grass   ┆ Poison ┆ … ┆ 80      ┆ 60    ┆ 1          ┆ false     │
│ 3   ┆ Venusaur              ┆ Grass   ┆ Poison ┆ … ┆ 100     ┆ 80    ┆ 1          ┆ false     │
│ 3   ┆ VenusaurMega Venusaur ┆ Grass   ┆ Poison ┆ … ┆ 120     ┆ 80    ┆ 1          ┆ false     │
│ …   ┆ …                     ┆ …       ┆ …      ┆ … ┆ …       ┆ …     ┆ …          ┆ …         │
│ 147 ┆ Dratini               ┆ Dragon  ┆ null   ┆ … ┆ 50      ┆ 50    ┆ 1          ┆ false     │
│ 148 ┆ Dragonair             ┆ Dragon  ┆ null   ┆ … ┆ 70      ┆ 70    ┆ 1          ┆ false     │
│ 149 ┆ Dragonite             ┆ Dragon  ┆ Flying ┆ … ┆ 100     ┆ 80    ┆ 1          ┆ false     │
│ 150 ┆ Mewtwo                ┆ Psychic ┆ null   ┆ … ┆ 90      ┆ 130   ┆ 1          ┆ true      │
└─────┴───────────────────────┴─────────┴────────┴───┴─────────┴───────┴────────────┴───────────┘
</code></pre>
<h2 id="groupby-aggregations-in-selection"><a class="header" href="#groupby-aggregations-in-selection">Groupby Aggregations in selection</a></h2>
<p>Below we show how to use window functions to group over different columns and perform an aggregation on them.
Doing so allows us to use multiple groupby operations in parallel, using a single query. The results of the aggregation
are projected back to the original rows. Therefore, a window function will always lead to a <code>DataFrame</code> with the same size
as the original.</p>
<p>Note how we call <code>.over(&quot;Type 1&quot;)</code> and <code>.over([&quot;Type 1&quot;, &quot;Type 2&quot;])</code>. Using window functions we can aggregate over different groups in a single <code>select</code> call!  Note that, in Rust, the type of the argument to <code>over()</code> must be a collection, so even when you're only using one column, you must provided it in an array.</p>
<p>The best part is, this won't cost you anything. The computed groups are cached and shared between different <code>window</code> expressions.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        &quot;Type 1&quot;,
        &quot;Type 2&quot;,
        pl.col(&quot;Attack&quot;).mean().over(&quot;Type 1&quot;).alias(&quot;avg_attack_by_type&quot;),
        pl.col(&quot;Defense&quot;).mean().over([&quot;Type 1&quot;, &quot;Type 2&quot;]).alias(&quot;avg_defense_by_type_combination&quot;),
        pl.col(&quot;Attack&quot;).mean().alias(&quot;avg_attack&quot;),
    ]
)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([
            col(&quot;Type 1&quot;),
            col(&quot;Type 2&quot;),
            col(&quot;Attack&quot;)
                .mean()
                .over([&quot;Type 1&quot;])
                .alias(&quot;avg_attack_by_type&quot;),
            col(&quot;Defense&quot;)
                .mean()
                .over([&quot;Type 1&quot;, &quot;Type 2&quot;])
                .alias(&quot;avg_defense_by_type_combination&quot;),
            col(&quot;Attack&quot;).mean().alias(&quot;avg_attack&quot;),
        ])
        .collect()?;

    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (163, 5)
┌─────────┬────────┬────────────────────┬─────────────────────────────────┬────────────┐
│ Type 1  ┆ Type 2 ┆ avg_attack_by_type ┆ avg_defense_by_type_combination ┆ avg_attack │
│ ---     ┆ ---    ┆ ---                ┆ ---                             ┆ ---        │
│ str     ┆ str    ┆ f64                ┆ f64                             ┆ f64        │
╞═════════╪════════╪════════════════════╪═════════════════════════════════╪════════════╡
│ Grass   ┆ Poison ┆ 72.923077          ┆ 67.8                            ┆ 75.349693  │
│ Grass   ┆ Poison ┆ 72.923077          ┆ 67.8                            ┆ 75.349693  │
│ Grass   ┆ Poison ┆ 72.923077          ┆ 67.8                            ┆ 75.349693  │
│ Grass   ┆ Poison ┆ 72.923077          ┆ 67.8                            ┆ 75.349693  │
│ …       ┆ …      ┆ …                  ┆ …                               ┆ …          │
│ Dragon  ┆ null   ┆ 94.0               ┆ 55.0                            ┆ 75.349693  │
│ Dragon  ┆ null   ┆ 94.0               ┆ 55.0                            ┆ 75.349693  │
│ Dragon  ┆ Flying ┆ 94.0               ┆ 95.0                            ┆ 75.349693  │
│ Psychic ┆ null   ┆ 53.875             ┆ 51.428571                       ┆ 75.349693  │
└─────────┴────────┴────────────────────┴─────────────────────────────────┴────────────┘
</code></pre>
<h2 id="operations-per-group"><a class="header" href="#operations-per-group">Operations per group</a></h2>
<p>Window functions can do more than aggregation. They can also be viewed as an operation within a group. If, for instance, you
want to <code>sort</code> the values within a <code>group</code>, you can write <code>col(&quot;value&quot;).sort().over(&quot;group&quot;)</code> and voilà! We sorted by group!</p>
<p>Let's filter out some rows to make this more clear.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">filtered = df.filter(pl.col(&quot;Type 2&quot;) == &quot;Psychic&quot;).select(
    [
        &quot;Name&quot;,
        &quot;Type 1&quot;,
        &quot;Speed&quot;,
    ]
)
print(filtered)
</code></pre>
<pre><code class="language-rust noplayground">    let filtered = df
        .clone()
        .lazy()
        .filter(col(&quot;Type 2&quot;).eq(lit(&quot;Psychic&quot;)))
        .select([col(&quot;Name&quot;), col(&quot;Type 1&quot;), col(&quot;Speed&quot;)])
        .collect()?;

    println!(&quot;{}&quot;, filtered);
</code></pre>
</div>
<pre><code class="language-text">shape: (7, 3)
┌─────────────────────┬────────┬───────┐
│ Name                ┆ Type 1 ┆ Speed │
│ ---                 ┆ ---    ┆ ---   │
│ str                 ┆ str    ┆ i64   │
╞═════════════════════╪════════╪═══════╡
│ Slowpoke            ┆ Water  ┆ 15    │
│ Slowbro             ┆ Water  ┆ 30    │
│ SlowbroMega Slowbro ┆ Water  ┆ 30    │
│ Exeggcute           ┆ Grass  ┆ 40    │
│ Exeggutor           ┆ Grass  ┆ 55    │
│ Starmie             ┆ Water  ┆ 115   │
│ Jynx                ┆ Ice    ┆ 95    │
└─────────────────────┴────────┴───────┘
</code></pre>
<p>Observe that the group <code>Water</code> of column <code>Type 1</code> is not contiguous. There are two rows of <code>Grass</code> in between. Also note
that each pokemon within a group are sorted by <code>Speed</code> in <code>ascending</code> order. Unfortunately, for this example we want them sorted in
<code>descending</code> speed order. Luckily with window functions this is easy to accomplish.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = filtered.with_columns(
    [
        pl.col([&quot;Name&quot;, &quot;Speed&quot;]).sort(descending=True).over(&quot;Type 1&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = filtered
        .lazy()
        .with_columns([cols([&quot;Name&quot;, &quot;Speed&quot;]).sort(true).over([&quot;Type 1&quot;])])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">
shape: (7, 3)
┌─────────────────────┬────────┬───────┐
│ Name                ┆ Type 1 ┆ Speed │
│ ---                 ┆ ---    ┆ ---   │
│ str                 ┆ str    ┆ i64   │
╞═════════════════════╪════════╪═══════╡
│ Starmie             ┆ Water  ┆ 115   │
│ Slowpoke            ┆ Water  ┆ 30    │
│ SlowbroMega Slowbro ┆ Water  ┆ 30    │
│ Exeggutor           ┆ Grass  ┆ 55    │
│ Exeggcute           ┆ Grass  ┆ 40    │
│ Slowbro             ┆ Water  ┆ 15    │
│ Jynx                ┆ Ice    ┆ 95    │
└─────────────────────┴────────┴───────┘
</code></pre>
<p><code>Polars</code> keeps track of each group's location and maps the expressions to the proper row locations. This will also work
over different groups in a single <code>select</code>.</p>
<p>The power of window expressions is that you often don't need a <code>groupby -&gt; explode</code> combination, but you can put the logic in a
single expression. It also makes the API cleaner. If properly used a:</p>
<ul>
<li><code>groupby</code> -&gt; marks that groups are aggregated and we expect a <code>DataFrame</code> of size <code>n_groups</code></li>
<li><code>over</code> -&gt; marks that we want to compute something within a group, but doesn't modify the original size of the <code>DataFrame</code></li>
</ul>
<h2 id="window-expression-rules"><a class="header" href="#window-expression-rules">Window expression rules</a></h2>
<p>The evaluations of window expressions are as follows (assuming we apply it to a <code>pl.Int32</code> column):</p>
<div class="tabbed-blocks">
<pre><code class="language-python"># aggregate and broadcast within a group
# output type: -&gt; Int32
pl.sum(&quot;foo&quot;).over(&quot;groups&quot;)

# sum within a group and multiply with group elements
# output type: -&gt; Int32
(pl.col(&quot;x&quot;).sum() * pl.col(&quot;y&quot;)).over(&quot;groups&quot;)

# sum within a group and multiply with group elements 
# and aggregate the group to a list
# output type: -&gt; List(Int32)
(pl.col(&quot;x&quot;).sum() * pl.col(&quot;y&quot;)).list().over(&quot;groups&quot;)

# note that it will require an explicit `list()` call
# sum within a group and multiply with group elements 
# and aggregate the group to a list
# the flatten call explodes that list

# This is the fastest method to do things over groups when the groups are sorted
(pl.col(&quot;x&quot;).sum() * pl.col(&quot;y&quot;)).list().over(&quot;groups&quot;).flatten()
</code></pre>
<pre><code class="language-rust noplayground">            // aggregate and broadcast within a group
            // output type: -&gt; i32
            sum(&quot;foo&quot;).over([col(&quot;groups&quot;)]),
            // sum within a group and multiply with group elements
            // output type: -&gt; i32
            (col(&quot;x&quot;).sum() * col(&quot;y&quot;))
                .over([col(&quot;groups&quot;)])
                .alias(&quot;x1&quot;),
            // sum within a group and multiply with group elements
            // and aggregate the group to a list
            // output type: -&gt; ChunkedArray&lt;i32&gt;
            (col(&quot;x&quot;).sum() * col(&quot;y&quot;))
                .list()
                .over([col(&quot;groups&quot;)])
                .alias(&quot;x2&quot;),
            // note that it will require an explicit `list()` call
            // sum within a group and multiply with group elements
            // and aggregate the group to a list
            // the flatten call explodes that list

            // This is the fastest method to do things over groups when the groups are sorted
            (col(&quot;x&quot;).sum() * col(&quot;y&quot;))
                .list()
                .over([col(&quot;groups&quot;)])
                .flatten()
                .alias(&quot;x3&quot;),
        ])
        .collect()?;
</code></pre>
</div>
<h2 id="more-examples"><a class="header" href="#more-examples">More examples</a></h2>
<p>For more exercise, below are some window functions for us to compute:</p>
<ul>
<li>sort all pokemon by type</li>
<li>select the first <code>3</code> pokemon per type as <code>&quot;Type 1&quot;</code></li>
<li>sort the pokemon within a type by speed and select the first <code>3</code> as <code>&quot;fastest/group&quot;</code></li>
<li>sort the pokemon within a type by attack and select the first <code>3</code> as <code>&quot;strongest/group&quot;</code></li>
<li>sort the pokemon by name within a type and select the first <code>3</code> as <code>&quot;sorted_by_alphabet&quot;</code></li>
</ul>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.sort(&quot;Type 1&quot;).select(
    [
        pl.col(&quot;Type 1&quot;).head(3).list().over(&quot;Type 1&quot;).flatten(),
        pl.col(&quot;Name&quot;).sort_by(pl.col(&quot;Speed&quot;)).head(3).list().over(&quot;Type 1&quot;).flatten().alias(&quot;fastest/group&quot;),
        pl.col(&quot;Name&quot;).sort_by(pl.col(&quot;Attack&quot;)).head(3).list().over(&quot;Type 1&quot;).flatten().alias(&quot;strongest/group&quot;),
        pl.col(&quot;Name&quot;).sort().head(3).list().over(&quot;Type 1&quot;).flatten().alias(&quot;sorted_by_alphabet&quot;),
    ]
)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .select([
            col(&quot;Type 1&quot;)
                .head(Some(3))
                .list()
                .over([&quot;Type 1&quot;])
                .flatten(),
            col(&quot;Name&quot;)
                .sort_by([&quot;Speed&quot;], [false])
                .head(Some(3))
                .list()
                .over([&quot;Type 1&quot;])
                .flatten()
                .alias(&quot;fastest/group&quot;),
            col(&quot;Name&quot;)
                .sort_by([&quot;Attack&quot;], [false])
                .head(Some(3))
                .list()
                .over([&quot;Type 1&quot;])
                .flatten()
                .alias(&quot;strongest/group&quot;),
            col(&quot;Name&quot;)
                .sort(false)
                .head(Some(3))
                .list()
                .over([&quot;Type 1&quot;])
                .flatten()
                .alias(&quot;sorted_by_alphabet&quot;),
        ])
        .collect()?;
    println!(&quot;{:?}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (43, 4)
┌────────┬─────────────────────┬─────────────────┬─────────────────────────┐
│ Type 1 ┆ fastest/group       ┆ strongest/group ┆ sorted_by_alphabet      │
│ ---    ┆ ---                 ┆ ---             ┆ ---                     │
│ str    ┆ str                 ┆ str             ┆ str                     │
╞════════╪═════════════════════╪═════════════════╪═════════════════════════╡
│ Bug    ┆ Paras               ┆ Metapod         ┆ Beedrill                │
│ Bug    ┆ Metapod             ┆ Kakuna          ┆ BeedrillMega Beedrill   │
│ Bug    ┆ Parasect            ┆ Caterpie        ┆ Butterfree              │
│ Dragon ┆ Dratini             ┆ Dratini         ┆ Dragonair               │
│ …      ┆ …                   ┆ …               ┆ …                       │
│ Rock   ┆ Omanyte             ┆ Omastar         ┆ Geodude                 │
│ Water  ┆ Slowpoke            ┆ Magikarp        ┆ Blastoise               │
│ Water  ┆ Slowbro             ┆ Tentacool       ┆ BlastoiseMega Blastoise │
│ Water  ┆ SlowbroMega Slowbro ┆ Horsea          ┆ Cloyster                │
└────────┴─────────────────────┴─────────────────┴─────────────────────────┘
</code></pre>
<h2 id="flattened-window-function"><a class="header" href="#flattened-window-function">Flattened window function</a></h2>
<p>If we have a window function that aggregates to a <code>list</code> like the example above with the following Python expression:</p>
<p><code>pl.col(&quot;Name&quot;).sort_by(pl.col(&quot;Speed&quot;)).head(3).list().over(&quot;Type 1&quot;)</code></p>
<p>and in Rust:</p>
<p><code>col(&quot;Name&quot;).sort_by([&quot;Speed&quot;], [false]).head(Some(3)).list().over([&quot;Type 1&quot;])</code></p>
<p>This still works, but that would give us a column type <code>List</code> which might not be what we want (this would significantly increase our memory usage!).</p>
<p>Instead we could <code>flatten</code>. This just turns our 2D list into a 1D array and projects that array/column back to our <code>DataFrame</code>.
This is very fast because the reshape is often free, and adding the column back the the original <code>DataFrame</code> is also a lot cheaper (since we don't require a join like in a normal window function).</p>
<p>However, for this operation to make sense, it is important that the columns used in <code>over([..])</code> are sorted!</p>
<h1 id="list-context"><a class="header" href="#list-context">List context</a></h1>
<p>An expression context we haven't discussed yet is the <code>List</code> context. This means simply we can apply any expression on the elements of a <code>List</code>.</p>
<blockquote>
<p>A note for <code>Rust</code> users, these features require the <code>list</code> feature flag.</p>
</blockquote>
<h1 id="row-wise-computations"><a class="header" href="#row-wise-computations">Row wise computations</a></h1>
<p>This context is ideal for computing things in row orientation.</p>
<p>Polars expressions work on columns that have the guarantee that they consist of homogeneous data.
Columns have this guarantee, rows in a <code>DataFrame</code> not so much.
Luckily we have a data type that has the guarantee that the rows are homogeneous: <code>pl.List</code> data type.</p>
<p>Let's say we have the following data:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">grades = pl.DataFrame(
    {
        &quot;student&quot;: [&quot;bas&quot;, &quot;laura&quot;, &quot;tim&quot;, &quot;jenny&quot;],
        &quot;arithmetic&quot;: [10, 5, 6, 8],
        &quot;biology&quot;: [4, 6, 2, 7],
        &quot;geography&quot;: [8, 4, 9, 7],
    }
)
print(grades)
</code></pre>
<pre><code class="language-rust noplayground">    let grades = df!(
        &quot;student&quot; =&gt; &amp;[&quot;bas&quot;, &quot;laura&quot;, &quot;tim&quot;, &quot;jenny&quot;],
        &quot;arithmetic&quot; =&gt; &amp;[10, 5, 6, 8],
        &quot;biology&quot; =&gt; &amp;[4, 6, 2, 7],
        &quot;geography&quot; =&gt; &amp;[8, 4, 9, 7],
    )?;
    println!(&quot;{}&quot;, grades);
</code></pre>
</div>
<pre><code class="language-text">shape: (4, 4)
┌─────────┬────────────┬─────────┬───────────┐
│ student ┆ arithmetic ┆ biology ┆ geography │
│ ---     ┆ ---        ┆ ---     ┆ ---       │
│ str     ┆ i64        ┆ i64     ┆ i64       │
╞═════════╪════════════╪═════════╪═══════════╡
│ bas     ┆ 10         ┆ 4       ┆ 8         │
│ laura   ┆ 5          ┆ 6       ┆ 4         │
│ tim     ┆ 6          ┆ 2       ┆ 9         │
│ jenny   ┆ 8          ┆ 7       ┆ 7         │
└─────────┴────────────┴─────────┴───────────┘
</code></pre>
<p>If we want to compute the <code>rank</code> of all the columns except for <code>&quot;student&quot;</code>, we can collect those into a <code>list</code> data type:</p>
<p>This would give:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = grades.select([pl.concat_list(pl.all().exclude(&quot;student&quot;)).alias(&quot;all_grades&quot;)])
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = grades
        .clone()
        .lazy()
        .select([concat_lst([all().exclude([&quot;student&quot;])]).alias(&quot;all_grades&quot;)])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (4, 1)
┌────────────┐
│ all_grades │
│ ---        │
│ list[i64]  │
╞════════════╡
│ [10, 4, 8] │
│ [5, 6, 4]  │
│ [6, 2, 9]  │
│ [8, 7, 7]  │
└────────────┘
</code></pre>
<h2 id="running-polars-expression-on-list-elements"><a class="header" href="#running-polars-expression-on-list-elements">Running polars expression on list elements</a></h2>
<p>We can run <strong>any</strong> polars expression on the elements of a list with the <code>arr.eval</code> (<code>arr().eval</code> in Rust) expression! These expressions run entirely on polars' query engine and can run in parallel so will be super fast.</p>
<p>Let's expand the example from above with something a little more interesting. Pandas allows you to compute the percentages of the <code>rank</code> values. Polars doesn't provide such a keyword argument. But because expressions are so versatile we can create our own percentage rank expression. Let's try that!</p>
<p>Note that we must <code>select</code> the list's element from the context. When we apply expressions over list elements, we use <code>pl.element()</code> to select the element of a list.</p>
<blockquote>
<p>Also note that to use <code>arr().eval</code> in Rust requires the <code>list_eval</code> feature flag.</p>
</blockquote>
<div class="tabbed-blocks">
<pre><code class="language-python"># the percentage rank expression
rank_pct = pl.element().rank(descending=True) / pl.col(&quot;&quot;).count()


grades.with_columns(
    # create the list of homogeneous data
    pl.concat_list(pl.all().exclude(&quot;student&quot;)).alias(&quot;all_grades&quot;)
).select([
    # select all columns except the intermediate list
    pl.all().exclude(&quot;all_grades&quot;),
    # compute the rank by calling `arr.eval`
    pl.col(&quot;all_grades&quot;).arr.eval(rank_pct, parallel=True).alias(&quot;grades_rank&quot;)
])
</code></pre>
<pre><code class="language-rust noplayground">    // the percentage rank expression
    let rank_opts = RankOptions {
        method: RankMethod::Average,
        descending: true,
    };
    let rank_pct = col(&quot;&quot;).rank(rank_opts) / col(&quot;&quot;).count().cast(DataType::Float32);

    let grades = grades
        .clone()
        .lazy()
        .with_columns(
            // create the list of homogeneous data
            concat_lst([all().exclude([&quot;student&quot;])]).alias(&quot;all_grades&quot;),
        )
        .select([
            // select all columns except the intermediate list
            all().exclude([&quot;all_grades&quot;]),
            // compute the rank by calling `arr.eval`
            col(&quot;all_grades&quot;)
                .arr()
                .eval(rank_pct, true)
                .alias(&quot;grades_rank&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, grades);
</code></pre>
</div>
<p>This outputs:</p>
<pre><code>shape: (4, 5)
┌─────────┬────────────┬─────────┬───────────┬────────────────────────────────┐
│ student ┆ arithmetic ┆ biology ┆ geography ┆ grades_rank                    │
│ ---     ┆ ---        ┆ ---     ┆ ---       ┆ ---                            │
│ str     ┆ i64        ┆ i64     ┆ i64       ┆ list [f32]                     │
╞═════════╪════════════╪═════════╪═══════════╪════════════════════════════════╡
│ bas     ┆ 10         ┆ 4       ┆ 8         ┆ [0.333333, 1.0, 0.666667]      │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ laura   ┆ 5          ┆ 6       ┆ 4         ┆ [0.666667, 0.333333, 1.0]      │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ tim     ┆ 6          ┆ 2       ┆ 9         ┆ [0.666667, 1.0, 0.333333]      │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ jenny   ┆ 8          ┆ 7       ┆ 7         ┆ [0.333333, 0.833333, 0.833333] │
└─────────┴────────────┴─────────┴───────────┴────────────────────────────────┘

</code></pre>
<p>Note that this solution works for any expressions/operation you want to do row wise.</p>
<h1 id="numpy-interop"><a class="header" href="#numpy-interop">Numpy interop</a></h1>
<p><code>Polars</code> expressions support <code>NumPy</code> <a href="https://numpy.org/doc/stable/reference/ufuncs.html">ufuncs</a>. See <a href="https://numpy.org/doc/stable/reference/ufuncs.html#available-ufuncs">here</a>
for a list on all supported numpy functions.</p>
<p>This means that if a function is not provided by <code>Polars</code>, we can use <code>NumPy</code> and we still have fast columnar operation through
the <code>NumPy</code> API.</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<pre><code class="language-python">import polars as pl
import numpy as np

df = pl.DataFrame({&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]})

out = df.select(
    [
        np.log(pl.all()).suffix(&quot;_log&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌──────────┬──────────┐
│ a_log    ┆ b_log    │
│ ---      ┆ ---      │
│ f64      ┆ f64      │
╞══════════╪══════════╡
│ 0.0      ┆ 1.386294 │
│ 0.693147 ┆ 1.609438 │
│ 1.098612 ┆ 1.791759 │
└──────────┴──────────┘
</code></pre>
<h2 id="gotchas"><a class="header" href="#gotchas">Gotcha's</a></h2>
<p>Read more about the <a href="dsl//polars-book/user-guide/howcani/interop/numpy.html">gotcha's here</a>.</p>
<h1 id="custom-functions"><a class="header" href="#custom-functions">Custom functions</a></h1>
<p>You should be convinced by now that polar expressions are so powerful and flexible that the need for custom python functions
is much less needed than you might need in other libraries.</p>
<p>Still, you need to have the power to be able to pass an expression's state to a third party library or apply your black box function
over data in polars.</p>
<p>For this we provide the following expressions:</p>
<ul>
<li><code>map</code></li>
<li><code>apply</code></li>
</ul>
<h2 id="to-map-or-to-apply"><a class="header" href="#to-map-or-to-apply">To <code>map</code> or to <code>apply</code>.</a></h2>
<p>These functions have an important distinction in how they operate and consequently what data they will pass to the user.</p>
<p>A <code>map</code> passes the <code>Series</code> backed by the <code>expression</code> as is.</p>
<p><code>map</code> follows the same rules in both the <code>select</code> and the <code>groupby</code> context, this will
mean that the <code>Series</code> represents a column in a <code>DataFrame</code>. Note that in the <code>groupby</code> context, that column is not yet
aggregated!</p>
<p>Use cases for <code>map</code> are for instance passing the <code>Series</code> in an expression to a third party library. Below we show how
we could use <code>map</code> to pass an expression column to a neural network model.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df.with_columns([
    pl.col(&quot;features&quot;).map(lambda s: MyNeuralNetwork.forward(s.to_numpy())).alias(&quot;activations&quot;)
])
</code></pre>
<pre><code class="language-rust noplayground">df.with_columns([
    col(&quot;features&quot;).map(|s| Ok(my_nn.forward(s))).alias(&quot;activations&quot;)
])
</code></pre>
</div>
<p>Use cases for <code>map</code> in the <code>groupby</code> context are slim. They are only used for performance reasons, but can quite easily
lead to incorrect results. Let me explain why.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;keys&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;b&quot;],
        &quot;values&quot;: [10, 7, 1],
    }
)

out = df.groupby(&quot;keys&quot;, maintain_order=True).agg(
    [
        pl.col(&quot;values&quot;).map(lambda s: s.shift()).alias(&quot;shift_map&quot;),
        pl.col(&quot;values&quot;).shift().alias(&quot;shift_expression&quot;),
    ]
)
print(df)
</code></pre>
<pre><code class="language-rust noplayground">        &quot;keys&quot; =&gt; &amp;[&quot;a&quot;, &quot;a&quot;, &quot;b&quot;],
        &quot;values&quot; =&gt; &amp;[10, 7, 1],
    )?;

    let out = df
        .lazy()
        .groupby([&quot;keys&quot;])
        .agg([
            col(&quot;values&quot;)
                .map(|s| Ok(s.shift(1)), GetOutput::default())
                .alias(&quot;shift_map&quot;),
            col(&quot;values&quot;).shift(1).alias(&quot;shift_expression&quot;),
        ])
        .collect()?;

    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code>shape: (3, 2)
┌──────┬────────┐
│ keys ┆ values │
│ ---  ┆ ---    │
│ str  ┆ i64    │
╞══════╪════════╡
│ a    ┆ 10     │
├╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ a    ┆ 7      │
├╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ b    ┆ 1      │
└──────┴────────┘

</code></pre>
<p>In the snippet above we groupby the <code>&quot;keys&quot;</code> column. That means we have the following groups:</p>
<pre><code class="language-c">&quot;a&quot; -&gt; [10, 7]
&quot;b&quot; -&gt; [1]
</code></pre>
<p>If we would then apply a <code>shift</code> operation to the right, we'd expect:</p>
<pre><code class="language-c">&quot;a&quot; -&gt; [null, 10]
&quot;b&quot; -&gt; [null]
</code></pre>
<p>Now, let's print and see what we've got.</p>
<pre><code class="language-python">print(out)
</code></pre>
<pre><code class="language-text">shape: (2, 3)
┌──────┬────────────┬──────────────────┐
│ keys ┆ shift_map  ┆ shift_expression │
│ ---  ┆ ---        ┆ ---              │
│ str  ┆ list[i64]  ┆ list[i64]        │
╞══════╪════════════╪══════════════════╡
│ a    ┆ [null, 10] ┆ [null, 10]       │
│ b    ┆ [7]        ┆ [null]           │
└──────┴────────────┴──────────────────┘
</code></pre>
<p>Ouch.. we clearly get the wrong results here. Group <code>&quot;b&quot;</code> even got a value from group <code>&quot;a&quot;</code> 😵.</p>
<p>This went horribly wrong, because the <code>map</code> applies the function before we aggregate! So that means the whole column
<code>[10, 7, 1</code>] got shifted to <code>[null, 10, 7]</code> and was then aggregated.</p>
<p>So my advice is to never use <code>map</code> in the <code>groupby</code> context unless you know you need it and know what you are doing.</p>
<h2 id="to-apply"><a class="header" href="#to-apply">To <code>apply</code></a></h2>
<p>Luckily we can fix previous example with <code>apply</code>. <code>apply</code> works on the smallest logical elements for that operation.</p>
<p>That is:</p>
<ul>
<li><code>select context</code> -&gt; single elements</li>
<li><code>groupby context</code> -&gt; single groups</li>
</ul>
<p>So with <code>apply</code> we should be able to fix our example:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.groupby(&quot;keys&quot;, maintain_order=True).agg(
    [
        pl.col(&quot;values&quot;).apply(lambda s: s.shift()).alias(&quot;shift_map&quot;),
        pl.col(&quot;values&quot;).shift().alias(&quot;shift_expression&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .clone()
        .lazy()
        .groupby([col(&quot;keys&quot;)])
        .agg([
            col(&quot;values&quot;)
                .apply(|s| Ok(s.shift(1)), GetOutput::default())
                .alias(&quot;shift_map&quot;),
            col(&quot;values&quot;).shift(1).alias(&quot;shift_expression&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (2, 3)
┌──────┬────────────┬──────────────────┐
│ keys ┆ shift_map  ┆ shift_expression │
│ ---  ┆ ---        ┆ ---              │
│ str  ┆ list[i64]  ┆ list[i64]        │
╞══════╪════════════╪══════════════════╡
│ a    ┆ [null, 10] ┆ [null, 10]       │
│ b    ┆ [null]     ┆ [null]           │
└──────┴────────────┴──────────────────┘
</code></pre>
<p>And observe, a valid result! 🎉</p>
<h2 id="apply-in-the-select-context"><a class="header" href="#apply-in-the-select-context"><code>apply</code> in the <code>select</code> context</a></h2>
<p>In the <code>select</code> context, the <code>apply</code> expression passes elements of the column to the python function.</p>
<p><em>Note that you are
now running python, this will be slow.</em></p>
<p>Let's go through some examples to see what to expect. We will continue with the <code>DataFrame</code> we defined at the start of
this section and show an example with the <code>apply</code> function and a counter example where we use the expression API to
achieve the same goals.</p>
<h3 id="adding-a-counter"><a class="header" href="#adding-a-counter">Adding a counter</a></h3>
<p>In this example we create a global <code>counter</code> and then add the integer <code>1</code> to the global state at every element processed.
Every iteration the result of the increment will be added to the element value.</p>
<blockquote>
<p>Note, this example isn't provided in Rust.  The reason is that the global <code>counter</code> value would lead to data races when this apply is evaluated in parallel.  It would be possible to wrap it in a <code>Mutex</code> to protect the variable, but that would be obscuring the point of the example.  This is a case where the Python Global Interpreter Lock's performance tradeoff provides some safety guarantees.</p>
</blockquote>
<pre><code class="language-python">counter = 0


def add_counter(val: int) -&gt; int:
    global counter
    counter += 1
    return counter + val


out = df.select(
    [
        pl.col(&quot;values&quot;).apply(add_counter).alias(&quot;solution_apply&quot;),
        (pl.col(&quot;values&quot;) + pl.arange(1, pl.count() + 1)).alias(&quot;solution_expr&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌────────────────┬───────────────┐
│ solution_apply ┆ solution_expr │
│ ---            ┆ ---           │
│ i64            ┆ i64           │
╞════════════════╪═══════════════╡
│ 11             ┆ 11            │
│ 9              ┆ 9             │
│ 4              ┆ 4             │
└────────────────┴───────────────┘
</code></pre>
<h3 id="combining-multiple-column-values"><a class="header" href="#combining-multiple-column-values">Combining multiple column values</a></h3>
<p>If we want to have access to values of different columns in a single <code>apply</code> function call, we can create <code>struct</code> data
type. This data type collects those columns as fields in the <code>struct</code>. So if we'd create a struct from the columns
<code>&quot;keys&quot;</code> and <code>&quot;values&quot;</code>, we would get the following struct elements:</p>
<pre><code class="language-python">[
    {&quot;keys&quot;: &quot;a&quot;, &quot;values&quot;: 10},
    {&quot;keys&quot;: &quot;a&quot;, &quot;values&quot;: 7},
    {&quot;keys&quot;: &quot;b&quot;, &quot;values&quot;: 1},
]
</code></pre>
<p>In Python, those would be passed as <code>dict</code> to the calling python function and can thus be indexed by <code>field: str</code>.  In rust, you'll get a <code>Series</code> with the <code>Struct</code> type. The fields of the struct can then be indexed and downcast.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">out = df.select(
    [
        pl.struct([&quot;keys&quot;, &quot;values&quot;]).apply(lambda x: len(x[&quot;keys&quot;]) + x[&quot;values&quot;]).alias(&quot;solution_apply&quot;),
        (pl.col(&quot;keys&quot;).str.lengths() + pl.col(&quot;values&quot;)).alias(&quot;solution_expr&quot;),
    ]
)
print(out)
</code></pre>
<pre><code class="language-rust noplayground">    let out = df
        .lazy()
        .select([
            // pack to struct to get access to multiple fields in a custom `apply/map`
            as_struct(&amp;[col(&quot;keys&quot;), col(&quot;values&quot;)])
                // we will compute the len(a) + b
                .apply(
                    |s| {
                        // downcast to struct
                        let ca = s.struct_()?;

                        // get the fields as Series
                        let s_a = &amp;ca.fields()[0];
                        let s_b = &amp;ca.fields()[1];

                        // downcast the `Series` to their known type
                        let ca_a = s_a.utf8()?;
                        let ca_b = s_b.i32()?;

                        // iterate both `ChunkedArrays`
                        let out: Int32Chunked = ca_a
                            .into_iter()
                            .zip(ca_b)
                            .map(|(opt_a, opt_b)| match (opt_a, opt_b) {
                                (Some(a), Some(b)) =&gt; Some(a.len() as i32 + b),
                                _ =&gt; None,
                            })
                            .collect();

                        Ok(out.into_series())
                    },
                    GetOutput::from_type(DataType::Int32),
                )
                .alias(&quot;solution_apply&quot;),
            (col(&quot;keys&quot;).str().count_match(&quot;.&quot;) + col(&quot;values&quot;)).alias(&quot;solution_expr&quot;),
        ])
        .collect()?;
    println!(&quot;{}&quot;, out);
</code></pre>
</div>
<pre><code class="language-text">shape: (3, 2)
┌────────────────┬───────────────┐
│ solution_apply ┆ solution_expr │
│ ---            ┆ ---           │
│ i64            ┆ i64           │
╞════════════════╪═══════════════╡
│ 11             ┆ 11            │
│ 8              ┆ 8             │
│ 2              ┆ 2             │
└────────────────┴───────────────┘
</code></pre>
<h3 id="return-types"><a class="header" href="#return-types">Return types?</a></h3>
<p>Custom python functions are black boxes for polars. We really don't know what kind of black arts you are doing, so we have
to infer and try our best to understand what you meant.</p>
<p>As a user it helps to understand what we do to better utilize custom functions.</p>
<p>The data type is automatically inferred. We do that by waiting for the first non-null value. That value will then be used
to determine the type of the <code>Series</code>.</p>
<p>The mapping of python types to polars data types is as follows:</p>
<ul>
<li><code>int</code> -&gt; <code>Int64</code></li>
<li><code>float</code> -&gt; <code>Float64</code></li>
<li><code>bool</code> -&gt; <code>Boolean</code></li>
<li><code>str</code> -&gt; <code>Utf8</code></li>
<li><code>list[tp]</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li>
<li><code>dict[str, [tp]]</code> -&gt; <code>struct</code></li>
<li><code>Any</code> -&gt; <code>object</code> (Prevent this at all times)</li>
</ul>
<p>Rust types map as follows:</p>
<ul>
<li><code>i32</code> or <code>i64</code> -&gt; <code>Int64</code></li>
<li><code>f32</code> or <code>f64</code> -&gt; <code>Float64</code></li>
<li><code>bool</code> -&gt; <code>Boolean</code></li>
<li><code>String</code> or <code>str</code> -&gt; <code>Utf8</code></li>
<li><code>Vec&lt;tp&gt;</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li>
</ul>
<pre><code class="language-python">import polars as pl

</code></pre>
<h1 id="expressions-1"><a class="header" href="#expressions-1">Expressions</a></h1>
<p><code>fn(Series) -&gt; Series</code></p>
<ul>
<li>Lazily evaluated
<ul>
<li>Can be optimized</li>
<li>Gives the library writer context and informed decision can be made</li>
</ul>
</li>
<li>Embarrassingly parallel</li>
<li>Context dependent
<ul>
<li>selection / projection -&gt; <code>Series</code> = <strong>COLUMN, LITERAL or VALUE</strong></li>
<li>aggregation -&gt; <code>Series</code> = <strong>GROUPS</strong></li>
</ul>
</li>
</ul>
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;A&quot;: [1, 2, 3, 4, 5],
        &quot;fruits&quot;: [&quot;banana&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;],
        &quot;B&quot;: [5, 4, 3, 2, 1],
        &quot;cars&quot;: [&quot;beetle&quot;, &quot;audi&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;beetle&quot;],
        &quot;optional&quot;: [28, 300, None, 2, -30],
    }
)
df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
fruits
</th>
<th>
B
</th>
<th>
cars
</th>
<th>
optional
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
"banana"
</td>
<td>
5
</td>
<td>
"beetle"
</td>
<td>
28
</td>
</tr>
<tr>
<td>
2
</td>
<td>
"banana"
</td>
<td>
4
</td>
<td>
"audi"
</td>
<td>
300
</td>
</tr>
<tr>
<td>
3
</td>
<td>
"apple"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
<td>
null
</td>
</tr>
<tr>
<td>
4
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
<td>
2
</td>
</tr>
<tr>
<td>
5
</td>
<td>
"banana"
</td>
<td>
1
</td>
<td>
"beetle"
</td>
<td>
-30
</td>
</tr>
</tbody>
</table>
</div>
<h1 id="selection-context-1"><a class="header" href="#selection-context-1">Selection context</a></h1>
<pre><code class="language-python"># We can select by name

(df.select([
    pl.col(&quot;A&quot;),
    &quot;B&quot;,      # the col part is inferred
    pl.lit(&quot;B&quot;),  # we must tell polars we mean the literal &quot;B&quot;
    pl.col(&quot;fruits&quot;),
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
B
</th>
<th>
literal
</th>
<th>
fruits
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
str
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
5
</td>
<td>
"B"
</td>
<td>
"banana"
</td>
</tr>
<tr>
<td>
2
</td>
<td>
4
</td>
<td>
"B"
</td>
<td>
"banana"
</td>
</tr>
<tr>
<td>
3
</td>
<td>
3
</td>
<td>
"B"
</td>
<td>
"apple"
</td>
</tr>
<tr>
<td>
4
</td>
<td>
2
</td>
<td>
"B"
</td>
<td>
"apple"
</td>
</tr>
<tr>
<td>
5
</td>
<td>
1
</td>
<td>
"B"
</td>
<td>
"banana"
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># you can select columns with a regex if it starts with '^' and ends with '$'

(df.select([
    pl.col(&quot;^A|B$&quot;).sum()
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
B
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
15
</td>
<td>
15
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># you can select multiple columns by name

(df.select([
    pl.col([&quot;A&quot;, &quot;B&quot;]).sum()
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
B
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
15
</td>
<td>
15
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We select everything in normal order
# Then we select everything in reversed order

(df.select([
    pl.all(),
    pl.all().reverse().suffix(&quot;_reverse&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
fruits
</th>
<th>
B
</th>
<th>
cars
</th>
<th>
optional
</th>
<th>
A_reverse
</th>
<th>
fruits_reverse
</th>
<th>
B_reverse
</th>
<th>
cars_reverse
</th>
<th>
optional_reverse
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
"banana"
</td>
<td>
5
</td>
<td>
"beetle"
</td>
<td>
28
</td>
<td>
5
</td>
<td>
"banana"
</td>
<td>
1
</td>
<td>
"beetle"
</td>
<td>
-30
</td>
</tr>
<tr>
<td>
2
</td>
<td>
"banana"
</td>
<td>
4
</td>
<td>
"audi"
</td>
<td>
300
</td>
<td>
4
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
<td>
2
</td>
</tr>
<tr>
<td>
3
</td>
<td>
"apple"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
<td>
null
</td>
<td>
3
</td>
<td>
"apple"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
<td>
null
</td>
</tr>
<tr>
<td>
4
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
<td>
2
</td>
<td>
2
</td>
<td>
"banana"
</td>
<td>
4
</td>
<td>
"audi"
</td>
<td>
300
</td>
</tr>
<tr>
<td>
5
</td>
<td>
"banana"
</td>
<td>
1
</td>
<td>
"beetle"
</td>
<td>
-30
</td>
<td>
1
</td>
<td>
"banana"
</td>
<td>
5
</td>
<td>
"beetle"
</td>
<td>
28
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># all expressions run in parallel
# single valued `Series` are broadcasted to the shape of the `DataFrame`

(df.select([
    pl.all(),
    pl.all().sum().suffix(&quot;_sum&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
fruits
</th>
<th>
B
</th>
<th>
cars
</th>
<th>
optional
</th>
<th>
A_sum
</th>
<th>
fruits_sum
</th>
<th>
B_sum
</th>
<th>
cars_sum
</th>
<th>
optional_sum
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
"banana"
</td>
<td>
5
</td>
<td>
"beetle"
</td>
<td>
28
</td>
<td>
15
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
300
</td>
</tr>
<tr>
<td>
2
</td>
<td>
"banana"
</td>
<td>
4
</td>
<td>
"audi"
</td>
<td>
300
</td>
<td>
15
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
300
</td>
</tr>
<tr>
<td>
3
</td>
<td>
"apple"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
300
</td>
</tr>
<tr>
<td>
4
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
<td>
2
</td>
<td>
15
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
300
</td>
</tr>
<tr>
<td>
5
</td>
<td>
"banana"
</td>
<td>
1
</td>
<td>
"beetle"
</td>
<td>
-30
</td>
<td>
15
</td>
<td>
null
</td>
<td>
15
</td>
<td>
null
</td>
<td>
300
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># there are `str` and `dt` namespaces for specialized functions

predicate = pl.col(&quot;fruits&quot;).str.contains(&quot;^b.*&quot;)

(df.select([
    predicate
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
</tr>
<tr>
<td>
bool
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
true
</td>
</tr>
<tr>
<td>
true
</td>
</tr>
<tr>
<td>
false
</td>
</tr>
<tr>
<td>
false
</td>
</tr>
<tr>
<td>
true
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># use the predicate to filter

df.filter(predicate)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
fruits
</th>
<th>
B
</th>
<th>
cars
</th>
<th>
optional
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
"banana"
</td>
<td>
5
</td>
<td>
"beetle"
</td>
<td>
28
</td>
</tr>
<tr>
<td>
2
</td>
<td>
"banana"
</td>
<td>
4
</td>
<td>
"audi"
</td>
<td>
300
</td>
</tr>
<tr>
<td>
5
</td>
<td>
"banana"
</td>
<td>
1
</td>
<td>
"beetle"
</td>
<td>
-30
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># predicate expressions can be used to filter

(df.select([
    pl.col(&quot;A&quot;).filter(pl.col(&quot;fruits&quot;).str.contains(&quot;^b.*&quot;)).sum(),
    (pl.col(&quot;B&quot;).filter(pl.col(&quot;cars&quot;).str.contains(&quot;^b.*&quot;)).sum() * pl.col(&quot;B&quot;).sum()).alias(&quot;some_compute()&quot;),
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
some_compute()
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
8
</td>
<td>
165
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We can do arithmetic on columns and (literal) values
# can be evaluated to 1 without programmer knowing

some_var = 1

(df.select([
    ((pl.col(&quot;A&quot;) / 124.0 * pl.col(&quot;B&quot;)) / pl.sum(&quot;B&quot;) * some_var).alias(&quot;computed&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
computed
</th>
</tr>
<tr>
<td>
f64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
0.0
</td>
</tr>
<tr>
<td>
0.0
</td>
</tr>
<tr>
<td>
0.0
</td>
</tr>
<tr>
<td>
0.0
</td>
</tr>
<tr>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We can combine columns by a predicate

(df.select([
    &quot;fruits&quot;,
    &quot;B&quot;,
    pl.when(pl.col(&quot;fruits&quot;) == &quot;banana&quot;).then(pl.col(&quot;B&quot;)).otherwise(-1).alias(&quot;b&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B
</th>
<th>
b
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"banana"
</td>
<td>
5
</td>
<td>
5
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
3
</td>
<td>
-1
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
2
</td>
<td>
-1
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
1
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We can combine columns by a fold operation on column level

(df.select([
    &quot;A&quot;,
    &quot;B&quot;,
    pl.fold(0, lambda a, b: a + b, [pl.col(&quot;A&quot;), &quot;B&quot;, pl.col(&quot;B&quot;)**2, pl.col(&quot;A&quot;) / 2.0]).alias(&quot;fold&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
A
</th>
<th>
B
</th>
<th>
fold
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
<td>
f64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
5
</td>
<td>
31
</td>
</tr>
<tr>
<td>
2
</td>
<td>
4
</td>
<td>
23
</td>
</tr>
<tr>
<td>
3
</td>
<td>
3
</td>
<td>
16
</td>
</tr>
<tr>
<td>
4
</td>
<td>
2
</td>
<td>
12
</td>
</tr>
<tr>
<td>
5
</td>
<td>
1
</td>
<td>
9
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># even combine all

(df.select([
    pl.arange(0, df.height).alias(&quot;idx&quot;),
    &quot;A&quot;,
    pl.col(&quot;A&quot;).shift().alias(&quot;A_shifted&quot;),
    pl.concat_str(pl.all(), &quot;-&quot;).alias(&quot;str_concat_1&quot;),  # prefer this
    pl.fold(pl.col(&quot;A&quot;), lambda a, b: a + &quot;-&quot; + b, pl.all().exclude(&quot;A&quot;)).alias(&quot;str_concat_2&quot;),  # over this (accidentally O(n^2))
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
idx
</th>
<th>
A
</th>
<th>
A_shifted
</th>
<th>
str_concat_1
</th>
<th>
str_concat_2
</th>
</tr>
<tr>
<td>
i64
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
str
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
0
</td>
<td>
1
</td>
<td>
null
</td>
<td>
"1-banana-5-beetle-28"
</td>
<td>
"1-banana-5-beetle-28"
</td>
</tr>
<tr>
<td>
1
</td>
<td>
2
</td>
<td>
1
</td>
<td>
"2-banana-4-audi-300"
</td>
<td>
"2-banana-4-audi-300"
</td>
</tr>
<tr>
<td>
2
</td>
<td>
3
</td>
<td>
2
</td>
<td>
null
</td>
<td>
null
</td>
</tr>
<tr>
<td>
3
</td>
<td>
4
</td>
<td>
3
</td>
<td>
"4-apple-2-beetle-2"
</td>
<td>
"4-apple-2-beetle-2"
</td>
</tr>
<tr>
<td>
4
</td>
<td>
5
</td>
<td>
4
</td>
<td>
"5-banana-1-beetle--30"
</td>
<td>
"5-banana-1-beetle--30"
</td>
</tr>
</tbody>
</table>
</div>
<h1 id="aggregation-context"><a class="header" href="#aggregation-context">Aggregation context</a></h1>
<ul>
<li>expressions are applied over groups instead of columns</li>
</ul>
<pre><code class="language-python"># we can still combine many expressions

(df.sort(&quot;cars&quot;).groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        pl.sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  # syntactic sugar for the first
        pl.first(&quot;fruits&quot;).alias(&quot;fruits_first&quot;),
        pl.count(&quot;A&quot;).alias(&quot;count&quot;),
        pl.col(&quot;cars&quot;).reverse()
    ]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B_sum
</th>
<th>
B_sum2
</th>
<th>
fruits_first
</th>
<th>
count
</th>
<th>
cars
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
u32
</td>
<td>
list
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
[beetle, beetle, audi]
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
[beetle, beetle]
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We can explode the list column &quot;cars&quot;

(df.sort(&quot;cars&quot;).groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        pl.sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  # syntactic sugar for the first
        pl.first(&quot;fruits&quot;).alias(&quot;fruits_first&quot;),
        pl.count(&quot;A&quot;).alias(&quot;count&quot;),
        pl.col(&quot;cars&quot;).reverse()
    ])).explode(&quot;cars&quot;)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B_sum
</th>
<th>
B_sum2
</th>
<th>
fruits_first
</th>
<th>
count
</th>
<th>
cars
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
u32
</td>
<td>
str
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"audi"
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python">(df.groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        pl.sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  # syntactic sugar for the first
        pl.first(&quot;fruits&quot;).alias(&quot;fruits_first&quot;),
        pl.count(),
        pl.col(&quot;B&quot;).shift().alias(&quot;B_shifted&quot;)
    ])
 .explode(&quot;B_shifted&quot;)
)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B_sum
</th>
<th>
B_sum2
</th>
<th>
fruits_first
</th>
<th>
count
</th>
<th>
B_shifted
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
u32
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
null
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
null
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
5
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
4
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># We can explode the list column &quot;cars&quot;

(df.sort(&quot;cars&quot;).groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).sum(),
        pl.sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  # syntactic sugar for the first
        pl.first(&quot;fruits&quot;).alias(&quot;fruits_first&quot;),
        pl.count(&quot;A&quot;).alias(&quot;count&quot;),
        pl.col(&quot;cars&quot;).reverse()
    ])).explode(&quot;cars&quot;)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B_sum
</th>
<th>
B_sum2
</th>
<th>
fruits_first
</th>
<th>
count
</th>
<th>
cars
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
str
</td>
<td>
u32
</td>
<td>
str
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
5
</td>
<td>
5
</td>
<td>
"apple"
</td>
<td>
2
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"beetle"
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
10
</td>
<td>
10
</td>
<td>
"banana"
</td>
<td>
3
</td>
<td>
"audi"
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># we can also get the list of the groups

(df.groupby(&quot;fruits&quot;)
    .agg([
         pl.col(&quot;B&quot;).shift().alias(&quot;shift_B&quot;),
         pl.col(&quot;B&quot;).reverse().alias(&quot;rev_B&quot;),
    ]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
shift_B
</th>
<th>
rev_B
</th>
</tr>
<tr>
<td>
str
</td>
<td>
list
</td>
<td>
list
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
[null, 3]
</td>
<td>
[2, 3]
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
[null, 5, 4]
</td>
<td>
[1, 4, 5]
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># we can do predicates in the groupby as well

(df.groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).filter(pl.col(&quot;B&quot;) &gt; 1).list().keep_name(),
    ]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B
</th>
</tr>
<tr>
<td>
str
</td>
<td>
list
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"banana"
</td>
<td>
[5, 4]
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
[3, 2]
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># and sum only by the values where the predicates are true

(df.groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).filter(pl.col(&quot;B&quot;) &gt; 1).mean(),
    ]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B_mean
</th>
</tr>
<tr>
<td>
str
</td>
<td>
f64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"banana"
</td>
<td>
4.5
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
2.5
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># Another example

(df.groupby(&quot;fruits&quot;)
    .agg([
        pl.col(&quot;B&quot;).shift_and_fill(1, fill_value=0).alias(&quot;shifted&quot;),
        pl.col(&quot;B&quot;).shift_and_fill(1, fill_value=0).sum().alias(&quot;shifted_sum&quot;),
    ]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
shifted
</th>
<th>
shifted_sum
</th>
</tr>
<tr>
<td>
str
</td>
<td>
list
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
[0, 3]
</td>
<td>
3
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
[0, 5, 4]
</td>
<td>
9
</td>
</tr>
</tbody>
</table>
</div>
<h1 id="window-functions"><a class="header" href="#window-functions">Window functions!</a></h1>
<ul>
<li>Expression with superpowers.</li>
<li>Aggregation in selection context</li>
</ul>
<pre><code class="language-python">pl.col(&quot;foo&quot;).aggregation_expression(..).over(&quot;column_used_to_group&quot;)
</code></pre>
<pre><code class="language-python"># groupby 2 different columns

(df.select([
    &quot;fruits&quot;,
    &quot;cars&quot;,
    &quot;B&quot;,
    pl.col(&quot;B&quot;).sum().over(&quot;fruits&quot;).alias(&quot;B_sum_by_fruits&quot;),
    pl.col(&quot;B&quot;).sum().over(&quot;cars&quot;).alias(&quot;B_sum_by_cars&quot;),
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
cars
</th>
<th>
B
</th>
<th>
B_sum_by_fruits
</th>
<th>
B_sum_by_cars
</th>
</tr>
<tr>
<td>
str
</td>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
"beetle"
</td>
<td>
3
</td>
<td>
5
</td>
<td>
11
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
"beetle"
</td>
<td>
2
</td>
<td>
5
</td>
<td>
11
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
"beetle"
</td>
<td>
5
</td>
<td>
10
</td>
<td>
11
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
"audi"
</td>
<td>
4
</td>
<td>
10
</td>
<td>
4
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
"beetle"
</td>
<td>
1
</td>
<td>
10
</td>
<td>
11
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># reverse B by groups and show the results in original DF

(df.select([
    &quot;fruits&quot;,
    &quot;B&quot;,
    pl.col(&quot;B&quot;).reverse().over(&quot;fruits&quot;).alias(&quot;B_reversed_by_fruits&quot;)
]))
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B
</th>
<th>
B_reversed_by_fruits
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
3
</td>
<td>
2
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
5
</td>
<td>
1
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
1
</td>
<td>
5
</td>
</tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># Lag a column within &quot;fruits&quot;

(df.select([
    &quot;fruits&quot;,
    &quot;B&quot;,
    pl.col(&quot;B&quot;).shift().over(&quot;fruits&quot;).alias(&quot;lag_B_by_fruits&quot;)
]))

</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1 "class="dataframe ">
<thead>
<tr>
<th>
fruits
</th>
<th>
B
</th>
<th>
lag_B_by_fruits
</th>
</tr>
<tr>
<td>
str
</td>
<td>
i64
</td>
<td>
i64
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
"apple"
</td>
<td>
3
</td>
<td>
null
</td>
</tr>
<tr>
<td>
"apple"
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
5
</td>
<td>
null
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
4
</td>
<td>
5
</td>
</tr>
<tr>
<td>
"banana"
</td>
<td>
1
</td>
<td>
4
</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>Note: To use this notebook, you must first install the <a href="https://github.com/google/evcxr/blob/main/evcxr_jupyter/README.md">Rust <code>excvr</code> jupyter kernel</a>.  Also, note that <code>clone()</code> is used fairly often in the examples.  This is because we tend to create one dataset for multiple examples.  When this dataset is used, the rust ownership system will <code>move</code> that dataframe, which will make it unavailable to later examples.  By <code>clone</code>ing, we can keep using it over and over.</p>
</blockquote>
<pre><code class="language-python">:dep polars = { version = &quot;0.23.2&quot;, features = [&quot;lazy&quot;, &quot;csv-file&quot;, &quot;strings&quot;, &quot;temporal&quot;, &quot;dtype-duration&quot;, &quot;dtype-categorical&quot;, &quot;concat_str&quot;, &quot;list&quot;, &quot;list_eval&quot;, &quot;rank&quot;, &quot;lazy_regex&quot;]}
:dep color-eyre = {version = &quot;0.6.2&quot;}
:dep rand = {version = &quot;0.8.5&quot;}
:dep reqwest = { version = &quot;0.11.11&quot;, features = [&quot;blocking&quot;]}

use color_eyre::{Result};
use polars::prelude::*;
</code></pre>
<h1 id="expressions-2"><a class="header" href="#expressions-2">Expressions</a></h1>
<p><code>fn(Series) -&gt; Series</code></p>
<ul>
<li>Lazily evaluated
<ul>
<li>Can be optimized</li>
<li>Gives the library writer context and informed decisions can be made</li>
</ul>
</li>
<li>Embarrassingly parallel</li>
<li>Context dependent
<ul>
<li>selection/projection -&gt; <code>Series</code> = <em>COLUMN, LITERAL, or VALUE</em></li>
<li>aggregation -&gt; <code>Series</code> = <em>GROUPS</em></li>
</ul>
</li>
</ul>
<pre><code class="language-python">let df = df! [
    &quot;A&quot;        =&gt; [1, 2, 3, 4, 5],
    &quot;fruits&quot;   =&gt; [&quot;banana&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;],
    &quot;B&quot;        =&gt; [5, 4, 3, 2, 1],
    &quot;cars&quot;     =&gt; [&quot;beetle&quot;, &quot;audi&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;beetle&quot;],
    &quot;optional&quot; =&gt; [Some(28), Some(300), None, Some(2), Some(-30)],
]?;
df
</code></pre>
<pre><code>shape: (5, 5)
┌─────┬────────┬─────┬────────┬──────────┐
│ A   ┆ fruits ┆ B   ┆ cars   ┆ optional │
│ --- ┆ ---    ┆ --- ┆ ---    ┆ ---      │
│ i32 ┆ str    ┆ i32 ┆ str    ┆ i32      │
╞═════╪════════╪═════╪════════╪══════════╡
│ 1   ┆ banana ┆ 5   ┆ beetle ┆ 28       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ banana ┆ 4   ┆ audi   ┆ 300      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ apple  ┆ 3   ┆ beetle ┆ null     │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ apple  ┆ 2   ┆ beetle ┆ 2        │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ banana ┆ 1   ┆ beetle ┆ -30      │
└─────┴────────┴─────┴────────┴──────────┘
</code></pre>
<h1 id="selection-context-2"><a class="header" href="#selection-context-2">Selection context</a></h1>
<pre><code class="language-python">// We can select by name
// We'll be re-using the dataframe a bunch, so we'll clone copies as we go.
df.clone().lazy().select([
    col(&quot;A&quot;),
    col(&quot;B&quot;),
    lit(&quot;B&quot;),  // we must tell polars we mean the literal &quot;B&quot;
    col(&quot;fruits&quot;),
]).collect()?
</code></pre>
<pre><code>shape: (5, 4)
┌─────┬─────┬─────────┬────────┐
│ A   ┆ B   ┆ literal ┆ fruits │
│ --- ┆ --- ┆ ---     ┆ ---    │
│ i32 ┆ i32 ┆ str     ┆ str    │
╞═════╪═════╪═════════╪════════╡
│ 1   ┆ 5   ┆ B       ┆ banana │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ 2   ┆ 4   ┆ B       ┆ banana │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ 3   ┆ 3   ┆ B       ┆ apple  │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ 4   ┆ 2   ┆ B       ┆ apple  │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ 5   ┆ 1   ┆ B       ┆ banana │
└─────┴─────┴─────────┴────────┘
</code></pre>
<pre><code class="language-python">// you can select columns with a regex if it starts with '^' and ends with '$'

df.clone().lazy().select([
    col(&quot;^A|B$&quot;).sum()
]).collect()?
</code></pre>
<pre><code>shape: (1, 2)
┌─────┬─────┐
│ A   ┆ B   │
│ --- ┆ --- │
│ i32 ┆ i32 │
╞═════╪═════╡
│ 15  ┆ 15  │
└─────┴─────┘
</code></pre>
<pre><code class="language-python">// you can select multiple columns by name

df.clone().lazy().select([
    cols([&quot;A&quot;, &quot;B&quot;]).sum()
]).collect()?
</code></pre>
<pre><code>shape: (1, 2)
┌─────┬─────┐
│ A   ┆ B   │
│ --- ┆ --- │
│ i32 ┆ i32 │
╞═════╪═════╡
│ 15  ┆ 15  │
└─────┴─────┘
</code></pre>
<pre><code class="language-python">// We select everything in normal order
// Then we select everything in reversed order

df.clone().lazy().select([
    all(),
    all().reverse().suffix(&quot;_reverse&quot;)
]).collect()?
</code></pre>
<pre><code>shape: (5, 10)
┌─────┬────────┬─────┬────────┬─────┬────────────────┬───────────┬──────────────┬──────┐
│ A   ┆ fruits ┆ B   ┆ cars   ┆ ... ┆ fruits_reverse ┆ B_reverse ┆ cars_reverse ┆ opti │
│ --- ┆ ---    ┆ --- ┆ ---    ┆     ┆ ---            ┆ ---       ┆ ---          ┆ onal │
│ i32 ┆ str    ┆ i32 ┆ str    ┆     ┆ str            ┆ i32       ┆ str          ┆ _rev │
│     ┆        ┆     ┆        ┆     ┆                ┆           ┆              ┆ erse │
│     ┆        ┆     ┆        ┆     ┆                ┆           ┆              ┆ ---  │
│     ┆        ┆     ┆        ┆     ┆                ┆           ┆              ┆ i32  │
╞═════╪════════╪═════╪════════╪═════╪════════════════╪═══════════╪══════════════╪══════╡
│ 1   ┆ banana ┆ 5   ┆ beetle ┆ ... ┆ banana         ┆ 1         ┆ beetle       ┆ -30  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 2   ┆ banana ┆ 4   ┆ audi   ┆ ... ┆ apple          ┆ 2         ┆ beetle       ┆ 2    │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 3   ┆ apple  ┆ 3   ┆ beetle ┆ ... ┆ apple          ┆ 3         ┆ beetle       ┆ null │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 4   ┆ apple  ┆ 2   ┆ beetle ┆ ... ┆ banana         ┆ 4         ┆ audi         ┆ 300  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 5   ┆ banana ┆ 1   ┆ beetle ┆ ... ┆ banana         ┆ 5         ┆ beetle       ┆ 28   │
└─────┴────────┴─────┴────────┴─────┴────────────────┴───────────┴──────────────┴──────┘
</code></pre>
<pre><code class="language-python">// all expressions run in parallel
// single valued `Series` are broadcasted to the shape of the `DataFrame`

df.clone().lazy().select([
    all(),
    all().sum().suffix(&quot;_sum&quot;)
]).collect()?
</code></pre>
<pre><code>shape: (5, 10)
┌─────┬────────┬─────┬────────┬─────┬────────────┬───────┬──────────┬──────────────┐
│ A   ┆ fruits ┆ B   ┆ cars   ┆ ... ┆ fruits_sum ┆ B_sum ┆ cars_sum ┆ optional_sum │
│ --- ┆ ---    ┆ --- ┆ ---    ┆     ┆ ---        ┆ ---   ┆ ---      ┆ ---          │
│ i32 ┆ str    ┆ i32 ┆ str    ┆     ┆ str        ┆ i32   ┆ str      ┆ i32          │
╞═════╪════════╪═════╪════════╪═════╪════════════╪═══════╪══════════╪══════════════╡
│ 1   ┆ banana ┆ 5   ┆ beetle ┆ ... ┆ null       ┆ 15    ┆ null     ┆ 300          │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ banana ┆ 4   ┆ audi   ┆ ... ┆ null       ┆ 15    ┆ null     ┆ 300          │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ apple  ┆ 3   ┆ beetle ┆ ... ┆ null       ┆ 15    ┆ null     ┆ 300          │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ apple  ┆ 2   ┆ beetle ┆ ... ┆ null       ┆ 15    ┆ null     ┆ 300          │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ banana ┆ 1   ┆ beetle ┆ ... ┆ null       ┆ 15    ┆ null     ┆ 300          │
└─────┴────────┴─────┴────────┴─────┴────────────┴───────┴──────────┴──────────────┘
</code></pre>
<pre><code class="language-python">// there are `str` and `dt` namespaces for specialized functions

let predicate = col(&quot;fruits&quot;).str().contains(&quot;^b.*&quot;);

df.clone().lazy().select([
    predicate
]).collect()?
</code></pre>
<pre><code>shape: (5, 1)
┌────────┐
│ fruits │
│ ---    │
│ bool   │
╞════════╡
│ true   │
├╌╌╌╌╌╌╌╌┤
│ true   │
├╌╌╌╌╌╌╌╌┤
│ false  │
├╌╌╌╌╌╌╌╌┤
│ false  │
├╌╌╌╌╌╌╌╌┤
│ true   │
└────────┘
</code></pre>
<pre><code class="language-python">// use the predicate to filter
let predicate = col(&quot;fruits&quot;).str().contains(&quot;^b.*&quot;);
df.clone().lazy().filter(predicate).collect()?
</code></pre>
<pre><code>shape: (3, 5)
┌─────┬────────┬─────┬────────┬──────────┐
│ A   ┆ fruits ┆ B   ┆ cars   ┆ optional │
│ --- ┆ ---    ┆ --- ┆ ---    ┆ ---      │
│ i32 ┆ str    ┆ i32 ┆ str    ┆ i32      │
╞═════╪════════╪═════╪════════╪══════════╡
│ 1   ┆ banana ┆ 5   ┆ beetle ┆ 28       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ banana ┆ 4   ┆ audi   ┆ 300      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ banana ┆ 1   ┆ beetle ┆ -30      │
└─────┴────────┴─────┴────────┴──────────┘
</code></pre>
<pre><code class="language-python">// predicate expressions can be used to filter

df.clone().lazy().select([
    col(&quot;A&quot;).filter(col(&quot;fruits&quot;).str().contains(&quot;^b.*&quot;)).sum(),
    (col(&quot;B&quot;).filter(col(&quot;cars&quot;).str().contains(&quot;^b.*&quot;)).sum() * col(&quot;B&quot;).sum()).alias(&quot;some_compute()&quot;),
]).collect()?
</code></pre>
<pre><code>shape: (1, 2)
┌─────┬────────────────┐
│ A   ┆ some_compute() │
│ --- ┆ ---            │
│ i32 ┆ i32            │
╞═════╪════════════════╡
│ 8   ┆ 165            │
└─────┴────────────────┘
</code></pre>
<pre><code class="language-python">// We can do arithmetic on columns and (literal) values
// can be evaluated to 1 without programmer knowing

let some_var = 1;

df.clone().lazy().select([
    ((col(&quot;A&quot;) / lit(124.0) * col(&quot;B&quot;)) / sum(&quot;B&quot;) * lit(some_var)).alias(&quot;computed&quot;)
]).collect()?
</code></pre>
<pre><code>shape: (5, 1)
┌──────────┐
│ computed │
│ ---      │
│ f64      │
╞══════════╡
│ 0.002688 │
├╌╌╌╌╌╌╌╌╌╌┤
│ 0.004301 │
├╌╌╌╌╌╌╌╌╌╌┤
│ 0.004839 │
├╌╌╌╌╌╌╌╌╌╌┤
│ 0.004301 │
├╌╌╌╌╌╌╌╌╌╌┤
│ 0.002688 │
└──────────┘
</code></pre>
<pre><code class="language-python">// We can combine columns by a predicate
// This doesn't work.  It seems like the condition always evaluates to true
df.clone().lazy().select([
    col(&quot;fruits&quot;),
    col(&quot;B&quot;),
    when(col(&quot;fruits&quot;) == lit(&quot;banana&quot;)).then(col(&quot;B&quot;)).otherwise(-1).alias(&quot;b when not banana&quot;)
]).collect()?
</code></pre>
<pre><code>shape: (5, 3)
┌────────┬─────┬───────────────────┐
│ fruits ┆ B   ┆ b when not banana │
│ ---    ┆ --- ┆ ---               │
│ str    ┆ i32 ┆ i32               │
╞════════╪═════╪═══════════════════╡
│ banana ┆ 5   ┆ -1                │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 4   ┆ -1                │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 3   ┆ -1                │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 2   ┆ -1                │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 1   ┆ -1                │
└────────┴─────┴───────────────────┘
</code></pre>
<pre><code class="language-python">// We can combine columns by a fold operation on column level

df.clone().lazy().select([
    col(&quot;A&quot;),
    col(&quot;B&quot;),
    fold_exprs(lit(0), |a, b| Ok(&amp;a + &amp;b), [
        col(&quot;A&quot;),
        lit(&quot;B&quot;),
        col(&quot;B&quot;).pow(lit(2)),
        col(&quot;A&quot;) / lit(2.0)
    ]).alias(&quot;fold&quot;)
]).collect()?
</code></pre>
<pre><code>shape: (5, 3)
┌─────┬─────┬───────────┐
│ A   ┆ B   ┆ fold      │
│ --- ┆ --- ┆ ---       │
│ i32 ┆ i32 ┆ str       │
╞═════╪═════╪═══════════╡
│ 1   ┆ 5   ┆ 1B25.00.5 │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 4   ┆ 2B16.01.0 │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 3   ┆ 3B9.01.5  │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 2   ┆ 4B4.02.0  │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ 1   ┆ 5B1.02.5  │
└─────┴─────┴───────────┘
</code></pre>
<pre><code class="language-python">// even combine all
use std::convert::TryInto;
let height: i32 = df.height().try_into()?;
df.clone().lazy().select([
    range(0i32, height).alias(&quot;idx&quot;),
    col(&quot;A&quot;),
    col(&quot;A&quot;).shift(1).alias(&quot;A_shifted&quot;),
    concat_str([all()], &quot;&quot;).alias(&quot;str_concat_1&quot;),  // prefer this
    fold_exprs(col(&quot;A&quot;), |a, b| Ok(a + b), [all().exclude([&quot;A&quot;])]).alias(&quot;str_concat_2&quot;), // over this (accidentally O(n^2))
]).collect()?
</code></pre>
<pre><code>shape: (5, 5)
┌─────┬─────┬───────────┬───────────────────┬───────────────────┐
│ idx ┆ A   ┆ A_shifted ┆ str_concat_1      ┆ str_concat_2      │
│ --- ┆ --- ┆ ---       ┆ ---               ┆ ---               │
│ i32 ┆ i32 ┆ i32       ┆ str               ┆ str               │
╞═════╪═════╪═══════════╪═══════════════════╪═══════════════════╡
│ 0   ┆ 1   ┆ null      ┆ 1banana5beetle28  ┆ 1banana5beetle28  │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ 2   ┆ 1         ┆ 2banana4audi300   ┆ 2banana4audi300   │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ 3   ┆ 2         ┆ null              ┆ null              │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆ 4   ┆ 3         ┆ 4apple2beetle2    ┆ 4apple2beetle2    │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ 5   ┆ 4         ┆ 5banana1beetle-30 ┆ 5banana1beetle-30 │
└─────┴─────┴───────────┴───────────────────┴───────────────────┘
</code></pre>
<h1 id="aggregation-context-1"><a class="header" href="#aggregation-context-1">Aggregation context</a></h1>
<ul>
<li>expressions are applied over groups instead of columns</li>
</ul>
<pre><code class="language-python">// we can still combine many expressions

df.clone().lazy().sort(&quot;cars&quot;, SortOptions::default()).groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  // syntactic sugar for the first
        col(&quot;fruits&quot;).first().alias(&quot;fruits_first&quot;),
        col(&quot;A&quot;).count().alias(&quot;count&quot;),
        col(&quot;cars&quot;).reverse()
    ]).collect()?
</code></pre>
<pre><code>shape: (2, 6)
┌────────┬───────┬────────┬──────────────┬───────┬──────────────────────────────┐
│ fruits ┆ B_sum ┆ B_sum2 ┆ fruits_first ┆ count ┆ cars                         │
│ ---    ┆ ---   ┆ ---    ┆ ---          ┆ ---   ┆ ---                          │
│ str    ┆ i32   ┆ i32    ┆ str          ┆ u32   ┆ list[str]                    │
╞════════╪═══════╪════════╪══════════════╪═══════╪══════════════════════════════╡
│ apple  ┆ 5     ┆ 5      ┆ apple        ┆ 2     ┆ [&quot;beetle&quot;, &quot;beetle&quot;]         │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ 10     ┆ banana       ┆ 3     ┆ [&quot;beetle&quot;, &quot;beetle&quot;, &quot;audi&quot;] │
└────────┴───────┴────────┴──────────────┴───────┴──────────────────────────────┘
</code></pre>
<pre><code class="language-python">// We can explode the list column &quot;cars&quot;

df.clone().lazy()
    .sort(&quot;cars&quot;, SortOptions { descending: false, nulls_last: false })
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        sum(&quot;B&quot;).alias(&quot;B_sum2&quot;),  // syntactic sugar for the first
        col(&quot;fruits&quot;).first().alias(&quot;fruits_first&quot;),
        col(&quot;A&quot;).count().alias(&quot;count&quot;),
        col(&quot;cars&quot;).reverse()
    ])
    .explode([&quot;cars&quot;])
    .collect()?
</code></pre>
<pre><code>shape: (5, 6)
┌────────┬───────┬────────┬──────────────┬───────┬────────┐
│ fruits ┆ B_sum ┆ B_sum2 ┆ fruits_first ┆ count ┆ cars   │
│ ---    ┆ ---   ┆ ---    ┆ ---          ┆ ---   ┆ ---    │
│ str    ┆ i32   ┆ i32    ┆ str          ┆ u32   ┆ str    │
╞════════╪═══════╪════════╪══════════════╪═══════╪════════╡
│ banana ┆ 10    ┆ 10     ┆ banana       ┆ 3     ┆ beetle │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ 10     ┆ banana       ┆ 3     ┆ beetle │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ 10     ┆ banana       ┆ 3     ┆ audi   │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ apple  ┆ 5     ┆ 5      ┆ apple        ┆ 2     ┆ beetle │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┤
│ apple  ┆ 5     ┆ 5      ┆ apple        ┆ 2     ┆ beetle │
└────────┴───────┴────────┴──────────────┴───────┴────────┘
</code></pre>
<pre><code class="language-python">df.clone().lazy()
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).sum().alias(&quot;B_sum&quot;),
        col(&quot;fruits&quot;).first().alias(&quot;fruits_first&quot;),
        count(),
        col(&quot;B&quot;).shift(1).alias(&quot;B_shifted&quot;)
    ])
    .explode([&quot;B_shifted&quot;])
    .collect()?
</code></pre>
<pre><code>shape: (5, 5)
┌────────┬───────┬──────────────┬───────┬───────────┐
│ fruits ┆ B_sum ┆ fruits_first ┆ count ┆ B_shifted │
│ ---    ┆ ---   ┆ ---          ┆ ---   ┆ ---       │
│ str    ┆ i32   ┆ str          ┆ u32   ┆ i32       │
╞════════╪═══════╪══════════════╪═══════╪═══════════╡
│ apple  ┆ 5     ┆ apple        ┆ 2     ┆ null      │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 5     ┆ apple        ┆ 2     ┆ 3         │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ banana       ┆ 3     ┆ null      │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ banana       ┆ 3     ┆ 5         │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 10    ┆ banana       ┆ 3     ┆ 4         │
└────────┴───────┴──────────────┴───────┴───────────┘
</code></pre>
<pre><code class="language-python">// we can also get the list of the groups

df.clone().lazy()
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).shift(1).alias(&quot;shift_B&quot;),
        col(&quot;B&quot;).reverse().alias(&quot;rev_B&quot;),
    ])
    .collect()?
</code></pre>
<pre><code>shape: (2, 3)
┌────────┬──────────────┬───────────┐
│ fruits ┆ shift_B      ┆ rev_B     │
│ ---    ┆ ---          ┆ ---       │
│ str    ┆ list[i32]    ┆ list[i32] │
╞════════╪══════════════╪═══════════╡
│ banana ┆ [null, 5, 4] ┆ [1, 4, 5] │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ [null, 3]    ┆ [2, 3]    │
└────────┴──────────────┴───────────┘
</code></pre>
<pre><code class="language-python">// we can do predicates in the groupby as well

df.clone().lazy()
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).filter(col(&quot;B&quot;).gt(lit(1))).list().keep_name(),
    ])
    .collect()?
</code></pre>
<pre><code>shape: (2, 2)
┌────────┬───────────┐
│ fruits ┆ B         │
│ ---    ┆ ---       │
│ str    ┆ list[i32] │
╞════════╪═══════════╡
│ apple  ┆ [3, 2]    │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ [5, 4]    │
└────────┴───────────┘
</code></pre>
<pre><code class="language-python">// and sum only by the values where the predicates are true

df.clone().lazy()
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).filter(col(&quot;B&quot;).gt(lit(1))).mean(),
    ])
    .collect()?
</code></pre>
<pre><code>shape: (2, 2)
┌────────┬─────┐
│ fruits ┆ B   │
│ ---    ┆ --- │
│ str    ┆ f64 │
╞════════╪═════╡
│ apple  ┆ 2.5 │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┤
│ banana ┆ 4.5 │
└────────┴─────┘
</code></pre>
<pre><code class="language-python">// Another example

df.clone().lazy()
    .groupby([&quot;fruits&quot;])
    .agg([
        col(&quot;B&quot;).shift_and_fill(1, 0).alias(&quot;shifted&quot;),
        col(&quot;B&quot;).shift_and_fill(1, 0).sum().alias(&quot;shifted_sum&quot;),
    ])
    .collect()?
</code></pre>
<pre><code>shape: (2, 3)
┌────────┬───────────┬─────────────┐
│ fruits ┆ shifted   ┆ shifted_sum │
│ ---    ┆ ---       ┆ ---         │
│ str    ┆ list[i32] ┆ i32         │
╞════════╪═══════════╪═════════════╡
│ apple  ┆ [0, 3]    ┆ 3           │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ [0, 5, 4] ┆ 9           │
└────────┴───────────┴─────────────┘
</code></pre>
<h1 id="window-functions-1"><a class="header" href="#window-functions-1">Window functions!</a></h1>
<ul>
<li>Expression with superpowers.</li>
<li>Aggregation in selection context</li>
</ul>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>col(&quot;foo&quot;).aggregation_expression(..).over(&quot;column_used_to_group&quot;)
<span class="boring">}
</span></code></pre></pre>
<pre><code class="language-python">// groupby 2 different columns

df.clone().lazy()
    .select([
        col(&quot;fruits&quot;),
        col(&quot;cars&quot;),
        col(&quot;B&quot;),
        col(&quot;B&quot;).sum().over([&quot;fruits&quot;]).alias(&quot;B_sum_by_fruits&quot;),
        col(&quot;B&quot;).sum().over([&quot;cars&quot;]).alias(&quot;B_sum_by_cars&quot;),
    ])
    .collect()?
</code></pre>
<pre><code>shape: (5, 5)
┌────────┬────────┬─────┬─────────────────┬───────────────┐
│ fruits ┆ cars   ┆ B   ┆ B_sum_by_fruits ┆ B_sum_by_cars │
│ ---    ┆ ---    ┆ --- ┆ ---             ┆ ---           │
│ str    ┆ str    ┆ i32 ┆ i32             ┆ i32           │
╞════════╪════════╪═════╪═════════════════╪═══════════════╡
│ banana ┆ beetle ┆ 5   ┆ 10              ┆ 11            │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ audi   ┆ 4   ┆ 10              ┆ 4             │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ beetle ┆ 3   ┆ 5               ┆ 11            │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ beetle ┆ 2   ┆ 5               ┆ 11            │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ beetle ┆ 1   ┆ 10              ┆ 11            │
└────────┴────────┴─────┴─────────────────┴───────────────┘
</code></pre>
<pre><code class="language-python">// reverse B by groups and show the results in original DF

df.clone().lazy()
    .select([
        col(&quot;fruits&quot;),
        col(&quot;B&quot;),
        col(&quot;B&quot;).reverse().over([&quot;fruits&quot;]).alias(&quot;B_reversed_by_fruits&quot;)
    ])
    .collect()?
</code></pre>
<pre><code>shape: (5, 3)
┌────────┬─────┬──────────────────────┐
│ fruits ┆ B   ┆ B_reversed_by_fruits │
│ ---    ┆ --- ┆ ---                  │
│ str    ┆ i32 ┆ i32                  │
╞════════╪═════╪══════════════════════╡
│ banana ┆ 5   ┆ 1                    │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 4   ┆ 4                    │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 3   ┆ 2                    │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 2   ┆ 3                    │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 1   ┆ 5                    │
└────────┴─────┴──────────────────────┘
</code></pre>
<pre><code class="language-python">// Lag a column within &quot;fruits&quot;

df.clone().lazy()
    .select([
        col(&quot;fruits&quot;),
        col(&quot;B&quot;),
        col(&quot;B&quot;).shift(1).over([&quot;fruits&quot;]).alias(&quot;lag_B_by_fruits&quot;)
    ])
    .collect()?
</code></pre>
<pre><code>shape: (5, 3)
┌────────┬─────┬─────────────────┐
│ fruits ┆ B   ┆ lag_B_by_fruits │
│ ---    ┆ --- ┆ ---             │
│ str    ┆ i32 ┆ i32             │
╞════════╪═════╪═════════════════╡
│ banana ┆ 5   ┆ null            │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 4   ┆ 5               │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 3   ┆ null            │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ apple  ┆ 2   ┆ 3               │
├╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ banana ┆ 1   ┆ 4               │
└────────┴─────┴─────────────────┘
</code></pre>
<pre><code class="language-python">
</code></pre>
<h1 id="expression-api"><a class="header" href="#expression-api">Expression API</a></h1>
<p>The full list of possible expressions is available on the <code>Expr</code>
class definition in the reference guide.</p>
<ul>
<li><a href="https://pola-rs.github.io/polars/py-polars/html/reference/expressions/index.html">Python</a></li>
<li><a href="https://docs.rs/polars/latest/polars/#expressions">Rust</a></li>
<li><a href="https://pola-rs.github.io/nodejs-polars/interfaces/lazy_expr.Expr.html">JavaScript</a></li>
</ul>
<h1 id="video-introduction"><a class="header" href="#video-introduction">Video Introduction</a></h1>
<p>Don't enjoy reading? Take a look at this introduction video on <code>Polars</code> and its expressions.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/iwGIuGk5nCE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h1 id="lazy-api-1"><a class="header" href="#lazy-api-1">Lazy API</a></h1>
<p>With the lazy API Polars doesn't run each query line-by-line but instead processes the full query end-to-end. To get the most out of Polars it is important that you use the lazy API because:</p>
<ul>
<li>the lazy API allows Polars to apply automatic query optimization with the query optimizer</li>
<li>the lazy API allows you to work with larger than memory datasets using streaming</li>
<li>the lazy API can catch schema errors before processing the data</li>
</ul>
<p>The pages in this section cover:</p>
<ul>
<li><a href="lazy-api/lazy-query-create.html">Using the lazy API</a></li>
<li><a href="lazy-api/lazy-schema.html">Schema in the lazy API</a></li>
<li><a href="lazy-api/lazy-query-plan.html">Understanding the query plan</a></li>
<li><a href="lazy-api/lazy-query-execution.html">Executing lazy queries</a></li>
<li><a href="lazy-api/streaming.html">Streaming larger-than-memory datasets</a></li>
</ul>
<h2 id="dataset"><a class="header" href="#dataset">Dataset</a></h2>
<p>To demonstrate the lazy <code>Polars</code> capabilities we'll explore a medium-large
dataset of usernames.</p>
<p>The <a href="https://www.reddit.com/r/datasets/comments/9i8s5j/dataset_metadata_for_69_million_reddit_users_in/">Reddit usernames dataset</a>
contains over 69 million rows with data on Reddit users.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

dataset = pl.read_csv(f&quot;{DATA_DIR}/reddit.csv&quot;, n_rows=10)
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/dataset.txt}}
</code></pre>
<h1 id="using-the-lazy-api"><a class="header" href="#using-the-lazy-api">Using the lazy API</a></h1>
<p>Here we see how to use the lazy API starting from either a file or an existing <code>DataFrame</code>.</p>
<h2 id="using-the-lazy-api-from-a-file"><a class="header" href="#using-the-lazy-api-from-a-file">Using the lazy API from a file</a></h2>
<p>In the ideal case we use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.</p>
<p>We create a lazy query from the Reddit CSV data and apply some transformations.</p>
<p>By starting the query with <code>pl.scan_csv</code> we are using the lazy API.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q1 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
)
</code></pre>
<p>A <code>pl.scan_</code> function is available for a number of file types including <a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_csv.html">CSV</a>, <a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_parquet.html">Parquet</a>, <a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_ipc.html">IPC</a> and <a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_ndjson.html">newline delimited JSON</a>.</p>
<p>In this query we tell Polars that we want to:</p>
<ul>
<li>load data from the Reddit CSV file</li>
<li>convert the <code>name</code> column to uppercase</li>
<li>apply a filter to the <code>comment_karma</code> column</li>
</ul>
<p>The lazy query will not be executed at this point. See this page on <a href="lazy-api/lazy-query-execution.html">executing lazy queries</a> for more on running lazy queries.</p>
<h2 id="using-the-lazy-api-from-a-dataframe"><a class="header" href="#using-the-lazy-api-from-a-dataframe">Using the lazy API from a <code>DataFrame</code></a></h2>
<p>An alternative way to access the lazy API is to call <code>.lazy</code> on a <code>DataFrame</code> that has already been created in memory.</p>
<pre><code class="language-python">q3 = pl.DataFrame({&quot;foo&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;bar&quot;: [0, 1, 2]}).lazy()
</code></pre>
<p>By calling <code>.lazy</code> we convert the <code>DataFrame</code> to a <code>LazyFrame</code>.</p>
<h1 id="schema-in-the-lazy-api"><a class="header" href="#schema-in-the-lazy-api">Schema in the lazy API</a></h1>
<p>The schema of a Polars <code>DataFrame</code> or <code>LazyFrame</code> sets out the names of the columns and their datatypes. You can see the schema with the <code>.schema</code> method on a <code>DataFrame</code> or <code>LazyFrame</code></p>
<pre><code class="language-python">import polars as pl


q3 = pl.DataFrame({&quot;foo&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;bar&quot;: [0, 1, 2]}).lazy()

q3.schema
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/output4.txt}}
</code></pre>
<p>The schema plays an important role in the lazy API.</p>
<h2 id="type-checking-in-the-lazy-api"><a class="header" href="#type-checking-in-the-lazy-api">Type checking in the lazy API</a></h2>
<p>One advantage of the lazy API is that Polars will check the schema before any data is processed. This check happens when you execute your lazy query.</p>
<p>We see how this works in the following simple example where we call the <code>.round</code> expression on the integer <code>bar</code> column.</p>
<pre><code class="language-python">import polars as pl

pl.DataFrame({&quot;foo&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;bar&quot;: [0, 1, 2]}).lazy().with_columns(pl.col(&quot;bar&quot;).round(0))
</code></pre>
<p>The <code>.round</code> expression is only valid for columns with a floating point dtype. Calling <code>.round</code> on an integer column means the operation will raise a <code>SchemaError</code>.</p>
<p>If we executed this query in eager mode the error would only be found once the data had been processed in all earlier steps.</p>
<p>When we execute a lazy query Polars checks for any potential <code>SchemaError</code> before the time-consuming step of actually processing the data in the pipeline.</p>
<h2 id="the-lazy-api-must-know-the-schema"><a class="header" href="#the-lazy-api-must-know-the-schema">The lazy API must know the schema</a></h2>
<p>In the lazy API the Polars query optimizer must be able to infer the schema at every step of a query plan. This means that operations where the schema is not knowable in advance cannot be used with the lazy API.</p>
<p>The classic example of an operation where the schema is not knowable in advance is a <code>.pivot</code> operation. In a <code>.pivot</code> the new column names come from data in one of the columns. As these column names cannot be known in advance a <code>.pivot</code> is not available in the lazy API.</p>
<h2 id="dealing-with-operations-not-available-in-the-lazy-api"><a class="header" href="#dealing-with-operations-not-available-in-the-lazy-api">Dealing with operations not available in the lazy API</a></h2>
<p>If your pipeline includes an operation that is not available in the lazy API it is normally best to:</p>
<ul>
<li>run the pipeline in lazy mode up until that point</li>
<li>execute the pipeline with <code>.collect</code> to materialize a <code>DataFrame</code></li>
<li>do the non-lazy operation on the <code>DataFrame</code></li>
<li>convert the output back to a <code>LazyFrame</code> with <code>.lazy</code> and continue in lazy mode</li>
</ul>
<p>We show how to deal with a non-lazy operation in this example where we:</p>
<ul>
<li>create a simple <code>DataFrame</code></li>
<li>convert it to a <code>LazyFrame</code> with <code>.lazy</code></li>
<li>do a transformation using <code>.with_columns</code></li>
<li>execute the query before the pivot with <code>.collect</code> to get a <code>DataFrame</code></li>
<li>do the <code>.pivot</code> on the <code>DataFrame</code></li>
<li>convert back in lazy mode</li>
<li>do a <code>.filter</code></li>
<li>finish by executing the query with <code>.collect</code> to get a <code>DataFrame</code></li>
</ul>
<pre><code class="language-python">import polars as pl

lazy_eager_query = (
    pl.DataFrame({&quot;id&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;month&quot;: [&quot;jan&quot;, &quot;feb&quot;, &quot;mar&quot;], &quot;values&quot;: [0, 1, 2]})
    .lazy()
    .with_columns((2 * pl.col(&quot;values&quot;)).alias(&quot;double_values&quot;))
    .collect()
    .pivot(index=&quot;id&quot;, columns=&quot;month&quot;, values=&quot;double_values&quot;)
    .lazy()
    .filter(pl.col(&quot;mar&quot;).is_null())
    .collect()
)
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/lazy_eager_query.txt}}
</code></pre>
<h1 id="understanding-a-query-plan"><a class="header" href="#understanding-a-query-plan">Understanding a query plan</a></h1>
<p>For any lazy query <code>Polars</code> has both:</p>
<ul>
<li>a non-optimized plan with the set of steps code as we provided it and</li>
<li>an optimized plan with changes made by the query optimizer</li>
</ul>
<p>We can understand both the non-optimized and optimized query plans with visualization and by printing them as text.</p>
<h2 id="non-optimized-query-plan"><a class="header" href="#non-optimized-query-plan">Non-optimized query plan</a></h2>
<h3 id="graphviz-visualization"><a class="header" href="#graphviz-visualization">Graphviz visualization</a></h3>
<p>First we visualise the non-optimized plan by setting <code>optimized=False</code>.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q1 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
)

q1.show_graph(optimized=False)
</code></pre>
<p><img src="lazy-api/../outputs/lazy_api/graph1.png" alt="" /></p>
<p>The query plan visualisation should be read from bottom to top. In the visualisation:</p>
<ul>
<li>each box corresponds to a stage in the query plan</li>
<li>the <code>sigma</code> stands for <code>SELECTION</code> and indicates any filter conditions</li>
<li>the <code>pi</code> stands for <code>PROJECTION</code> and indicates choosing a subset of columns</li>
</ul>
<h3 id="printed-query-plan"><a class="header" href="#printed-query-plan">Printed query plan</a></h3>
<p>We can also print the non-optimized plan with <code>describe_plan</code></p>
<pre><code class="language-python">q1.describe_plan()
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/q1_plan.txt}}
</code></pre>
<p>The printed plan should also be read from bottom to top. This non-optimized plan is roughly to:</p>
<ul>
<li>read from the <code>data/reddit.csv</code> file</li>
<li>read all 6 columns (where the * wildcard in PROJECT */6 COLUMNS means take all columns)</li>
<li>transform the <code>name</code> column to uppercase</li>
<li>apply a filter on the <code>comment_karma</code> column</li>
</ul>
<h2 id="optimized-query-plan"><a class="header" href="#optimized-query-plan">Optimized query plan</a></h2>
<p>Now we visualise the optimized plan with <code>show_graph</code>.</p>
<pre><code class="language-python">q1.show_graph()
</code></pre>
<p><img src="lazy-api/../outputs/lazy_api/graph1-optimized.png" alt="" /></p>
<p>We can also print the optimized plan with <code>describe_optimized_plan</code></p>
<pre><code class="language-python">q1.describe_optimized_plan()
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/q1_opt_plan.txt}}
</code></pre>
<p>The optimized plan is to:</p>
<ul>
<li>read the data from the Reddit CSV</li>
<li>apply the filter on the <code>comment_karma</code> column while the CSV is being read line-by-line</li>
<li>transform the <code>name</code> column to uppercase</li>
</ul>
<p>In this case the query optimizer has identified that the <code>filter</code> can be applied while the CSV is read from disk rather than writing the whole file to disk and then applying it. This optimization is called <em>Predicate Pushdown</em>.</p>
<h1 id="query-execution"><a class="header" href="#query-execution">Query execution</a></h1>
<p>Our example query on the Reddit dataset is:</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q1 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
)

</code></pre>
<p>If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.</p>
<p>When we execute the code Polars executes the optimized query graph by default.</p>
<h3 id="execution-on-the-full-dataset"><a class="header" href="#execution-on-the-full-dataset">Execution on the full dataset</a></h3>
<p>We can execute our query on the full dataset by calling the <code>.collect</code> method on the query.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q4 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
    .collect()
)
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/df4.txt}}
</code></pre>
<p>Above we see that from the 10 Million rows there 14,029 rows match our predicate.</p>
<p>With the default <code>collect</code> method Polars processes all of your data as one batch. This means that all the data has to fit into your available memory at the point of peak memory usage in your query.</p>
<h3 id="execution-on-larger-than-memory-data"><a class="header" href="#execution-on-larger-than-memory-data">Execution on larger-than-memory data</a></h3>
<p>If your data requires more memory than you have available Polars may be able to process the data in batches using <em>streaming</em> mode. To use streaming mode you simply pass the <code>streaming=True</code> argument to <code>collect</code></p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q5 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
    .collect(streaming=True)
)
</code></pre>
<p>We look at <a href="lazy-api/streaming.html">streaming in more detail here</a>.</p>
<h3 id="execution-on-a-partial-dataset"><a class="header" href="#execution-on-a-partial-dataset">Execution on a partial dataset</a></h3>
<p>While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.</p>
<p>You can instead execute the query with the <code>.fetch</code> method. The <code>.fetch</code> method takes a parameter <code>n_rows</code> and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.</p>
<p>Here we &quot;fetch&quot; 100 rows from the source file and apply the predicates.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q9 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
    .fetch(n_rows=int(100))
)
</code></pre>
<pre><code class="language-text">{{#include ../outputs/lazy_api/q9.txt}}
</code></pre>
<h1 id="streaming-larger-than-memory-datasets"><a class="header" href="#streaming-larger-than-memory-datasets">Streaming larger-than-memory datasets</a></h1>
<p>When a lazy query is executed in streaming mode Polars processes the dataset in batches rather than all-at-once. This can allow Polars to process datasets that are larger-than-memory.</p>
<p>To tell Polars we want to execute a query in streaming mode we pass the <code>streaming=True</code> argument to <code>collect</code></p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q5 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
    .collect(streaming=True)
)
</code></pre>
<p>We can also use streaming mode when we execute lazy queries in other ways such as a partial execution with <code>fetch</code> or a profiling execution with <code>profile</code>.</p>
<h2 id="when-is-streaming-available"><a class="header" href="#when-is-streaming-available">When is streaming available?</a></h2>
<p>Streaming is still a developing feature of Polars. We can ask Polars to execute any lazy query in streaming mode. However, not all lazy operations support streaming. If there is an operation for which streaming is not supported Polars will run the query in non-streaming mode.</p>
<p>Streaming is supported for many operations including:</p>
<ul>
<li><code>filter</code>,<code>slice</code>,<code>head</code>,<code>tail</code></li>
<li><code>with_columns</code>,<code>select</code></li>
<li><code>groupby</code>,<code>groupby_dynamic</code></li>
<li><code>join</code>,<code>join_asof</code></li>
<li><code>sort</code></li>
<li><code>scan_csv</code>,<code>scan_parquet</code>,<code>scan_ipc</code></li>
</ul>
<h2 id="sinking-to-a-file"><a class="header" href="#sinking-to-a-file">Sinking to a file</a></h2>
<p>Although streaming allows you to process larger than memory datasets the output <code>DataFrame</code> must still fit in memory.</p>
<p>To work with data where the output <code>DataFrame</code> is too large to fit in memory we can write it directly to disk. We use streaming to do this by executing the lazy query with a <code>sink_</code> function instead of <code>collect</code>. The <code>sink_</code> functions use streaming by default.</p>
<pre><code class="language-python">import polars as pl

from ..paths import DATA_DIR

q11 = (
    pl.scan_csv(f&quot;{DATA_DIR}/reddit.csv&quot;)
    .with_columns(pl.col(&quot;name&quot;).str.to_uppercase())
    .filter(pl.col(&quot;comment_karma&quot;) &gt; 0)
    .sink_parquet(f&quot;{DATA_DIR}/reddit.parquet&quot;)
)
</code></pre>
<p>There are <code>sink_</code> functions available to write to <a href="https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/api/polars.LazyFrame.sink_parquet.html">Parquet</a> and <a href="https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/api/polars.LazyFrame.sink_ipc.html">IPC</a> file formats.</p>
<h1 id="data-types"><a class="header" href="#data-types">Data types</a></h1>
<p><code>Polars</code> is entirely based on <code>Arrow</code> data types and backed by <code>Arrow</code> memory arrays. This makes data processing
cache-efficient and well-supported for Inter Process Communication. Most data types follow the exact implementation
from <code>Arrow</code>, with exception of <code>Utf8</code> (this is actually <code>LargeUtf8</code>), <code>Categorical</code>, and <code>Object</code> (support is limited).</p>
<p>The data types are:</p>
<ul>
<li><code>Int8</code>: 8-bit signed integer.</li>
<li><code>Int16</code>: 16-bit signed integer.</li>
<li><code>Int32</code>: 32-bit signed integer.</li>
<li><code>Int64</code>: 64-bit signed integer.</li>
<li><code>UInt8</code>: 8-bit unsigned integer.</li>
<li><code>UInt16</code>: 16-bit unsigned integer.</li>
<li><code>UInt32</code>: 32-bit unsigned integer.</li>
<li><code>UInt64</code>: 64-bit unsigned integer.</li>
<li><code>Float32</code>: 32-bit floating point.</li>
<li><code>Float64</code>: 64-bit floating point.</li>
<li><code>Boolean</code>: Boolean type effectively bit packed.</li>
<li><code>Utf8</code>: String data (this is actually <code>Arrow</code> <code>LargeUtf8</code> internally).</li>
<li><code>Binary</code>: Store data as bytes.</li>
<li><code>List</code>: A list array contains a child array containing the list values and an offset array. (this is actually <code>Arrow</code> <code>LargeList</code> internally).</li>
<li><code>Struct</code>: A struct array is represented as <code>Vec&lt;Series&gt;</code> and is useful to pack multiple/heterogenous values in a single column.</li>
<li><code>Object</code>: A limited supported data type that can be any value.</li>
<li><code>Date</code>: Date representation, internally represented as days since UNIX epoch encoded by a 32-bit signed integer.</li>
<li><code>Datetime</code>: Datetime representation, internally represented as microseconds since UNIX epoch encoded by a 64-bit signed integer.</li>
<li><code>Duration</code>: A timedelta type, internally represented as microseconds. Created when subtracting <code>Date/Datetime</code>.</li>
<li><code>Time</code>: Time representation, internally represented as nanoseconds since midnight.</li>
</ul>
<p>To learn more about the internal representation of these data types, check the <a href="https://arrow.apache.org/docs/format/Columnar.html"><code>Arrow</code> columnar format</a>.</p>
<h1 id="coming-from-pandas"><a class="header" href="#coming-from-pandas">Coming from <code>Pandas</code></a></h1>
<p>Here we set out the key points that anyone who has experience with <code>Pandas</code> and wants to
try <code>Polars</code> should know. We include both differences in the concepts the libraries are
built on and differences in how you should write <code>Polars</code> code compared to <code>Pandas</code>
code.</p>
<h2 id="differences-in-concepts-between-polars-and-pandas"><a class="header" href="#differences-in-concepts-between-polars-and-pandas">Differences in concepts between <code>Polars</code> and <code>Pandas</code></a></h2>
<h3 id="polars-does-not-have-a-multi-indexindex"><a class="header" href="#polars-does-not-have-a-multi-indexindex"><code>Polars</code> does not have a multi-index/index</a></h3>
<p><code>Pandas</code> gives a label to each row with an index. <code>Polars</code> does not use an index and
each row is indexed by its integer position in the table.</p>
<p>Polars aims to have predictable results and readable queries, as such we think an index does not help us reach that
objective. We believe the semantics of a query should not change by the state of an index or a <code>reset_index</code> call.</p>
<p>In Polars a DataFrame will always be a 2D table with heterogeneous data-types. The data-types may have nesting, but the
table itself will not.
Operations like resampling will be done by specialized functions or methods that act like 'verbs' on a table explicitly
stating columns that 'verb' operates on. As such, it is our conviction that not having indices make things simpler,
more explicit, more readable and less error-prone.</p>
<p>Note that an 'index' data structure as known in databases will be used by polars as an optimization technique.</p>
<p>For more detail on how you select data in <code>Polars</code> see the <a href="howcani/selecting_data/selecting_data_intro.html">selecting data</a>
section.</p>
<h3 id="polars-uses-apache-arrow-arrays-to-represent-data-in-memory-while-pandas-uses-numpy-arrays"><a class="header" href="#polars-uses-apache-arrow-arrays-to-represent-data-in-memory-while-pandas-uses-numpy-arrays"><code>Polars</code> uses Apache Arrow arrays to represent data in memory while <code>Pandas</code> uses <code>Numpy</code> arrays</a></h3>
<p><code>Polars</code> represents data in memory with Arrow arrays while <code>Pandas</code> represents data in
memory in <code>Numpy</code> arrays. Apache Arrow is an emerging standard for in-memory columnar
analytics that can accelerate data load times, reduce memory usage and accelerate
calculations.</p>
<p><code>Polars</code> can convert data to <code>Numpy</code> format with the <code>to_numpy</code> method.</p>
<h3 id="polars-has-more-support-for-parallel-operations-than-pandas"><a class="header" href="#polars-has-more-support-for-parallel-operations-than-pandas"><code>Polars</code> has more support for parallel operations than <code>Pandas</code></a></h3>
<p><code>Polars</code> exploits the strong support for concurrency in Rust to run many operations in
parallel. While some operations in <code>Pandas</code> are multi-threaded the core of the library
is single-threaded and an additional library such as <code>Dask</code> must be used to parallelise
operations.</p>
<h3 id="polars-can-lazily-evaluate-queries-and-apply-query-optimization"><a class="header" href="#polars-can-lazily-evaluate-queries-and-apply-query-optimization"><code>Polars</code> can lazily evaluate queries and apply query optimization</a></h3>
<p>Eager evaluation is where code is evaluated as soon as you run the code. Lazy evaluation
is where running a line of code means that the underlying logic is added to a query plan
rather than being evaluated.</p>
<p><code>Polars</code> supports eager evaluation and lazy evaluation whereas <code>Pandas</code> only supports
eager evaluation. The lazy evaluation mode is powerful because <code>Polars</code> carries out
automatic query optimization where it examines the query plan and looks for ways to
accelerate the query or reduce memory usage.</p>
<p><code>Dask</code> also supports lazy evaluation where it generates a query plan. However, <code>Dask</code>
does not carry out query optimization on the query plan.</p>
<h2 id="key-syntax-differences"><a class="header" href="#key-syntax-differences">Key syntax differences</a></h2>
<p>Users coming from <code>Pandas</code> generally need to know one thing...</p>
<pre><code>polars != pandas
</code></pre>
<p>If your <code>Polars</code> code looks like it could be <code>Pandas</code> code, it might run, but it likely
runs slower than it should.</p>
<p>Let's go through some typical <code>Pandas</code> code and see how we might write that in <code>Polars</code>.</p>
<h3 id="selecting-data"><a class="header" href="#selecting-data">Selecting data</a></h3>
<p>As there is no index in <code>Polars</code> there is no <code>.loc</code> or <code>iloc</code> method in <code>Polars</code> - and
there is also no <code>SettingWithCopyWarning</code> in <code>Polars</code>.</p>
<p>To learn more about how you select data in <code>Polars</code> see the <a href="howcani/selecting_data/selecting_data_intro.html">selecting data</a> section.</p>
<p>However, the best way to select data in <code>Polars</code> is to use the expression API. For
example, if you want to select a column in <code>Pandas</code> you can do one of the following:</p>
<pre><code class="language-python">df['a']
df.loc[:,'a']
</code></pre>
<p>but in <code>Polars</code> you would use the <code>.select</code> method:</p>
<pre><code class="language-python">df.select(['a'])
</code></pre>
<p>If you want to select rows based on the values then in <code>Polars</code> you use the <code>.filter</code>
method:</p>
<pre><code class="language-python">df.filter(pl.col('a') &lt; 10)
</code></pre>
<p>As noted in the section on expressions below, <code>Polars</code> can run operations in <code>.select</code>
and <code>filter</code> in parallel and <code>Polars</code> can carry out query optimization on the full set
of data selection criteria.</p>
<h3 id="be-lazy"><a class="header" href="#be-lazy">Be lazy</a></h3>
<p>Working in lazy evaluation mode is straightforward and should be your default in
<code>Polars</code> as the lazy mode allows <code>Polars</code> to do query optimization.</p>
<p>We can run in lazy mode by either using an implicitly lazy function (such as <code>scan_csv</code>)
or explicitly using the <code>lazy</code> method.</p>
<p>Take the following simple example where we read a CSV file from disk and do a groupby.
The CSV file has numerous columns but we just want to do a groupby on one of the id
columns (<code>id1</code>) and then sum by a value column (<code>v1</code>). In <code>Pandas</code> this would be:</p>
<pre><code class="language-python">    df = pd.read_csv(csv_file, usecols=['id1','v1'])
    grouped_df = df.loc[:,['id1','v1']].groupby('id1').sum('v1')
</code></pre>
<p>In <code>Polars</code> you can build this query in lazy mode with query optimization and evaluate
it by replacing the eager <code>Pandas</code> function <code>read_csv</code> with the implicitly lazy <code>Polars</code>
function <code>scan_csv</code>:</p>
<pre><code class="language-python">    df = pl.scan_csv(csv_file)
    grouped_df = df.groupby('id1').agg(pl.col('v1').sum()).collect()
</code></pre>
<p><code>Polars</code> optimizes this query by identifying that only the <code>id1</code> and <code>v1</code> columns are
relevant and so will only read these columns from the CSV. By calling the <code>.collect</code>
method at the end of the second line we instruct <code>Polars</code> to eagerly evaluate the query.</p>
<p>If you do want to run this query in eager mode you can just replace <code>scan_csv</code> with
<code>read_csv</code> in the <code>Polars</code> code.</p>
<p>Read more about working with lazy evaluation in the
<a href="optimizations/lazy/intro.html">lazy API</a> section.</p>
<h3 id="express-yourself"><a class="header" href="#express-yourself">Express yourself</a></h3>
<p>A typical <code>Pandas</code> script consists of multiple data transformations that are executed
sequentially. However, in <code>Polars</code> these transformations can be executed in parallel
using expressions.</p>
<h4 id="column-assignment"><a class="header" href="#column-assignment">Column assignment</a></h4>
<p>We have a dataframe <code>df</code> with a column called <code>value</code>. We want to add two new columns, a
column called <code>tenXValue</code> where the <code>value</code> column is multiplied by 10 and a column
called <code>hundredXValue</code> where the <code>value</code> column is multiplied by 100.</p>
<p>In <code>Pandas</code> this would be:</p>
<pre><code class="language-python">df[&quot;tenXValue&quot;] = df[&quot;value&quot;] * 10
df[&quot;hundredXValue&quot;] = df[&quot;value&quot;] * 100
</code></pre>
<p>These column assignments are executed sequentially.</p>
<p>In <code>Polars</code> we add columns to <code>df</code> using the <code>.with_columns</code> method and name them with
the <code>.alias</code> method:</p>
<pre><code class="language-python">df.with_columns([
    (pl.col(&quot;value&quot;) * 10).alias(&quot;tenXValue&quot;),
    (pl.col(&quot;value&quot;) * 100).alias(&quot;hundredXValue&quot;),
])
</code></pre>
<p>These column assignments are executed in parallel.</p>
<h4 id="column-assignment-based-on-predicate"><a class="header" href="#column-assignment-based-on-predicate">Column assignment based on predicate</a></h4>
<p>In this case we have a dataframe <code>df</code> with columns <code>a</code>,<code>b</code> and <code>c</code>. We want to re-assign
the values in column <code>a</code> based on a condition. When the value in column <code>c</code> is equal to
2 then we replace the value in <code>a</code> with the value in <code>b</code>.</p>
<p>In <code>Pandas</code> this would be:</p>
<pre><code class="language-python">df.loc[df[&quot;c&quot;] == 2, &quot;a&quot;] = df.loc[df[&quot;c&quot;] == 2, &quot;b&quot;]
</code></pre>
<p>while in <code>Polars</code> this would be:</p>
<pre><code class="language-python">df.with_columns(
    pl.when(pl.col(&quot;c&quot;) == 2)
    .then(pl.col(&quot;b&quot;))
    .otherwise(pl.col(&quot;a&quot;)).alias(&quot;a&quot;)
)
</code></pre>
<p>The <code>Polars</code> way is pure in that the original <code>DataFrame</code> is not modified. The <code>mask</code> is
also not computed twice as in <code>Pandas</code> (you could prevent this in <code>Pandas</code>, but that
would require setting a temporary variable).</p>
<p>Additionally <code>Polars</code> can compute every branch of an <code>if -&gt; then -&gt; otherwise</code> in
parallel. This is valuable, when the branches get more expensive to compute.</p>
<h4 id="filtering-1"><a class="header" href="#filtering-1">Filtering</a></h4>
<p>We want to filter the dataframe <code>df</code> with housing data based on some criteria.</p>
<p>In <code>Pandas</code> you filter the dataframe by passing Boolean expressions to the <code>loc</code> method:</p>
<pre><code class="language-python">df.loc[(df['sqft_living'] &gt; 2500) &amp; (df['price'] &lt; 300000)]
</code></pre>
<p>while in <code>Polars</code> you call the <code>filter</code> method:</p>
<pre><code class="language-python">df.filter(
    (pl.col(&quot;m2_living&quot;) &gt; 2500) &amp; (pl.col(&quot;price&quot;) &lt; 300000)
)
</code></pre>
<p>The query optimizer in <code>Polars</code> can also detect if you write multiple filters separately
and combine them into a single filter in the optimized plan.</p>
<h2 id="pandas-transform"><a class="header" href="#pandas-transform"><code>Pandas</code> transform</a></h2>
<p>The <code>Pandas</code> documentation demonstrates an operation on a groupby called <code>transform</code>. In
this case we have a dataframe <code>df</code> and we want a new column showing the number of rows
in each group.</p>
<p>In <code>Pandas</code> we have:</p>
<pre><code class="language-python">df = pd.DataFrame({
    &quot;type&quot;: [&quot;m&quot;, &quot;n&quot;, &quot;o&quot;, &quot;m&quot;, &quot;m&quot;, &quot;n&quot;, &quot;n&quot;],
    &quot;c&quot;: [1, 1, 1, 2, 2, 2, 2],
})

df[&quot;size&quot;] = df.groupby(&quot;c&quot;)[&quot;type&quot;].transform(len)
</code></pre>
<p>Here <code>Pandas</code> does a groupby on <code>&quot;c&quot;</code>, takes column <code>&quot;type&quot;</code>, computes the group length
and then joins the result back to the original <code>DataFrame</code> producing:</p>
<pre><code>   c type size
0  1    m    3
1  1    n    3
2  1    o    3
3  2    m    4
4  2    m    4
5  2    n    4
6  2    n    4
</code></pre>
<p>In <code>Polars</code> the same can be achieved with <code>window</code> functions:</p>
<pre><code class="language-python">df.select([
    pl.all(),
    pl.col(&quot;type&quot;).count().over(&quot;c&quot;).alias(&quot;size&quot;)
])
</code></pre>
<pre><code>shape: (7, 3)
┌─────┬──────┬──────┐
│ c   ┆ type ┆ size │
│ --- ┆ ---  ┆ ---  │
│ i64 ┆ str  ┆ u32  │
╞═════╪══════╪══════╡
│ 1   ┆ m    ┆ 3    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 1   ┆ n    ┆ 3    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 1   ┆ o    ┆ 3    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 2   ┆ m    ┆ 4    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 2   ┆ m    ┆ 4    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 2   ┆ n    ┆ 4    │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┤
│ 2   ┆ n    ┆ 4    │
└─────┴──────┴──────┘
</code></pre>
<p>Because we can store the whole operation in a single expression, we can combine several
<code>window</code> functions and even combine different groups!</p>
<p><code>Polars</code> will cache window expressions that are applied over the same group, so storing
them in a single <code>select</code> is both convenient <strong>and</strong> optimal. In the following example
we look at a case where we are calculating group statistics over <code>&quot;c&quot;</code> twice:</p>
<pre><code class="language-python">df.select([
    pl.all(),
    pl.col(&quot;c&quot;).count().over(&quot;c&quot;).alias(&quot;size&quot;),
    pl.col(&quot;c&quot;).sum().over(&quot;type&quot;).alias(&quot;sum&quot;),
    pl.col(&quot;c&quot;).reverse().over(&quot;c&quot;).flatten().alias(&quot;reverse_type&quot;)
])
</code></pre>
<pre><code>shape: (7, 5)
┌─────┬──────┬──────┬─────┬──────────────┐
│ c   ┆ type ┆ size ┆ sum ┆ reverse_type │
│ --- ┆ ---  ┆ ---  ┆ --- ┆ ---          │
│ i64 ┆ str  ┆ u32  ┆ i64 ┆ i64          │
╞═════╪══════╪══════╪═════╪══════════════╡
│ 1   ┆ m    ┆ 3    ┆ 5   ┆ 2            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ n    ┆ 3    ┆ 5   ┆ 2            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ o    ┆ 3    ┆ 1   ┆ 2            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ m    ┆ 4    ┆ 5   ┆ 2            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ m    ┆ 4    ┆ 5   ┆ 1            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ n    ┆ 4    ┆ 5   ┆ 1            │
├╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ n    ┆ 4    ┆ 5   ┆ 1            │
└─────┴──────┴──────┴─────┴──────────────┘

</code></pre>
<h2 id="missing-data"><a class="header" href="#missing-data">Missing data</a></h2>
<p><code>Pandas</code> uses <code>NaN</code> and/or <code>None</code> values to indicate missing values depending on the dtype of the column. In addition the behaviour in <code>Pandas</code> varies depending on whether the default dtypes or optional nullable arrays are used. In <code>Polars</code> missing data corresponds to a <code>null</code> value for all data types.</p>
<p>For float columns <code>Polars</code> permits the use of <code>NaN</code> values. These <code>NaN</code> values are not considered to be missing data but instead a special floating point value.</p>
<p>In <code>Pandas</code> an integer column with missing values is cast to be a float column with <code>NaN</code> values for the missing values (unless using optional nullable integer dtypes). In <code>Polars</code> any missing values in an integer column are simply <code>null</code> values and the column remains an integer column.</p>
<p>See the <a href="howcani/missing_data.html">missing data</a> section for more details.</p>
<h1 id="coming-from-apache-spark"><a class="header" href="#coming-from-apache-spark">Coming from Apache Spark</a></h1>
<h2 id="column-based-api-vs-row-based-api"><a class="header" href="#column-based-api-vs-row-based-api">Column-based API vs. Row-based API</a></h2>
<p>Whereas the <code>Spark</code> <code>DataFrame</code> is analogous to a collection of rows, a <code>Polars</code> <code>DataFrame</code> is closer to a collection of columns. This means that you can combine columns in <code>Polars</code> in ways that are not possible in <code>Spark</code>, because <code>Spark</code> preserves the relationship of the data in each row.</p>
<p>Consider this sample dataset:</p>
<pre><code class="language-python">import polars as pl

df = pl.DataFrame({
    &quot;foo&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;d&quot;],
    &quot;bar&quot;: [1, 2, 3, 4, 5],
})

dfs = spark.createDataFrame(
    [
        (&quot;a&quot;, 1),
        (&quot;b&quot;, 2),
        (&quot;c&quot;, 3),
        (&quot;d&quot;, 4),
        (&quot;d&quot;, 5),
    ],
    schema=[&quot;foo&quot;, &quot;bar&quot;],
)
</code></pre>
<h3 id="example-1-combining-head-and-sum"><a class="header" href="#example-1-combining-head-and-sum">Example 1: Combining <code>head</code> and <code>sum</code></a></h3>
<p>In <code>Polars</code> you can write something like this:</p>
<pre><code class="language-python">df.select([
    pl.col(&quot;foo&quot;).sort().head(2),
    pl.col(&quot;bar&quot;).filter(pl.col(&quot;foo&quot;) == &quot;d&quot;).sum()
])
</code></pre>
<p>Output:</p>
<pre><code>shape: (2, 2)
┌─────┬─────┐
│ foo ┆ bar │
│ --- ┆ --- │
│ str ┆ i64 │
╞═════╪═════╡
│ a   ┆ 9   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ b   ┆ 9   │
└─────┴─────┘
</code></pre>
<p>The expressions on columns <code>foo</code> and <code>bar</code> are completely independent. Since the expression on <code>bar</code> returns a single value, that value is repeated for each value output by the expression on <code>foo</code>. But <code>a</code> and <code>b</code> have no relation to the data that produced the sum of <code>9</code>.</p>
<p>To do something similar in <code>Spark</code>, you'd need to compute the sum separately and provide it as a literal:</p>
<pre><code class="language-python">from pyspark.sql.functions import col, sum, lit

bar_sum = (
    dfs
    .where(col(&quot;foo&quot;) == &quot;d&quot;)
    .groupBy()
    .agg(sum(col(&quot;bar&quot;)))
    .take(1)[0][0]
)

(
    dfs
    .orderBy(&quot;foo&quot;)
    .limit(2)
    .withColumn(&quot;bar&quot;, lit(bar_sum))
    .show()
)
</code></pre>
<p>Output:</p>
<pre><code>+---+---+
|foo|bar|
+---+---+
|  a|  9|
|  b|  9|
+---+---+
</code></pre>
<h3 id="example-2-combining-two-heads"><a class="header" href="#example-2-combining-two-heads">Example 2: Combining Two <code>head</code>s</a></h3>
<p>In <code>Polars</code> you can combine two different <code>head</code> expressions on the same DataFrame, provided that they return the same number of values.</p>
<pre><code class="language-python">df.select([
    pl.col(&quot;foo&quot;).sort().head(2),
    pl.col(&quot;bar&quot;).sort(descending=True).head(2),
])
</code></pre>
<p>Output:</p>
<pre><code>shape: (3, 2)
┌─────┬─────┐
│ foo ┆ bar │
│ --- ┆ --- │
│ str ┆ i64 │
╞═════╪═════╡
│ a   ┆ 5   │
├╌╌╌╌╌┼╌╌╌╌╌┤
│ b   ┆ 4   │
└─────┴─────┘
</code></pre>
<p>Again, the two <code>head</code> expressions here are completely independent, and the pairing of <code>a</code> to <code>5</code> and <code>b</code> to <code>4</code> results purely from the juxtaposition of the two columns output by the expressions.</p>
<p>To accomplish something similar in <code>Spark</code>, you would need to generate an artificial key that enables you to join the values in this way.</p>
<pre><code class="language-python">from pyspark.sql import Window
from pyspark.sql.functions import row_number

foo_dfs = (
    dfs
    .withColumn(
        &quot;rownum&quot;,
        row_number().over(Window.orderBy(&quot;foo&quot;))
    )
)

bar_dfs = (
    dfs
    .withColumn(
        &quot;rownum&quot;,
        row_number().over(Window.orderBy(col(&quot;bar&quot;).desc()))
    )
)

(
    foo_dfs.alias(&quot;foo&quot;)
    .join(bar_dfs.alias(&quot;bar&quot;), on=&quot;rownum&quot;)
    .select(&quot;foo.foo&quot;, &quot;bar.bar&quot;)
    .limit(2)
    .show()
)
</code></pre>
<p>Output:</p>
<pre><code>+---+---+
|foo|bar|
+---+---+
|  a|  5|
|  b|  4|
+---+---+
</code></pre>
<h1 id="polars-sql"><a class="header" href="#polars-sql">Polars SQL</a></h1>
<h2 id="starting-the-sql-context"><a class="header" href="#starting-the-sql-context">Starting the SQL Context</a></h2>
<p>You can query a <code>Polars</code> <code>LazyFrame</code> with SQL.
The first step is to initialize a SQL context, and register a <code>LazyFrame</code> with it.</p>
<p>Let's load some data and initialize the SQL context:</p>
<pre><code class="language-python">import polars as pl

# convert 'pokemon' into a Lazyframe by calling the .lazy() method
pokemon = pl.read_csv(
    &quot;https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv&quot;
).lazy()

# initialize the SQL context and register the lazyframe
sql = pl.SQLContext()
sql.register(&quot;pokemon&quot;, pokemon)
</code></pre>
<p>Polars supports a single SQL context per thread, and the registered dataframe should be a <code>LazyFrame</code>.
You can call the register function multiple time for each of your LazyFrame.</p>
<h2 id="running-your-sql-queries-"><a class="header" href="#running-your-sql-queries-">Running your SQL queries 🚀🚀</a></h2>
<p>You run your SQL queries with <code>SQLContext.query</code>.</p>
<pre><code class="language-python">out = sql.query(
    &quot;&quot;&quot;
SELECT 
    &quot;Type 1&quot;,
    COUNT(DISTINCT &quot;Type 2&quot;) AS count_type_2,
    AVG(Attack) AS avg_attack_by_type,
    MAX(Speed) AS max_speed
FROM pokemon
GROUP BY &quot;Type 1&quot;
&quot;&quot;&quot;
)
</code></pre>
<pre><code class="language-text">shape: (15, 4)
┌──────────┬──────────────┬────────────────────┬───────────┐
│ Type 1   ┆ count_type_2 ┆ avg_attack_by_type ┆ max_speed │
│ ---      ┆ ---          ┆ ---                ┆ ---       │
│ str      ┆ u32          ┆ f64                ┆ i64       │
╞══════════╪══════════════╪════════════════════╪═══════════╡
│ Rock     ┆ 3            ┆ 87.5               ┆ 150       │
│ Dragon   ┆ 2            ┆ 94.0               ┆ 80        │
│ Fairy    ┆ 1            ┆ 57.5               ┆ 60        │
│ …        ┆ …            ┆ …                  ┆ …         │
│ Fire     ┆ 3            ┆ 88.642857          ┆ 105       │
│ Ice      ┆ 2            ┆ 67.5               ┆ 95        │
│ Electric ┆ 3            ┆ 62.0               ┆ 140       │
└──────────┴──────────────┴────────────────────┴───────────┘
</code></pre>
<h1 id="how-can-i"><a class="header" href="#how-can-i">How can I?</a></h1>
<p>This chapter contains some snippets that will get you up to speed with the most
idiomatic way to get things done in <code>Polars</code>.</p>
<h1 id="io"><a class="header" href="#io">IO</a></h1>
<p><code>Polars</code> supports different file types, and its respective parsers are amongst the fastest
out there.</p>
<p>For instance, it is faster to load a CSV file <em>via</em> <code>Polars</code> before handing it to <code>Pandas</code>
than loading them using <code>Pandas</code>. Just run a
<code>pl.read_csv(&quot;&lt;FILE&gt;&quot;, rechunk=False).to_pandas()</code> to convince yourself!</p>
<h1 id="character-separated-values"><a class="header" href="#character-separated-values">Character-Separated Values</a></h1>
<h2 id="read--write"><a class="header" href="#read--write">Read &amp; Write</a></h2>
<p>Reading a CSV file should look familiar:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.read_csv(&quot;path.csv&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let df = CsvReader::from_path(&quot;path.csv&quot;).unwrap().finish().unwrap();
</code></pre>
</div>
<p>CSV files come in many different flavors, so make sure to check the
<a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html"><code>read_csv()</code></a> API.</p>
<p>Writing to a CSV file can be done with the
<a href="https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.write_csv.html"><code>write_csv()</code></a> method.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [None, &quot;bak&quot;, &quot;baz&quot;]})
df.write_csv(&quot;path.csv&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let mut df = df!(
    &quot;foo&quot; =&gt; &amp;[1, 2, 3],
    &quot;bar&quot; =&gt; &amp;[None, Some(&quot;bak&quot;), Some(&quot;baz&quot;)],
)
.unwrap();

let mut file = std::fs::File::create(&quot;path.csv&quot;).unwrap();
CsvWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();
</code></pre>
</div>
<h2 id="scan"><a class="header" href="#scan">Scan</a></h2>
<p><code>Polars</code> allows you to <em>scan</em> a CSV input. Scanning delays the actual parsing of the
file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.scan_csv(&quot;path.csv&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let df = LazyCsvReader::new(&quot;./test.csv&quot;).finish().unwrap();
</code></pre>
</div>
<p>If you want to know why this is desirable, you can read more about those <code>Polars</code>
optimizations <a href="howcani/io/../../optimizations/intro.html">here</a>.</p>
<p>The following video shows how to efficiently load CSV files with Polars and how the built-in query optimization makes this much faster.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nGritAo-71o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h1 id="parquet-1"><a class="header" href="#parquet-1">Parquet</a></h1>
<p>Loading or writing <a href="https://parquet.apache.org/"><code>Parquet</code> files</a> is lightning fast.
<code>Pandas</code> uses <a href="https://arrow.apache.org/docs/python/"><code>PyArrow</code></a> -<code>Python</code> bindings
exposed by <code>Arrow</code>- to load <code>Parquet</code> files into memory, but it has to copy that data into
<code>Pandas</code> memory. With <code>Polars</code> there is no extra cost due to
copying as we read <code>Parquet</code> directly into <code>Arrow</code> memory and <em>keep it there</em>.</p>
<h2 id="read"><a class="header" href="#read">Read</a></h2>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.read_parquet(&quot;path.parquet&quot;)
</code></pre>
<pre><code class="language-rust noplayground">
let mut file = std::fs::File::open(&quot;path.parquet&quot;).unwrap();

let df = ParquetReader::new(&amp;mut file).finish().unwrap();
</code></pre>
</div>
<h2 id="write"><a class="header" href="#write">Write</a></h2>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [None, &quot;bak&quot;, &quot;baz&quot;]})
df.write_parquet(&quot;path.parquet&quot;)
</code></pre>
<pre><code class="language-rust noplayground">let mut df = df!(
    &quot;foo&quot; =&gt; &amp;[1, 2, 3],
    &quot;bar&quot; =&gt; &amp;[None, Some(&quot;bak&quot;), Some(&quot;baz&quot;)],
)
.unwrap();

let mut file = std::fs::File::create(&quot;path.parquet&quot;).unwrap();
ParquetWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();
</code></pre>
</div>
<h2 id="scan-1"><a class="header" href="#scan-1">Scan</a></h2>
<p><code>Polars</code> allows you to <em>scan</em> a <code>Parquet</code> input. Scanning delays the actual parsing of the
file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.scan_parquet(&quot;path.parquet&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let args = ScanArgsParquet::default();
let df = LazyFrame::scan_parquet(&quot;./file.parquet&quot;,args).unwrap();
</code></pre>
</div>
<p>If you want to know why this is desirable,
you can read more about those <code>Polars</code> optimizations <a href="howcani/io/../../optimizations/intro.html">here</a>.</p>
<h3 id="note-about-rust-usage-1"><a class="header" href="#note-about-rust-usage-1">Note about Rust usage</a></h3>
<p>Parquet functionality is not enabled by default. It must be added as an additional feature.
This can be enabled via <code>cargo add polars --features parquet</code> or by directly adding it to your <code>Cargo.toml</code></p>
<pre><code class="language-toml">[dependencies]
polars = { version = &quot;0.24.3&quot;, features = [&quot;parquet&quot;] }
</code></pre>
<p>Additionally, scanning of parquet files requires the <code>lazy</code> feature</p>
<pre><code class="language-toml">[dependencies]
polars = { version = &quot;0.24.3&quot;, features = [&quot;parquet&quot;, &quot;lazy&quot;] }
</code></pre>
<h1 id="json-files"><a class="header" href="#json-files">JSON files</a></h1>
<h2 id="read--write-1"><a class="header" href="#read--write-1">Read &amp; Write</a></h2>
<h3 id="json-1"><a class="header" href="#json-1">JSON</a></h3>
<p>Reading a JSON file should look familiar:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.read_json(&quot;path.json&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let mut file = std::fs::File::open(&quot;path.json&quot;).unwrap();
let df = JsonReader::new(&amp;mut file).finish().unwrap();
</code></pre>
</div>
<h3 id="newline-delimited-json"><a class="header" href="#newline-delimited-json">Newline Delimited JSON</a></h3>
<p>JSON objects that are delimited by newlines can be read into polars in a much more performant way than standard json.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.read_ndjson(&quot;path.json&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let mut file = std::fs::File::open(&quot;path.json&quot;).unwrap();
let df = JsonLineReader::new(&amp;mut file).finish().unwrap();
</code></pre>
</div>
<h2 id="write-1"><a class="header" href="#write-1">Write</a></h2>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [None, &quot;bak&quot;, &quot;baz&quot;]})
# json
df.write_json(&quot;path.json&quot;)
# ndjson
df.write_ndjson(&quot;path.json&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let mut df = df!(
    &quot;foo&quot; =&gt; &amp;[1, 2, 3],
    &quot;bar&quot; =&gt; &amp;[None, Some(&quot;bak&quot;), Some(&quot;baz&quot;)],
)
.unwrap();

let mut file = std::fs::File::create(&quot;path.csv&quot;).unwrap();

// json
JsonWriter::new(&amp;mut file)
    .with_json_format(JsonFormat::Json)
    .finish(&amp;mut df)
    .unwrap();

// ndjson
JsonWriter::new(&amp;mut file)
    .with_json_format(JsonFormat::JsonLines)
    .finish(&amp;mut df)
    .unwrap();
</code></pre>
</div>
<h2 id="scan-2"><a class="header" href="#scan-2">Scan</a></h2>
<p><code>Polars</code> allows you to <em>scan</em> a JSON input <strong>only for newline delimited json</strong>. Scanning delays the actual parsing of the
file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">df = pl.scan_ndjson(&quot;path.json&quot;)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

let df = LazyJsonLineReader::new(&quot;path.json&quot;.to_string()).finish().unwrap();
</code></pre>
</div>
<h3 id="note-about-rust-usage-2"><a class="header" href="#note-about-rust-usage-2">Note about Rust usage</a></h3>
<p>json functionality is not enabled by default. It must be added as an additional feature.
This can be enabled via <code>cargo add polars --features json</code> or by directly adding it to your <code>Cargo.toml</code></p>
<pre><code class="language-toml">[dependencies]
polars = { version = &quot;0.24.3&quot;, features = [&quot;json&quot;] }
</code></pre>
<p>Additionally, scanning of json files requires the <code>lazy</code> feature</p>
<pre><code class="language-toml">[dependencies]
polars = { version = &quot;0.24.3&quot;, features = [&quot;json&quot;, &quot;lazy&quot;] }
</code></pre>
<h2 id="dealing-with-multiple-files"><a class="header" href="#dealing-with-multiple-files">Dealing with multiple files.</a></h2>
<p><code>Polars</code> can deal with multiple files differently depending on your needs and memory strain.</p>
<p>Let's create some files to give use some context:</p>
<pre><code class="language-python">import polars as pl

df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3], &quot;bar&quot;: [None, &quot;ham&quot;, &quot;spam&quot;]})

for i in range(5):
    df.write_csv(f&quot;my_many_files_{i}.csv&quot;)
</code></pre>
<h2 id="reading-into-a-single-dataframe"><a class="header" href="#reading-into-a-single-dataframe">Reading into a single <code>DataFrame</code></a></h2>
<p>To read multiple files into a single <code>DataFrame</code>, we can use globbing patterns:</p>
<pre><code class="language-python">df = pl.read_csv(&quot;my_many_files_*.csv&quot;)
print(df)
</code></pre>
<pre><code class="language-text">shape: (15, 2)
┌─────┬──────┐
│ foo ┆ bar  │
│ --- ┆ ---  │
│ i64 ┆ str  │
╞═════╪══════╡
│ 1   ┆ null │
│ 2   ┆ ham  │
│ 3   ┆ spam │
│ 1   ┆ null │
│ …   ┆ …    │
│ 3   ┆ spam │
│ 1   ┆ null │
│ 2   ┆ ham  │
│ 3   ┆ spam │
└─────┴──────┘
</code></pre>
<p>To see how this works we can take a look at the query plan. Below we see that all files are read separately and
concatenated into a single <code>DataFrame</code>. <code>Polars</code> will try to parallelize the reading.</p>
<pre><code class="language-python">pl.scan_csv(&quot;my_many_files_*.csv&quot;).show_graph()
</code></pre>
<p><img src="multiple_files/../outputs/multiple_files/single_df_graph.png" alt="single_df_graph" /></p>
<h2 id="reading-and-processing-in-parallel"><a class="header" href="#reading-and-processing-in-parallel">Reading and processing in parallel</a></h2>
<p>If your files don't have to be in a single table you can also build a query plan for each file and execute them in parallel
on the <code>Polars</code> thread pool.</p>
<p>All query plan execution is embarrassingly parallel and doesn't require any communication.</p>
<pre><code class="language-python">import polars as pl
import glob

queries = []
for file in glob.glob(&quot;my_many_files_*.csv&quot;):
    q = pl.scan_csv(file).groupby(&quot;bar&quot;).agg([pl.count(), pl.sum(&quot;foo&quot;)])
    queries.append(q)

dataframes = pl.collect_all(queries)
print(dataframes)
</code></pre>
<pre><code class="language-text">[shape: (3, 3)
┌──────┬───────┬─────┐
│ bar  ┆ count ┆ foo │
│ ---  ┆ ---   ┆ --- │
│ str  ┆ u32   ┆ i64 │
╞══════╪═══════╪═════╡
│ null ┆ 1     ┆ 1   │
│ ham  ┆ 1     ┆ 2   │
│ spam ┆ 1     ┆ 3   │
└──────┴───────┴─────┘, shape: (3, 3)
┌──────┬───────┬─────┐
│ bar  ┆ count ┆ foo │
│ ---  ┆ ---   ┆ --- │
│ str  ┆ u32   ┆ i64 │
╞══════╪═══════╪═════╡
│ null ┆ 1     ┆ 1   │
│ ham  ┆ 1     ┆ 2   │
│ spam ┆ 1     ┆ 3   │
└──────┴───────┴─────┘, shape: (3, 3)
┌──────┬───────┬─────┐
│ bar  ┆ count ┆ foo │
│ ---  ┆ ---   ┆ --- │
│ str  ┆ u32   ┆ i64 │
╞══════╪═══════╪═════╡
│ null ┆ 1     ┆ 1   │
│ spam ┆ 1     ┆ 3   │
│ ham  ┆ 1     ┆ 2   │
└──────┴───────┴─────┘, shape: (3, 3)
┌──────┬───────┬─────┐
│ bar  ┆ count ┆ foo │
│ ---  ┆ ---   ┆ --- │
│ str  ┆ u32   ┆ i64 │
╞══════╪═══════╪═════╡
│ spam ┆ 1     ┆ 3   │
│ null ┆ 1     ┆ 1   │
│ ham  ┆ 1     ┆ 2   │
└──────┴───────┴─────┘, shape: (3, 3)
┌──────┬───────┬─────┐
│ bar  ┆ count ┆ foo │
│ ---  ┆ ---   ┆ --- │
│ str  ┆ u32   ┆ i64 │
╞══════╪═══════╪═════╡
│ ham  ┆ 1     ┆ 2   │
│ spam ┆ 1     ┆ 3   │
│ null ┆ 1     ┆ 1   │
└──────┴───────┴─────┘]
</code></pre>
<h1 id="read-from-mysql-postgres-sqlite-redshift-clickhouse"><a class="header" href="#read-from-mysql-postgres-sqlite-redshift-clickhouse">Read from MySQL, Postgres, Sqlite, Redshift, Clickhouse</a></h1>
<p>To read from one of the supported databases <code>connector-x</code> needs to be installed.</p>
<pre><code class="language-shell">$  pip install connectorx&gt;=0.2.0a3
</code></pre>
<pre><code class="language-python">import polars as pl

conn = &quot;postgres://username:password@server:port/database&quot;
query = &quot;SELECT * FROM foo&quot;

pl.read_sql(query, conn)
</code></pre>
<h1 id="interact-with-aws"><a class="header" href="#interact-with-aws">Interact with AWS</a></h1>
<blockquote>
<p>The Interact with AWS page is under construction.</p>
</blockquote>
<p>To read from or write to an AWS bucket, additional dependencies are needed:</p>
<div class="tabbed-blocks">
<pre><code class="language-shell-python">$ pip install s3fs
</code></pre>
<pre><code class="language-shell-rust">$ cargo add aws_sdk_s3 aws_config tokio --features tokio/full
</code></pre>
</div>
<p>In the next few snippets we'll demonstrate interacting with a <code>Parquet</code> file
located on an AWS bucket.</p>
<h2 id="read-1"><a class="header" href="#read-1">Read</a></h2>
<p>Load a <code>.parquet</code> file using:</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl
import pyarrow.parquet as pq
import s3fs

fs = s3fs.S3FileSystem()
bucket = &quot;&lt;YOUR_BUCKET&gt;&quot;
path = &quot;&lt;YOUR_PATH&gt;&quot;

dataset = pq.ParquetDataset(f&quot;s3://{bucket}/{path}&quot;, filesystem=fs)
df = pl.from_arrow(dataset.read())
</code></pre>
<pre><code class="language-rust noplayground">use aws_sdk_s3::Region;

use aws_config::meta::region::RegionProviderChain;
use aws_sdk_s3::Client;
use std::borrow::Cow;

use polars::prelude::*;

#[tokio::main]
async fn main() {
    let bucket = &quot;&lt;YOUR_BUCKET&gt;&quot;;
    let path = &quot;&lt;YOUR_PATH&gt;&quot;;

    let config = aws_config::from_env().load().await;
    let client = Client::new(&amp;config);

    let req = client.get_object().bucket(bucket).key(path);

    let res = req.clone().send().await.unwrap();
    let bytes = res.body.collect().await.unwrap();
    let bytes = bytes.into_bytes();

    let cursor = std::io::Cursor::new(bytes);

    let df = CsvReader::new(cursor).finish().unwrap();

    println!(&quot;{:?}&quot;, df);
}
</code></pre>
</div>
<h2 id="write-2"><a class="header" href="#write-2">Write</a></h2>
<blockquote>
<p>This content is under construction.</p>
</blockquote>
<h1 id="interact-with-google-bigquery"><a class="header" href="#interact-with-google-bigquery">Interact with Google BigQuery</a></h1>
<p>To read or write from GBQ, additional dependencies are needed:</p>
<pre><code class="language-shell">$ pip install google-cloud-bigquery
</code></pre>
<h2 id="read-2"><a class="header" href="#read-2">Read</a></h2>
<p>We can load a query into a <code>DataFrame</code> like this:</p>
<pre><code class="language-python">import polars as pl
from google.cloud import bigquery

client = bigquery.Client()

# Perform a query.
QUERY = (
    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '
    'WHERE state = &quot;TX&quot; '
    'LIMIT 100')
query_job = client.query(QUERY)  # API request
rows = query_job.result()  # Waits for query to finish

df = pl.from_arrow(rows.to_arrow())
</code></pre>
<h2 id="write-3"><a class="header" href="#write-3">Write</a></h2>
<blockquote>
<p>This content is under construction.</p>
</blockquote>
<h1 id="interact-with-postgres"><a class="header" href="#interact-with-postgres">Interact with Postgres</a></h1>
<h2 id="read-3"><a class="header" href="#read-3">Read</a></h2>
<p>To read from postgres, additional dependencies are needed:</p>
<pre><code class="language-shell">$  pip install connectorx&gt;=0.2.0a3
</code></pre>
<pre><code class="language-python">import polars as pl

conn = &quot;postgresql://username:password@server:port/database&quot;
query = &quot;SELECT * FROM foo&quot;

pl.read_sql(query, conn)
</code></pre>
<h2 id="write-4"><a class="header" href="#write-4">Write</a></h2>
<p>To write to postgres, additional dependencies are needed:</p>
<pre><code class="language-shell">$ pip install psycopg2-binary
</code></pre>
<p>For writing to a postgres database with <code>psycopg2</code>, we utilize <code>execute_batch</code>. This will limit round trips needed
to the server.</p>
<p>We first make sure that all our dtypes are in a format that <code>psycopg2</code> recognizes, and then we use <code>DataFrame.rows</code> to
easily transform the columnar data to rows that the database driver can work with.</p>
<pre><code class="language-python">from psycopg2 import sql
import psycopg2.extras
import polars as pl

# let's assume we have a DataFrame with some floats, integers, strings, and date64 columns.
df = pl.read_parquet(&quot;somefile.parquet&quot;)

# first me convert polars date64 representation to python datetime objects 
for col in df:
    # only for date64
    if col.dtype == pl.Date64:
        df = df.with_columns(col.dt.to_python_datetime())

# create sql identifiers for the column names
# we do this to safely insert this into a sql query
columns = sql.SQL(&quot;,&quot;).join(sql.Identifier(name) for name in df.columns)

# create placeholders for the values. These will be filled later
values = sql.SQL(&quot;,&quot;).join([sql.Placeholder() for _ in df.columns])

table_id = &quot;mytable&quot;

# prepare the insert query
insert_stmt = sql.SQL(&quot;INSERT INTO {} ({}) VALUES({});&quot;).format(
    sql.Identifier(table_id), columns, values
)

# make a connection
conn = psycopg2.connect()
cur = conn.cursor()

# do the insert
psycopg2.extras.execute_batch(cur, insert_stmt, df.rows())
conn.commit()
</code></pre>
<h1 id="interoperability"><a class="header" href="#interoperability">Interoperability</a></h1>
<h1 id="arrow"><a class="header" href="#arrow">Arrow</a></h1>
<p><code>Arrow</code> is rapidly becoming the <em>de facto</em> standard for columnar data. This means that
support for <code>Arrow</code> is growing rapidly (both languages and tools). Due to the amazing
effort behind the format, using <code>Arrow</code> is now likely the fastest way to:</p>
<ul>
<li>Read and write <code>Parquet</code> formatted files</li>
<li>Read CSV into columnar data</li>
<li>Exchanging columnar data</li>
</ul>
<p><code>Polars</code> uses an <code>Arrow</code> memory buffer as the most basic building block for the <code>Polars</code>
<code>Series</code>. This means that we exchange data between <code>Polars</code> and <code>Arrow</code> <strong>without
copying</strong> it. It also means that <code>Polars</code> shares the same performance gains that <code>Arrow</code> receives.</p>
<p>Convert a <code>Polars</code> <code>DataFrame</code> or <code>Series</code> to <code>Arrow</code> using the <code>.to_arrow()</code>
method. Similarly, importing from <code>Arrow</code> data structure can be performed with the
<code>.from_arrow()</code> functions.</p>
<h1 id="numpy"><a class="header" href="#numpy">NumPy</a></h1>
<p><code>Polars</code> <code>Series</code> have support for <code>NumPy</code>
<a href="https://numpy.org/doc/stable/reference/ufuncs.html">universal functions (ufuncs)</a>.
Element-wise functions such as <code>np.exp()</code>, <code>np.cos()</code>, <code>np.div()</code>, <em>etc.</em> all work with
almost zero overhead.</p>
<p>However, as a <code>Polars</code>-specific remark: missing values are a separate bitmask and are not
visible by <code>NumPy</code>. This can lead to a window function or a <code>np.convolve()</code> giving
flawed or incomplete results.</p>
<p>Convert a <code>Polars</code> <code>Series</code> to a <code>NumPy</code> array with the <code>.to_numpy()</code> method.
Missing values will be replaced by <code>np.nan</code> during the conversion. If the <code>Series</code> does
not include missing values, or those values are not desired anymore, the <code>.view()</code>
method can be used instead, providing a zero-copy <code>NumPy</code> array of the data.</p>
<h1 id="selecting-data-1"><a class="header" href="#selecting-data-1">Selecting data</a></h1>
<p>In this section we show how to select rows and/or columns from a <code>DataFrame</code>. We can
<a href="howcani/selecting_data/selecting_data_expressions.html">select data with expressions</a> or <a href="howcani/selecting_data/selecting_data_indexing.html">select data with square bracket indexing</a>.</p>
<p>The Expression API is key to writing performant queries in <code>Polars</code>. The simplest way to get started with the Expression API is to get familiar with the <code>filter</code> and <code>select</code> methods in this section.</p>
<blockquote>
<p>Although they may give the same output, selecting data with expressions or square bracket indexing <strong>are not equivalent</strong>. The implementation of selecting data with expressions is completely different from the implementation of selecting data with square bracket indexing.</p>
</blockquote>
<p>We <strong>strongly recommend</strong> selecting data with expressions for almost all use cases. Square bracket indexing is perhaps useful when doing exploratory data analysis in a terminal or notebook when you just want a quick look at a subset of data.</p>
<p>For all other use cases we recommend using expressions because:</p>
<ul>
<li>expressions can be parallelized</li>
<li>the expression approach can be used in lazy and eager mode while the indexing approach can only be used in eager mode</li>
<li>in lazy mode the query optimizer can optimize expressions</li>
</ul>
<h1 id="selecting-with-expressions"><a class="header" href="#selecting-with-expressions">Selecting with expressions</a></h1>
<p>In this page we show how to select rows and columns with the preferred expression approach. We cover:</p>
<ul>
<li>use of the Expression API via the <code>filter</code> and <code>select</code> methods to select data</li>
<li>combining these expressions and</li>
<li>optimization of these expression in lazy mode.</li>
</ul>
<p>To select data with expressions we use:</p>
<ul>
<li>the <code>filter</code> method to select rows</li>
<li>the <code>select</code> method to select columns</li>
</ul>
<p>For simplicity we deal with <code>DataFrame</code> examples throughout. The principles are the same for <code>Series</code> objects except that columns obviously cannot be selected in a <code>Series</code>. To illustrate the <code>filter</code> and <code>select</code> methods we define a simple <code>DataFrame</code>:</p>
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;id&quot;: [1, 2, 3],
        &quot;color&quot;: [&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;],
        &quot;size&quot;: [&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;],
    }
)
print(df)
</code></pre>
<pre><code class="language-text">shape: (3, 3)
┌─────┬───────┬────────┐
│ id  ┆ color ┆ size   │
│ --- ┆ ---   ┆ ---    │
│ i64 ┆ str   ┆ str    │
╞═════╪═══════╪════════╡
│ 1   ┆ blue  ┆ small  │
│ 2   ┆ red   ┆ medium │
│ 3   ┆ green ┆ large  │
└─────┴───────┴────────┘
</code></pre>
<h2 id="selecting-rows-with-the-filter-method"><a class="header" href="#selecting-rows-with-the-filter-method">Selecting rows with the <code>filter</code> method</a></h2>
<p>We can select rows by using the <code>filter</code> method. In the <code>filter</code> method we pass the condition we are using to select the rows as an expression:</p>
<pre><code class="language-python">filter_df = df.filter(pl.col(&quot;id&quot;) &lt;= 2)
print(filter_df)
</code></pre>
<pre><code class="language-text">shape: (2, 3)
┌─────┬───────┬────────┐
│ id  ┆ color ┆ size   │
│ --- ┆ ---   ┆ ---    │
│ i64 ┆ str   ┆ str    │
╞═════╪═══════╪════════╡
│ 1   ┆ blue  ┆ small  │
│ 2   ┆ red   ┆ medium │
└─────┴───────┴────────┘
</code></pre>
<p>We can specify multiple conditions in <code>filter</code> using the <code>&amp;</code> operator:</p>
<pre><code class="language-python">multi_filter_df = df.filter((pl.col(&quot;id&quot;) &lt;= 2) &amp; (pl.col(&quot;size&quot;) == &quot;small&quot;))
print(multi_filter_df)
</code></pre>
<pre><code class="language-text">shape: (1, 3)
┌─────┬───────┬───────┐
│ id  ┆ color ┆ size  │
│ --- ┆ ---   ┆ ---   │
│ i64 ┆ str   ┆ str   │
╞═════╪═══════╪═══════╡
│ 1   ┆ blue  ┆ small │
└─────┴───────┴───────┘
</code></pre>
<h2 id="selecting-columns-with-the-select-method"><a class="header" href="#selecting-columns-with-the-select-method">Selecting columns with the <code>select</code> method</a></h2>
<p>We select columns using the <code>select</code> method. In the <code>select</code> method we can specify the columns with:</p>
<ul>
<li>a (string) column name</li>
<li>a list of (string) column names</li>
<li>a boolean list of the same length as the number of columns</li>
<li>an expression such as a condition on the column name</li>
<li>a <code>Series</code></li>
</ul>
<h3 id="select-a-single-column"><a class="header" href="#select-a-single-column">Select a single column</a></h3>
<pre><code class="language-python">single_select_df = df.select(&quot;id&quot;)
print(single_select_df)
</code></pre>
<pre><code class="language-text">shape: (3, 1)
┌─────┐
│ id  │
│ --- │
│ i64 │
╞═════╡
│ 1   │
│ 2   │
│ 3   │
└─────┘
</code></pre>
<h3 id="select-a-list-of-columns"><a class="header" href="#select-a-list-of-columns">Select a list of columns</a></h3>
<pre><code class="language-python">list_select_df = df.select([&quot;id&quot;, &quot;color&quot;])
print(list_select_df)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌─────┬───────┐
│ id  ┆ color │
│ --- ┆ ---   │
│ i64 ┆ str   │
╞═════╪═══════╡
│ 1   ┆ blue  │
│ 2   ┆ red   │
│ 3   ┆ green │
└─────┴───────┘
</code></pre>
<h3 id="select-columns-with-an-expression"><a class="header" href="#select-columns-with-an-expression">Select columns with an expression</a></h3>
<p>To select based on a condition on the column name:</p>
<pre><code class="language-python">condition_select_df = df.select(pl.col(&quot;^col.*$&quot;))
print(condition_select_df)
</code></pre>
<pre><code class="language-text">shape: (3, 1)
┌───────┐
│ color │
│ ---   │
│ str   │
╞═══════╡
│ blue  │
│ red   │
│ green │
└───────┘
</code></pre>
<p>To select based on the dtype of the columns:</p>
<pre><code class="language-python">dtype_select_df = df.select(pl.col(pl.Int64))
print(dtype_select_df)
</code></pre>
<pre><code class="language-text">shape: (3, 1)
┌─────┐
│ id  │
│ --- │
│ i64 │
╞═════╡
│ 1   │
│ 2   │
│ 3   │
└─────┘
</code></pre>
<h1 id="selecting-rows-and-columns"><a class="header" href="#selecting-rows-and-columns">Selecting rows and columns</a></h1>
<p>We can combine the <code>filter</code> and <code>select</code> methods to select rows and columns</p>
<pre><code class="language-python">expression_df = df.filter(pl.col(&quot;id&quot;) &lt;= 2).select([&quot;id&quot;, &quot;color&quot;])
print(expression_df)
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌─────┬───────┐
│ id  ┆ color │
│ --- ┆ ---   │
│ i64 ┆ str   │
╞═════╪═══════╡
│ 1   ┆ blue  │
│ 2   ┆ red   │
└─────┴───────┘
</code></pre>
<h1 id="query-optimization"><a class="header" href="#query-optimization">Query optimization</a></h1>
<p>In lazy mode the query optimizer may be able to optimize the query based on the expressions.</p>
<p>In this example we scan a CSV file with many columns using <code>scan_csv</code> and then <code>select</code> a subset of them. The query optimizer creates a query plan that causes only the selected columns to be read from the CSV - see how the <code>Project</code> part of the query plan below states that only 1 of 2 columns will be read:</p>
<pre><code class="language-python">lazy_select_df = pl.scan_csv(&quot;data/appleStock.csv&quot;).select([&quot;Date&quot;])
print(lazy_select_df.describe_optimized_plan())
</code></pre>
<pre><code class="language-text">FAST_PROJECT: [Date]

    CSV SCAN data/appleStock.csv
    PROJECT 1/2 COLUMNS
</code></pre>
<p>If you specify two separate filter conditions the query optimizer will combine them into a single joint condition (see the <code>Selection</code> part of the query plan below):</p>
<pre><code class="language-python">lazy_filter_df = (
    pl.scan_csv(&quot;data/appleStock.csv&quot;)
    .filter(
        pl.col(&quot;Date&quot;) == datetime(1995, 10, 16),
    )
    .filter(pl.col(&quot;Close&quot;) &gt; 100)
)
print(lazy_filter_df.describe_optimized_plan())
</code></pre>
<pre><code class="language-text">
  CSV SCAN data/appleStock.csv
  PROJECT */2 COLUMNS
  SELECTION: [([(col(&quot;Date&quot;)) == (1995-10-16 00:00:00.cast(Utf8))]) &amp; ([(col(&quot;Close&quot;)) &gt; (100.0)])]
</code></pre>
<h1 id="selecting-with-indexing"><a class="header" href="#selecting-with-indexing">Selecting with indexing</a></h1>
<p>In this page we cover use of square bracket indexing to select data. Square bracket indexing can be used to select rows and/or columns.</p>
<h2 id="indexing-has-a-limited-use-case-in-polars"><a class="header" href="#indexing-has-a-limited-use-case-in-polars">Indexing has a limited use case in <code>Polars</code></a></h2>
<p>There are some use cases in Polars where square bracket indexing is effective. However, there are many use cases where indexing prevents you from using the full power of Polars.</p>
<p>Use cases where indexing <strong>is</strong> effective:</p>
<ul>
<li>to extract a scalar value from a <code>DataFrame</code></li>
<li>to convert a <code>DataFrame</code> column to a <code>Series</code></li>
<li>for exploratory data analysis and to inspect some rows and/or columns</li>
</ul>
<p>The first downside of indexing with square brackets is that indexing only works in eager mode. Any steps in your query that involve square bracket indexing cannot be included in a lazy query meaning that the step cannot be optimised as part of a lazy query and the step cannot be part of a streaming query that processes larger-than-memory data in batches</p>
<p>The second downside of indexing with square brackets is that operations on multiple columns are not parallelised.</p>
<p>Outside of the use cases noted above Polars <a href="howcani/selecting_data/selecting_data_expressions.html">strongly favours the expression API with <code>select</code> and <code>filter</code></a> in favor of accessing by square bracket indexing.</p>
<h2 id="rules-for-square-bracket-indexing"><a class="header" href="#rules-for-square-bracket-indexing">Rules for square bracket indexing</a></h2>
<p>The rules for square bracket indexing are as follows (depending on the datatypes of the values):</p>
<ul>
<li>
<p><strong>numeric</strong></p>
<ul>
<li>axis 0: row</li>
<li>axis 1: column</li>
</ul>
</li>
<li>
<p><strong>numeric + strings</strong></p>
<ul>
<li>axis 0: row (only accept numbers here)</li>
<li>axis 1: column (accept numeric + string values)</li>
</ul>
</li>
<li>
<p><strong>only strings</strong></p>
<ul>
<li>axis 0: column</li>
<li>axis 1: error</li>
</ul>
</li>
<li>
<p><strong>expressions</strong></p>
<p><em>All expression evaluations are executed in parallel</em></p>
<ul>
<li>axis 0: column</li>
<li>axis 1: column</li>
<li>..</li>
<li>axis n: column</li>
</ul>
</li>
</ul>
<h2 id="comparison-with-pandas"><a class="header" href="#comparison-with-pandas">Comparison with pandas</a></h2>
<table><thead><tr><th>pandas</th><th>polars</th></tr></thead><tbody>
<tr><td>select row<br> <code>df.iloc[2]</code></td><td><code>df[2, :]</code></td></tr>
<tr><td>select several rows by their indices<br> <code>df.iloc[[2, 5, 6]]</code></td><td><code>df[[2, 5, 6], :]</code></td></tr>
<tr><td>select slice of rows<br> <code>df.iloc[2:6]</code></td><td><code>df[2:6, :]</code></td></tr>
<tr><td>select rows using a boolean mask<br> <code>df.iloc[True, True, False]</code></td><td><code>df.filter([True, True, False])</code></td></tr>
<tr><td>select rows by a predicate condition<br> <code>df.loc[df[&quot;A&quot;] &gt; 3]</code></td><td><code>df[df[&quot;A&quot;] &gt; 3]</code></td></tr>
<tr><td>select slice of columns<br> <code>df.iloc[:, 1:3]</code></td><td><code>df[:, 1:3]</code></td></tr>
<tr><td>select slice of columns by string order<br> <code>df.loc[:, &quot;A&quot;:&quot;Z&quot;]</code></td><td><code>df[:, &quot;A&quot;:&quot;Z&quot;]</code></td></tr>
<tr><td>select a single value (scalar)<br> <code>df.loc[2, &quot;A&quot;]</code></td><td><code>df[2, &quot;A&quot;]</code></td></tr>
<tr><td>select a single value (scalar)<br> <code>df.iloc[2, 1]</code></td><td><code>df[2, 1]</code></td></tr>
<tr><td>select a single value (Series/DataFrame)<br> <code>df.loc[2, [&quot;A&quot;]]</code></td><td><code>df[2, [&quot;A&quot;]]</code></td></tr>
<tr><td>select a single value (Series/DataFrame)<br> <code>df.iloc[2, [1]]</code></td><td><code>df[2, [1]]</code></td></tr>
</tbody></table>
<h1 id="data-handling"><a class="header" href="#data-handling">Data handling</a></h1>
<h1 id="process-strings"><a class="header" href="#process-strings">Process strings</a></h1>
<p>Thanks to its <code>Arrow</code> backend, <code>Polars</code> string operations are much faster compared to the
same operations performed with <code>NumPy</code> or <code>Pandas</code>. In the latter, strings are stored as
<code>Python</code> objects. While traversing the <code>np.array</code> or the <code>pd.Series</code> the CPU needs to
follow all the string pointers, and jump to many random memory locations -- which
is very cache-inefficient. In <code>Polars</code> (via the <code>Arrow</code> data
structure) strings are contiguous in memory. Thus traversing is cache-optimal and
predictable for the CPU.</p>
<p>The string processing functions available in <code>Polars</code> are available in the
<a href="https://pola-rs.github.io/polars/py-polars/html/reference/series/strings.html"><code>str</code> namespace</a>.</p>
<p>Below are a few examples. To compute string lengths:</p>
<pre><code class="language-python">import polars as pl

df = pl.DataFrame({&quot;shakespeare&quot;: &quot;All that glitters is not gold&quot;.split(&quot; &quot;)})

df = df.with_columns(pl.col(&quot;shakespeare&quot;).str.lengths().alias(&quot;letter_count&quot;))
</code></pre>
<p>returning:</p>
<pre><code class="language-text">shape: (6, 2)
┌─────────────┬──────────────┐
│ shakespeare ┆ letter_count │
│ ---         ┆ ---          │
│ str         ┆ u32          │
╞═════════════╪══════════════╡
│ All         ┆ 3            │
│ that        ┆ 4            │
│ glitters    ┆ 8            │
│ is          ┆ 2            │
│ not         ┆ 3            │
│ gold        ┆ 4            │
└─────────────┴──────────────┘
</code></pre>
<p>And below a regex pattern to filter out articles (<code>the</code>, <code>a</code>, <code>and</code>, <em>etc.</em>) from a
sentence:</p>
<pre><code class="language-python">import polars as pl

df = pl.DataFrame({&quot;a&quot;: &quot;The man that ate a whole cake&quot;.split(&quot; &quot;)})

df = df.filter(pl.col(&quot;a&quot;).str.contains(r&quot;(?i)^the$|^a$&quot;).is_not())
</code></pre>
<p>yielding:</p>
<pre><code class="language-text">shape: (5, 1)
┌───────┐
│ a     │
│ ---   │
│ str   │
╞═══════╡
│ man   │
│ that  │
│ ate   │
│ whole │
│ cake  │
└───────┘
</code></pre>
<h1 id="timestamp-parsing"><a class="header" href="#timestamp-parsing">Timestamp parsing</a></h1>
<p><code>Polars</code> offers <code>4</code> time datatypes:</p>
<ul>
<li><code>pl.Date</code>, to be used for <strong>date</strong> objects: the number of days since the UNIX epoch as
a 32 bit signed integer.</li>
<li><code>pl.Datetime</code>, to be used for <strong>datetime</strong> objects: the number of nanoseconds since the
UNIX epoch as a 64 bit signed integer.</li>
<li><code>pl.Time</code>, encoded as the number of nanoseconds since midnight.</li>
<li><code>pl.Duration</code>, to be used for <strong>timedelta</strong> objects: the difference between Date,
Datetime or Time as a 64 bit signed integer offering microsecond resolution.</li>
</ul>
<p><code>Polars</code> string (<code>pl.Utf8</code>) datatypes can be parsed as either of them. You can let
<code>Polars</code> try to guess the format of the date[time], or explicitly provide a <code>fmt</code>
rule.</p>
<p>For instance (check <a href="https://docs.rs/chrono/latest/chrono/format/strftime/index.html">this link</a> for an comprehensive list):</p>
<ul>
<li><code>&quot;%Y-%m-%d&quot;</code> for <code>&quot;2020-12-31&quot;</code></li>
<li><code>&quot;%Y/%B/%d&quot;</code> for <code>&quot;2020/December/31&quot;</code></li>
<li><code>&quot;%B %y&quot;</code> for <code>&quot;December 20&quot;</code></li>
</ul>
<p>Below a quick example:</p>
<pre><code class="language-python">import polars as pl

dataset = pl.DataFrame({&quot;date&quot;: [&quot;2020-01-02&quot;, &quot;2020-01-03&quot;, &quot;2020-01-04&quot;], &quot;index&quot;: [1, 2, 3]})

q = dataset.lazy().with_columns(pl.col(&quot;date&quot;).str.strptime(pl.Date, &quot;%Y-%m-%d&quot;))

df = q.collect()
</code></pre>
<p>returning:</p>
<pre><code class="language-text">shape: (3, 2)
┌────────────┬───────┐
│ date       ┆ index │
│ ---        ┆ ---   │
│ date       ┆ i64   │
╞════════════╪═══════╡
│ 2020-01-02 ┆ 1     │
│ 2020-01-03 ┆ 2     │
│ 2020-01-04 ┆ 3     │
└────────────┴───────┘
</code></pre>
<p>All datetime functionality is shown in the <a href="https://pola-rs.github.io/polars/py-polars/html/reference/series/timeseries.html"><code>dt</code> namespace</a>.</p>
<h1 id="missing-data-1"><a class="header" href="#missing-data-1">Missing data</a></h1>
<p>This page sets out how missing data is represented in <code>Polars</code> and how missing data can be filled.</p>
<h2 id="null-and-nan-values"><a class="header" href="#null-and-nan-values"><code>null</code> and <code>NaN</code> values</a></h2>
<p>Each column in a <code>DataFrame</code> (or equivalently a <code>Series</code>) is an Arrow array or a collection of Arrow arrays <a href="https://arrow.apache.org/docs/format/Columnar.html#null-count">based on the Apache Arrow format</a>. Missing data is represented in Arrow and <code>Polars</code> with a <code>null</code> value. This <code>null</code> missing value applies for all data types including numerical values.</p>
<p><code>Polars</code> also allows <code>NotaNumber</code> or <code>NaN</code> values for float columns. These <code>NaN</code> values are considered to be a type of floating point data rather than missing data. We discuss <code>NaN</code> values separately below.</p>
<p>You can manually define a missing value with the python <code>None</code> value:</p>
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;value&quot;: [1, None],
    },
)
print(df)
</code></pre>
<pre><code class="language-text">shape: (2, 1)
┌───────┐
│ value │
│ ---   │
│ i64   │
╞═══════╡
│ 1     │
│ null  │
└───────┘
</code></pre>
<blockquote>
<p>In <code>Pandas</code> the value for missing data depends on the dtype of the column. In <code>Polars</code> missing data is always represented as a <code>null</code> value.</p>
</blockquote>
<h2 id="missing-data-metadata"><a class="header" href="#missing-data-metadata">Missing data metadata</a></h2>
<p>Each Arrow array used by <code>Polars</code> stores two kinds of metadata related to missing data. This metadata allows <code>Polars</code> to quickly show how many missing values there are and which values are missing.</p>
<p>The first piece of metadata is the <code>null_count</code> - this is the number of rows with <code>null</code> values in the column:</p>
<pre><code class="language-python">null_count_df = df.null_count()
print(null_count_df)
</code></pre>
<pre><code class="language-text">shape: (1, 1)
┌───────┐
│ value │
│ ---   │
│ u32   │
╞═══════╡
│ 1     │
└───────┘
</code></pre>
<p>The <code>null_count</code> method can be called on a <code>DataFrame</code>, a column from a <code>DataFrame</code> or a <code>Series</code>. The <code>null_count</code>method is a cheap operation as <code>null_count</code> is already calculated for the underlying Arrow array.</p>
<p>The second piece of metadata is an array called a <em>validity bitmap</em> that indicates whether each data value is valid or missing.
The validity bitmap is memory efficient as it is bit encoded - each value is either a 0 or a 1. This bit encoding means the memory overhead per array is only (array length / 8) bytes. The validity bitmap is used by the <code>is_null</code> method in <code>Polars</code>.</p>
<p>You can return a <code>Series</code> based on the validity bitmap for a column in a <code>DataFrame</code> or a <code>Series</code> with the <code>is_null</code> method:</p>
<pre><code class="language-python">is_null_series = df.select(
    pl.col(&quot;value&quot;).is_null(),
)
print(is_null_series)
</code></pre>
<pre><code class="language-text">shape: (2, 1)
┌───────┐
│ value │
│ ---   │
│ bool  │
╞═══════╡
│ false │
│ true  │
└───────┘
</code></pre>
<p>The <code>is_null</code> method is a cheap operation that does not require scanning the full column for <code>null</code> values. This is because the validity bitmap already exists and can be returned as a Boolean array.</p>
<h2 id="filling-missing-data"><a class="header" href="#filling-missing-data">Filling missing data</a></h2>
<p>Missing data in a <code>Series</code> can be filled with the <code>fill_null</code> method. You have to specify how you want the <code>fill_null</code> method to fill the missing data. The main ways to do this are filling with:</p>
<ul>
<li>a literal such as 0 or &quot;0&quot;</li>
<li>a strategy such as filling forwards</li>
<li>an expression such as replacing with values from another column</li>
<li>interpolation</li>
</ul>
<p>We illustrate each way to fill nulls by defining a simple <code>DataFrame</code> with a missing value in <code>col2</code>:</p>
<pre><code class="language-python">df = pl.DataFrame(
    {
        &quot;col1&quot;: [1, 2, 3],
        &quot;col2&quot;: [1, None, 3],
    },
)
print(df)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌──────┬──────┐
│ col1 ┆ col2 │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 1    │
│ 2    ┆ null │
│ 3    ┆ 3    │
└──────┴──────┘
</code></pre>
<h3 id="fill-with-specified-literal-value"><a class="header" href="#fill-with-specified-literal-value">Fill with specified literal value</a></h3>
<p>We can fill the missing data with a specified literal value with <code>pl.lit</code>:</p>
<pre><code class="language-python">fill_literal_df = (
    df.with_columns(
        pl.col(&quot;col2&quot;).fill_null(
            pl.lit(2),
        ),
    ),
)
print(fill_literal_df)
</code></pre>
<pre><code class="language-text">(shape: (3, 2)
┌──────┬──────┐
│ col1 ┆ col2 │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 1    │
│ 2    ┆ 2    │
│ 3    ┆ 3    │
└──────┴──────┘,)
</code></pre>
<h3 id="fill-with-a-strategy"><a class="header" href="#fill-with-a-strategy">Fill with a strategy</a></h3>
<p>We can fill the missing data with a strategy such as filling forward:</p>
<pre><code class="language-python">fill_forward_df = df.with_columns(
    pl.col(&quot;col2&quot;).fill_null(strategy=&quot;forward&quot;),
)
print(fill_forward_df)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌──────┬──────┐
│ col1 ┆ col2 │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 1    │
│ 2    ┆ 1    │
│ 3    ┆ 3    │
└──────┴──────┘
</code></pre>
<p>See the <a href="https://pola-rs.github.io/polars/py-polars/html/reference/series/api/polars.Series.fill_null.html">API docs</a> for other available strategies.</p>
<h3 id="fill-with-an-expression"><a class="header" href="#fill-with-an-expression">Fill with an expression</a></h3>
<p>For more flexibility we can fill the missing data with an expression. For example,
to fill nulls with the median value from that column:</p>
<pre><code class="language-python">fill_median_df = df.with_columns(
    pl.col(&quot;col2&quot;).fill_null(pl.median(&quot;col2&quot;)),
)
print(fill_median_df)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌──────┬──────┐
│ col1 ┆ col2 │
│ ---  ┆ ---  │
│ i64  ┆ f64  │
╞══════╪══════╡
│ 1    ┆ 1.0  │
│ 2    ┆ 2.0  │
│ 3    ┆ 3.0  │
└──────┴──────┘
</code></pre>
<p>In this case the column is cast from integer to float because the median is a float statistic.</p>
<h3 id="fill-with-interpolation"><a class="header" href="#fill-with-interpolation">Fill with interpolation</a></h3>
<p>In addition, we can fill nulls with interpolation (without using the <code>fill_null</code> function):</p>
<pre><code class="language-python">fill_interpolation_df = df.with_columns(
    pl.col(&quot;col2&quot;).interpolate(),
)
print(fill_interpolation_df)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌──────┬──────┐
│ col1 ┆ col2 │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 1    │
│ 2    ┆ 2    │
│ 3    ┆ 3    │
└──────┴──────┘
</code></pre>
<h2 id="notanumber-or-nan-values"><a class="header" href="#notanumber-or-nan-values"><code>NotaNumber</code> or <code>NaN</code> values</a></h2>
<p>Missing data in a <code>Series</code> has a <code>null</code> value. However, you can use <code>NotaNumber</code> or <code>NaN</code> values in columns with float datatypes. These <code>NaN</code> values can be created from Numpy's <code>np.nan</code> or the native python <code>float('nan')</code>:</p>
<pre><code class="language-python">nan_df = pl.DataFrame(
    {
        &quot;value&quot;: [1.0, np.NaN, float(&quot;nan&quot;), 3.0],
    },
)
print(nan_df)
</code></pre>
<pre><code class="language-text">shape: (4, 1)
┌───────┐
│ value │
│ ---   │
│ f64   │
╞═══════╡
│ 1.0   │
│ NaN   │
│ NaN   │
│ 3.0   │
└───────┘
</code></pre>
<blockquote>
<p>In <code>Pandas</code> by default a <code>NaN</code> value in an integer column causes the column to be cast to float. This does not happen in <code>Polars</code> - instead an exception is raised.</p>
</blockquote>
<p><code>NaN</code> values are considered to be a type of floating point data and are <strong>not considered to be missing data</strong> in <code>Polars</code>. This means:</p>
<ul>
<li><code>NaN</code> values are <strong>not</strong> counted with the <code>null_count</code> method</li>
<li><code>NaN</code> values are filled when you use <code>fill_nan</code> method but are <strong>not</strong> filled with the <code>fill_null</code> method</li>
</ul>
<p><code>Polars</code> has <code>is_nan</code> and <code>fill_nan</code> methods which work in a similar way to the <code>is_null</code> and <code>fill_null</code> methods. The underlying Arrow arrays do not have a pre-computed validity bitmask for <code>NaN</code> values so this has to be computed for the <code>is_nan</code> method.</p>
<p>One further difference between <code>null</code> and <code>NaN</code> values is that taking the <code>mean</code> of a column with <code>null</code> values excludes the <code>null</code> values from the calculation but with <code>NaN</code> values taking the mean results in a <code>NaN</code>. This behaviour can be avoided by replacing the <code>NaN</code> values with <code>null</code> values;</p>
<pre><code class="language-python">mean_nan_df = nan_df.with_columns(
    pl.col(&quot;value&quot;).fill_nan(None).alias(&quot;value&quot;),
).mean()
print(mean_nan_df)
</code></pre>
<pre><code class="language-text">shape: (1, 1)
┌───────┐
│ value │
│ ---   │
│ f64   │
╞═══════╡
│ 2.0   │
└───────┘
</code></pre>
<h1 id="time-series"><a class="header" href="#time-series">Time Series</a></h1>
<p>Polars has native support for parsing time series data and doing more sophisticated operations such as temporal grouping and resampling.</p>
<ul>
<li><a href="howcani/timeseries/parsing_dates_times.html">Parsing dates and times</a></li>
<li><a href="howcani/timeseries/selecting_dates.html">Filtering date columns</a></li>
<li><a href="howcani/timeseries/temporal_groupby.html">Temporal groupby</a></li>
<li><a href="howcani/timeseries/resampling.html">Resampling</a></li>
</ul>
<h1 id="parsing-dates-and-times"><a class="header" href="#parsing-dates-and-times">Parsing dates and times</a></h1>
<h2 id="datatypes"><a class="header" href="#datatypes">Datatypes</a></h2>
<p><code>Polars</code> has the following datetime datatypes:</p>
<ul>
<li><code>Date</code>: Date representation e.g. 2014-07-08. It is internally represented as days since UNIX epoch encoded by a 32-bit signed integer.</li>
<li><code>Datetime</code>: Datetime representation e.g. 2014-07-08 07:00:00. It is internally represented as a 64 bit integer since the Unix epoch and can have different units such as ns, us, ms.</li>
<li><code>Duration</code>: A time delta type that is created when subtracting <code>Date/Datetime</code>. Similar to <code>timedelta</code> in python.</li>
<li><code>Time</code>: Time representation, internally represented as nanoseconds since midnight.</li>
</ul>
<h2 id="parsing-dates-from-a-file"><a class="header" href="#parsing-dates-from-a-file">Parsing dates from a file</a></h2>
<p>When loading from a CSV file <code>Polars</code> attempts to parse dates and times if the <code>parse_dates</code> flag is set to <code>True</code>:</p>
<pre><code class="language-python">df = pl.read_csv(&quot;data/appleStock.csv&quot;, parse_dates=True)
print(df)
</code></pre>
<pre><code class="language-text">shape: (100, 2)
┌────────────┬────────┐
│ Date       ┆ Close  │
│ ---        ┆ ---    │
│ date       ┆ f64    │
╞════════════╪════════╡
│ 1981-02-23 ┆ 24.62  │
│ 1981-05-06 ┆ 27.38  │
│ 1981-05-18 ┆ 28.0   │
│ 1981-09-25 ┆ 14.25  │
│ …          ┆ …      │
│ 2012-12-04 ┆ 575.85 │
│ 2013-07-05 ┆ 417.42 │
│ 2013-11-07 ┆ 512.49 │
│ 2014-02-25 ┆ 522.06 │
└────────────┴────────┘
</code></pre>
<p>On the other hand binary formats such as parquet have a schema that is respected by <code>Polars</code>.</p>
<h2 id="casting-strings-to-dates"><a class="header" href="#casting-strings-to-dates">Casting strings to dates</a></h2>
<p>You can also cast a column of datetimes encoded as strings to a datetime type. You do this by calling the string <code>str.strptime</code> method and passing the format of the date string:</p>
<pre><code class="language-python">df = pl.read_csv(&quot;data/appleStock.csv&quot;, parse_dates=False)

df = df.with_columns(pl.col(&quot;Date&quot;).str.strptime(pl.Date, fmt=&quot;%Y-%m-%d&quot;))
print(df)
</code></pre>
<pre><code class="language-text">shape: (100, 2)
┌────────────┬────────┐
│ Date       ┆ Close  │
│ ---        ┆ ---    │
│ date       ┆ f64    │
╞════════════╪════════╡
│ 1981-02-23 ┆ 24.62  │
│ 1981-05-06 ┆ 27.38  │
│ 1981-05-18 ┆ 28.0   │
│ 1981-09-25 ┆ 14.25  │
│ …          ┆ …      │
│ 2012-12-04 ┆ 575.85 │
│ 2013-07-05 ┆ 417.42 │
│ 2013-11-07 ┆ 512.49 │
│ 2014-02-25 ┆ 522.06 │
└────────────┴────────┘
</code></pre>
<p><a href="https://docs.rs/chrono/latest/chrono/format/strftime/index.html">The strptime date formats can be found here.</a>.</p>
<h2 id="extracting-date-features-from-a-date-column"><a class="header" href="#extracting-date-features-from-a-date-column">Extracting date features from a date column</a></h2>
<p>You can extract data features such as the year or day from a date column using the <code>.dt</code> namespace on a date column:</p>
<pre><code class="language-python">df_with_year = df.with_column(pl.col(&quot;Date&quot;).dt.year().alias(&quot;year&quot;))
print(df_with_year)
</code></pre>
<pre><code class="language-text">shape: (100, 3)
┌────────────┬────────┬──────┐
│ Date       ┆ Close  ┆ year │
│ ---        ┆ ---    ┆ ---  │
│ date       ┆ f64    ┆ i32  │
╞════════════╪════════╪══════╡
│ 1981-02-23 ┆ 24.62  ┆ 1981 │
│ 1981-05-06 ┆ 27.38  ┆ 1981 │
│ 1981-05-18 ┆ 28.0   ┆ 1981 │
│ 1981-09-25 ┆ 14.25  ┆ 1981 │
│ …          ┆ …      ┆ …    │
│ 2012-12-04 ┆ 575.85 ┆ 2012 │
│ 2013-07-05 ┆ 417.42 ┆ 2013 │
│ 2013-11-07 ┆ 512.49 ┆ 2013 │
│ 2014-02-25 ┆ 522.06 ┆ 2014 │
└────────────┴────────┴──────┘
</code></pre>
<p>See the <a href="https://pola-rs.github.io/polars/py-polars/html/reference/series/timeseries.html">API docs</a>
for more date feature options.</p>
<h2 id="mixed-offsets"><a class="header" href="#mixed-offsets">Mixed offsets</a></h2>
<p>If you have mixed offsets (say, due to crossing daylight saving time),
then you can use <code>utc=True</code> and then convert to your time zone:</p>
<pre><code class="language-python">{{#include ../../examples/time_series/time_zones/mixed_offsets.py:03:09}}
</code></pre>
<pre><code class="language-text">{{#include ../../outputs/time_series/time_zones/mixed_parsed.txt}}
</code></pre>
<h1 id="filtering-date-columns"><a class="header" href="#filtering-date-columns">Filtering date columns</a></h1>
<p>Filtering date columns works in the same way as with other types of columns using the <code>.filter</code> method.</p>
<p>Polars uses Python's native <code>datetime</code>, <code>date</code> and <code>timedelta</code> for equality comparisons between the datatypes
<code>pl.Datetime</code>, <code>pl.Date</code> and <code>pl.Duration</code>.</p>
<p>In the following example we use a time series of Apple stock prices.</p>
<pre><code class="language-python">import polars as pl
from datetime import datetime

df = pl.read_csv(&quot;data/appleStock.csv&quot;, parse_dates=True)
print(df)
</code></pre>
<pre><code class="language-text">shape: (100, 2)
┌────────────┬────────┐
│ Date       ┆ Close  │
│ ---        ┆ ---    │
│ date       ┆ f64    │
╞════════════╪════════╡
│ 1981-02-23 ┆ 24.62  │
│ 1981-05-06 ┆ 27.38  │
│ 1981-05-18 ┆ 28.0   │
│ 1981-09-25 ┆ 14.25  │
│ …          ┆ …      │
│ 2012-12-04 ┆ 575.85 │
│ 2013-07-05 ┆ 417.42 │
│ 2013-11-07 ┆ 512.49 │
│ 2014-02-25 ┆ 522.06 │
└────────────┴────────┘
</code></pre>
<h2 id="filtering-by-single-dates"><a class="header" href="#filtering-by-single-dates">Filtering by single dates</a></h2>
<p>We can filter by a single date by casting the desired date string to a <code>Date</code> object
in a filter expression:</p>
<pre><code class="language-python">filtered_df = df.filter(
    pl.col(&quot;Date&quot;) == datetime(1995, 10, 16),
)
</code></pre>
<pre><code class="language-text">shape: (1, 2)
┌────────────┬───────┐
│ Date       ┆ Close │
│ ---        ┆ ---   │
│ date       ┆ f64   │
╞════════════╪═══════╡
│ 1995-10-16 ┆ 36.13 │
└────────────┴───────┘
</code></pre>
<p>Note we are using the lowercase <code>datetime</code> method rather than the uppercase <code>Datetime</code> data type.</p>
<h2 id="filtering-by-a-date-range"><a class="header" href="#filtering-by-a-date-range">Filtering by a date range</a></h2>
<p>We can filter by a range of dates using the <code>is_between</code> method in a filter expression with the start and end dates:</p>
<pre><code class="language-python">filtered_range_df = df.filter(
    pl.col(&quot;Date&quot;).is_between(datetime(1995, 7, 1), datetime(1995, 11, 1)),
)
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌────────────┬───────┐
│ Date       ┆ Close │
│ ---        ┆ ---   │
│ date       ┆ f64   │
╞════════════╪═══════╡
│ 1995-07-06 ┆ 47.0  │
│ 1995-10-16 ┆ 36.13 │
└────────────┴───────┘
</code></pre>
<h2 id="filtering-with-negative-dates"><a class="header" href="#filtering-with-negative-dates">Filtering with negative dates</a></h2>
<p>Say you are working with an archeologist and are dealing in negative dates.
Polars can parse and store them just fine, but the Python <code>datetime</code> library
does not. So for filtering, you should use attributes in the <code>.dt</code> namespace:</p>
<pre><code class="language-python">ts = pl.Series([&quot;-1300-05-23&quot;, &quot;-1400-03-02&quot;]).str.strptime(pl.Date)

negative_dates_df = pl.DataFrame({&quot;ts&quot;: ts, &quot;values&quot;: [3, 4]})

negative_dates_filtered_df = negative_dates_df.filter(pl.col(&quot;ts&quot;).dt.year() &lt; -1300)
</code></pre>
<pre><code class="language-text">shape: (1, 2)
┌─────────────┬────────┐
│ ts          ┆ values │
│ ---         ┆ ---    │
│ date        ┆ i64    │
╞═════════════╪════════╡
│ -1400-03-02 ┆ 4      │
└─────────────┴────────┘
</code></pre>
<h1 id="fixed-and-rolling-temporal-groupby"><a class="header" href="#fixed-and-rolling-temporal-groupby">Fixed and rolling temporal groupby</a></h1>
<h2 id="grouping-by-fixed-windows-with-groupby_dynamic"><a class="header" href="#grouping-by-fixed-windows-with-groupby_dynamic">Grouping by fixed windows with <code>groupby_dynamic</code></a></h2>
<p>We can calculate temporal statistics using <code>groupby_dynamic</code> to group rows into days/months/years etc.</p>
<h3 id="annual-average-example"><a class="header" href="#annual-average-example">Annual average example</a></h3>
<p>In following simple example we calculate the annual average closing price of Apple stock prices. We first load the data from CSV:</p>
<pre><code class="language-python">df = pl.read_csv(&quot;data/appleStock.csv&quot;, parse_dates=True)
print(df)
</code></pre>
<pre><code class="language-text">shape: (100, 2)
┌────────────┬────────┐
│ Date       ┆ Close  │
│ ---        ┆ ---    │
│ date       ┆ f64    │
╞════════════╪════════╡
│ 1981-02-23 ┆ 24.62  │
│ 1981-05-06 ┆ 27.38  │
│ 1981-05-18 ┆ 28.0   │
│ 1981-09-25 ┆ 14.25  │
│ …          ┆ …      │
│ 2012-12-04 ┆ 575.85 │
│ 2013-07-05 ┆ 417.42 │
│ 2013-11-07 ┆ 512.49 │
│ 2014-02-25 ┆ 522.06 │
└────────────┴────────┘
</code></pre>
<blockquote>
<p>The dates are sorted in ascending order - if they are not sorted in this way the <code>groupby_dynamic</code> output will not be correct!</p>
</blockquote>
<p>To get the annual average closing price we tell <code>groupby_dynamic</code> that we want to:</p>
<ul>
<li>group by the <code>Date</code> column on an annual (<code>1y</code>) basis</li>
<li>take the mean values of the <code>Close</code> column for each year:</li>
</ul>
<pre><code class="language-python">annual_average_df = df.groupby_dynamic(&quot;Date&quot;, every=&quot;1y&quot;).agg(pl.col(&quot;Close&quot;).mean())

df_with_year = df.with_column(pl.col(&quot;Date&quot;).dt.year().alias(&quot;year&quot;))

</code></pre>
<p>The annual average closing price is then:</p>
<pre><code class="language-text">shape: (34, 2)
┌────────────┬───────────┐
│ Date       ┆ Close     │
│ ---        ┆ ---       │
│ date       ┆ f64       │
╞════════════╪═══════════╡
│ 1981-01-01 ┆ 23.5625   │
│ 1982-01-01 ┆ 11.0      │
│ 1983-01-01 ┆ 30.543333 │
│ 1984-01-01 ┆ 27.583333 │
│ …          ┆ …         │
│ 2011-01-01 ┆ 368.225   │
│ 2012-01-01 ┆ 560.965   │
│ 2013-01-01 ┆ 464.955   │
│ 2014-01-01 ┆ 522.06    │
└────────────┴───────────┘
</code></pre>
<h3 id="parameters-for-groupby_dynamic"><a class="header" href="#parameters-for-groupby_dynamic">Parameters for <code>groupby_dynamic</code></a></h3>
<p>A dynamic window is defined by a:</p>
<ul>
<li><strong>every</strong>: indicates the interval of the window</li>
<li><strong>period</strong>: indicates the duration of the window</li>
<li><strong>offset</strong>: can be used to offset the start of the windows</li>
</ul>
<h4 id="every-period-and-offset-parameters-to-control-temporal-window-size"><a class="header" href="#every-period-and-offset-parameters-to-control-temporal-window-size"><code>every</code>, <code>period</code> and <code>offset</code> parameters to control temporal window size</a></h4>
<p>The value for <code>every</code> sets how often the groups start. The time period values are flexible - for example we could take:</p>
<ul>
<li>the average over 2 year intervals by replacing <code>1y</code> with <code>2y</code></li>
<li>the average over 18 month periods by replacing <code>1y</code> with <code>1y6mo</code></li>
</ul>
<p>We can also use the <code>period</code> parameter to set how long the time period for each group is. For example, if we set the <code>every</code> parameter to be <code>1y</code> and the <code>period</code> parameter to be <code>2y</code> then we would get groups at one year intervals where each groups spanned two years.</p>
<p>If the <code>period</code> parameter is not specified then it is set equal to the <code>every</code> parameter so that if the <code>every</code> parameter is set to be <code>1y</code> then each group spans <code>1y</code> as well.</p>
<p>Because <em><strong>every</strong></em> does not have to be equal to <em><strong>period</strong></em>, we can create many groups in a very flexible way. They may overlap
or leave boundaries between them.</p>
<p>Let's see how the windows for some parameter combinations would look. Let's start out boring. 🥱</p>
<blockquote>
</blockquote>
<ul>
<li>every: 1 day -&gt; <code>&quot;1d&quot;</code></li>
<li>period: 1 day -&gt; <code>&quot;1d&quot;</code></li>
</ul>
<pre><code class="language-text">this creates adjacent windows of the same size
|--|
   |--|
      |--|
</code></pre>
<blockquote>
</blockquote>
<ul>
<li>every: 1 day -&gt; <code>&quot;1d&quot;</code></li>
<li>period: 2 days -&gt; <code>&quot;2d&quot;</code></li>
</ul>
<pre><code class="language-text">these windows have an overlap of 1 day
|----|
   |----|
      |----|
</code></pre>
<blockquote>
</blockquote>
<ul>
<li>every: 2 days -&gt; <code>&quot;2d&quot;</code></li>
<li>period: 1 day -&gt; <code>&quot;1d&quot;</code></li>
</ul>
<pre><code class="language-text">this would leave gaps between the windows
data points that in these gaps will not be a member of any group
|--|
       |--|
              |--|
</code></pre>
<p>See <a href="https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.groupby_dynamic.html">the API pages</a> for the full range of time periods.</p>
<h4 id="truncate-parameter-to-set-the-start-date-for-each-group"><a class="header" href="#truncate-parameter-to-set-the-start-date-for-each-group"><code>truncate</code> parameter to set the start date for each group</a></h4>
<p>The <code>truncate</code> parameter is a Boolean variable that determines what datetime value is associated with each group in the output. In the example above the first data point is on 23rd February 1981. If <code>truncate = True</code> (the default) then the date for the first year in the annual average is 1st January 1981. However, if <code>truncate = False</code> then the date for the first year in the annual average is the date of the first data point on 23rd February 1981.</p>
<h3 id="using-expressions-in-groupby_dynamic"><a class="header" href="#using-expressions-in-groupby_dynamic">Using expressions in <code>groupby_dynamic</code></a></h3>
<p>We aren't restricted to using simple aggregations like <code>mean</code> in a groupby operation - we can use the full range of expressions available in Polars.</p>
<p>In the snippet below we create a <code>date range</code> with every <strong>day</strong> (<code>&quot;1d&quot;</code>) in 2021 and turn this into a <code>DataFrame</code>.</p>
<p>Then in the <code>groupby_dynamic</code> we create dynamic windows that start every <strong>month</strong> (<code>&quot;1mo&quot;</code>) and have a window length of <code>1</code> month. The values that match these dynamic windows are then assigned to that group and can be aggregated with the powerful expression API.</p>
<p>Below we show an example where we use <strong>groupby_dynamic</strong> to compute:</p>
<ul>
<li>the number of days until the end of the month</li>
<li>the number of days in a month</li>
</ul>
<pre><code class="language-python">
df = pl.date_range(low=datetime(2021, 1, 1), high=datetime(2021, 12, 31), interval=&quot;1d&quot;, name=&quot;time&quot;).to_frame()

out = (
    df.groupby_dynamic(&quot;time&quot;, every=&quot;1mo&quot;, period=&quot;1mo&quot;, closed=&quot;left&quot;)
    .agg(
        [
            pl.col(&quot;time&quot;).cumcount().reverse().head(3).alias(&quot;day/eom&quot;),
            ((pl.col(&quot;time&quot;) - pl.col(&quot;time&quot;).first()).last().dt.days() + 1).alias(&quot;days_in_month&quot;),
        ]
    )
    .explode(&quot;day/eom&quot;)
)
print(out)
</code></pre>
<pre><code class="language-text">shape: (36, 3)
┌─────────────────────┬─────────┬───────────────┐
│ time                ┆ day/eom ┆ days_in_month │
│ ---                 ┆ ---     ┆ ---           │
│ datetime[μs]        ┆ u32     ┆ i64           │
╞═════════════════════╪═════════╪═══════════════╡
│ 2021-01-01 00:00:00 ┆ 30      ┆ 31            │
│ 2021-01-01 00:00:00 ┆ 29      ┆ 31            │
│ 2021-01-01 00:00:00 ┆ 28      ┆ 31            │
│ 2021-02-01 00:00:00 ┆ 27      ┆ 28            │
│ …                   ┆ …       ┆ …             │
│ 2021-11-01 00:00:00 ┆ 27      ┆ 30            │
│ 2021-12-01 00:00:00 ┆ 30      ┆ 31            │
│ 2021-12-01 00:00:00 ┆ 29      ┆ 31            │
│ 2021-12-01 00:00:00 ┆ 28      ┆ 31            │
└─────────────────────┴─────────┴───────────────┘
</code></pre>
<h2 id="grouping-by-rolling-windows-with-groupby_rolling"><a class="header" href="#grouping-by-rolling-windows-with-groupby_rolling">Grouping by rolling windows with <code>groupby_rolling</code></a></h2>
<p>The rolling groupby is another entrance to the <code>groupby</code> context. But different from the <code>groupby_dynamic</code> the windows are
not fixed by a parameter <code>every</code> and <code>period</code>. In a rolling groupby the windows are not fixed at all! They are determined
by the values in the <code>index_column</code>.</p>
<p>So imagine having a time column with the values <code>{2021-01-06, 20210-01-10}</code> and a <code>period=&quot;5d&quot;</code> this would create the following
windows:</p>
<pre><code class="language-text">
2021-01-01   2021-01-06
    |----------|

       2021-01-05   2021-01-10
             |----------|
</code></pre>
<p>Because the windows of a rolling groupby are always determined by the values in the <code>DataFrame</code> column, the number of
groups is always equal to the original <code>DataFrame</code>.</p>
<h2 id="combining-groupby-and-dynamic--rolling"><a class="header" href="#combining-groupby-and-dynamic--rolling">Combining Groupby and Dynamic / Rolling</a></h2>
<p>Rolling and dynamic groupby's can be combined with normal groupby operations.</p>
<p>Below is an example with a dynamic groupby.</p>
<pre><code class="language-python">from datetime import datetime

import polars as pl

df = pl.DataFrame(
    {
        &quot;time&quot;: pl.date_range(
            low=datetime(2021, 12, 16),
            high=datetime(2021, 12, 16, 3),
            interval=&quot;30m&quot;,
        ),
        &quot;groups&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;],
    }
)
print(df)
</code></pre>
<pre><code class="language-text">shape: (7, 2)
┌─────────────────────┬────────┐
│ time                ┆ groups │
│ ---                 ┆ ---    │
│ datetime[μs]        ┆ str    │
╞═════════════════════╪════════╡
│ 2021-12-16 00:00:00 ┆ a      │
│ 2021-12-16 00:30:00 ┆ a      │
│ 2021-12-16 01:00:00 ┆ a      │
│ 2021-12-16 01:30:00 ┆ b      │
│ 2021-12-16 02:00:00 ┆ b      │
│ 2021-12-16 02:30:00 ┆ a      │
│ 2021-12-16 03:00:00 ┆ a      │
└─────────────────────┴────────┘
</code></pre>
<pre><code class="language-python">
out = df.groupby_dynamic(&quot;time&quot;, every=&quot;1h&quot;, closed=&quot;both&quot;, by=&quot;groups&quot;, include_boundaries=True,).agg(
    [
        pl.count(),
    ]
)
print(out)
</code></pre>
<pre><code class="language-text">shape: (7, 5)
┌────────┬─────────────────────┬─────────────────────┬─────────────────────┬───────┐
│ groups ┆ _lower_boundary     ┆ _upper_boundary     ┆ time                ┆ count │
│ ---    ┆ ---                 ┆ ---                 ┆ ---                 ┆ ---   │
│ str    ┆ datetime[μs]        ┆ datetime[μs]        ┆ datetime[μs]        ┆ u32   │
╞════════╪═════════════════════╪═════════════════════╪═════════════════════╪═══════╡
│ a      ┆ 2021-12-15 23:00:00 ┆ 2021-12-16 00:00:00 ┆ 2021-12-15 23:00:00 ┆ 1     │
│ a      ┆ 2021-12-16 00:00:00 ┆ 2021-12-16 01:00:00 ┆ 2021-12-16 00:00:00 ┆ 3     │
│ a      ┆ 2021-12-16 01:00:00 ┆ 2021-12-16 02:00:00 ┆ 2021-12-16 01:00:00 ┆ 1     │
│ a      ┆ 2021-12-16 02:00:00 ┆ 2021-12-16 03:00:00 ┆ 2021-12-16 02:00:00 ┆ 2     │
│ a      ┆ 2021-12-16 03:00:00 ┆ 2021-12-16 04:00:00 ┆ 2021-12-16 03:00:00 ┆ 1     │
│ b      ┆ 2021-12-16 01:00:00 ┆ 2021-12-16 02:00:00 ┆ 2021-12-16 01:00:00 ┆ 2     │
│ b      ┆ 2021-12-16 02:00:00 ┆ 2021-12-16 03:00:00 ┆ 2021-12-16 02:00:00 ┆ 1     │
└────────┴─────────────────────┴─────────────────────┴─────────────────────┴───────┘
</code></pre>
<h1 id="resampling"><a class="header" href="#resampling">Resampling</a></h1>
<p>We can resample by either:</p>
<ul>
<li>upsampling (moving data to a higher frequency)</li>
<li>downsampling (moving data to a lower frequency)</li>
<li>combinations of these e.g. first upsample and then downsample</li>
</ul>
<h2 id="downsampling-to-a-lower-frequency"><a class="header" href="#downsampling-to-a-lower-frequency">Downsampling to a lower frequency</a></h2>
<p><code>Polars</code> views downsampling as a special case of the <strong>groupby</strong> operation and you can do this with <code>groupby_dynamic</code> and <code>groupby_rolling</code> - <a href="howcani/timeseries/temporal_groupby.html">see the temporal groupby page for examples</a>.</p>
<h2 id="upsampling-to-a-higher-frequency"><a class="header" href="#upsampling-to-a-higher-frequency">Upsampling to a higher frequency</a></h2>
<p>Let's go through an example where we generate data at 30 minute intervals:</p>
<pre><code class="language-python">from datetime import datetime

import polars as pl

df = pl.DataFrame(
    {
        &quot;time&quot;: pl.date_range(low=datetime(2021, 12, 16), high=datetime(2021, 12, 16, 3), interval=&quot;30m&quot;),
        &quot;groups&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;],
        &quot;values&quot;: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
    }
print(df)
</code></pre>
<pre><code class="language-text">shape: (7, 3)
┌─────────────────────┬────────┬────────┐
│ time                ┆ groups ┆ values │
│ ---                 ┆ ---    ┆ ---    │
│ datetime[μs]        ┆ str    ┆ f64    │
╞═════════════════════╪════════╪════════╡
│ 2021-12-16 00:00:00 ┆ a      ┆ 1.0    │
│ 2021-12-16 00:30:00 ┆ a      ┆ 2.0    │
│ 2021-12-16 01:00:00 ┆ a      ┆ 3.0    │
│ 2021-12-16 01:30:00 ┆ b      ┆ 4.0    │
│ 2021-12-16 02:00:00 ┆ b      ┆ 5.0    │
│ 2021-12-16 02:30:00 ┆ a      ┆ 6.0    │
│ 2021-12-16 03:00:00 ┆ a      ┆ 7.0    │
└─────────────────────┴────────┴────────┘
</code></pre>
<p>Upsampling can be done by defining the new sampling interval. By upsampling we are adding in extra rows where we do not have data. As such upsampling by itself gives a DataFrame with nulls. These nulls can then be filled with a fill strategy or interpolation.</p>
<h3 id="upsampling-strategies"><a class="header" href="#upsampling-strategies">Upsampling strategies</a></h3>
<p>In this example we upsample from the original 30 minutes to 15 minutes and then use a <code>forward</code> strategy to replace the nulls with the previous non-null value:</p>
<pre><code class="language-python">out1 = df.upsample(time_column=&quot;time&quot;, every=&quot;15m&quot;).fill_null(strategy=&quot;forward&quot;)
print(out1)
</code></pre>
<pre><code class="language-text">shape: (13, 3)
┌─────────────────────┬────────┬────────┐
│ time                ┆ groups ┆ values │
│ ---                 ┆ ---    ┆ ---    │
│ datetime[μs]        ┆ str    ┆ f64    │
╞═════════════════════╪════════╪════════╡
│ 2021-12-16 00:00:00 ┆ a      ┆ 1.0    │
│ 2021-12-16 00:15:00 ┆ a      ┆ 1.0    │
│ 2021-12-16 00:30:00 ┆ a      ┆ 2.0    │
│ 2021-12-16 00:45:00 ┆ a      ┆ 2.0    │
│ …                   ┆ …      ┆ …      │
│ 2021-12-16 02:15:00 ┆ b      ┆ 5.0    │
│ 2021-12-16 02:30:00 ┆ a      ┆ 6.0    │
│ 2021-12-16 02:45:00 ┆ a      ┆ 6.0    │
│ 2021-12-16 03:00:00 ┆ a      ┆ 7.0    │
└─────────────────────┴────────┴────────┘
</code></pre>
<p>In this example we instead fill the nulls by linear interpolation:</p>
<pre><code class="language-python">out2 = df.upsample(time_column=&quot;time&quot;, every=&quot;15m&quot;).interpolate().fill_null(strategy=&quot;forward&quot;)
print(out2)
</code></pre>
<pre><code class="language-text">shape: (13, 3)
┌─────────────────────┬────────┬────────┐
│ time                ┆ groups ┆ values │
│ ---                 ┆ ---    ┆ ---    │
│ datetime[μs]        ┆ str    ┆ f64    │
╞═════════════════════╪════════╪════════╡
│ 2021-12-16 00:00:00 ┆ a      ┆ 1.0    │
│ 2021-12-16 00:15:00 ┆ a      ┆ 1.5    │
│ 2021-12-16 00:30:00 ┆ a      ┆ 2.0    │
│ 2021-12-16 00:45:00 ┆ a      ┆ 2.5    │
│ …                   ┆ …      ┆ …      │
│ 2021-12-16 02:15:00 ┆ b      ┆ 5.5    │
│ 2021-12-16 02:30:00 ┆ a      ┆ 6.0    │
│ 2021-12-16 02:45:00 ┆ a      ┆ 6.5    │
│ 2021-12-16 03:00:00 ┆ a      ┆ 7.0    │
└─────────────────────┴────────┴────────┘
</code></pre>
<h1 id="time-zones"><a class="header" href="#time-zones">Time zones</a></h1>
<blockquote>
<p>You really should never, ever deal with time zones if you can help it</p>
</blockquote>
<p>-- <cite>Tom Scott</cite></p>
<p>The main methods for setting and converting between time zones are:</p>
<ul>
<li><code>dt.convert_time_zone</code>: convert from one time zone to another;</li>
<li><code>dt.replace_time_zone</code>: set/unset/change time zone;</li>
</ul>
<p>Let's look at some examples of common operations:</p>
<pre><code class="language-python">ts = [&quot;2021-03-27 03:00&quot;, &quot;2021-03-28 03:00&quot;]
tz_naive = pl.Series(&quot;tz_naive&quot;, ts).str.strptime(pl.Datetime)
tz_aware = tz_naive.dt.replace_time_zone(&quot;UTC&quot;).rename(&quot;tz_aware&quot;)
time_zones_df = pl.DataFrame([tz_naive, tz_aware])
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌─────────────────────┬─────────────────────────┐
│ tz_naive            ┆ tz_aware                │
│ ---                 ┆ ---                     │
│ datetime[μs]        ┆ datetime[μs, UTC]       │
╞═════════════════════╪═════════════════════════╡
│ 2021-03-27 03:00:00 ┆ 2021-03-27 03:00:00 UTC │
│ 2021-03-28 03:00:00 ┆ 2021-03-28 03:00:00 UTC │
└─────────────────────┴─────────────────────────┘
</code></pre>
<pre><code class="language-python">time_zones_operations = time_zones_df.select(
    [
        pl.col(&quot;tz_aware&quot;).dt.replace_time_zone(&quot;Europe/Brussels&quot;).alias(&quot;replace time zone&quot;),
        pl.col(&quot;tz_aware&quot;).dt.convert_time_zone(&quot;Asia/Kathmandu&quot;).alias(&quot;convert time zone&quot;),
        pl.col(&quot;tz_aware&quot;).dt.replace_time_zone(None).alias(&quot;unset time zone&quot;),
    ]
)
</code></pre>
<pre><code class="language-text">shape: (2, 3)
┌───────────────────────────────┬──────────────────────────────┬─────────────────────┐
│ replace time zone             ┆ convert time zone            ┆ unset time zone     │
│ ---                           ┆ ---                          ┆ ---                 │
│ datetime[μs, Europe/Brussels] ┆ datetime[μs, Asia/Kathmandu] ┆ datetime[μs]        │
╞═══════════════════════════════╪══════════════════════════════╪═════════════════════╡
│ 2021-03-27 03:00:00 CET       ┆ 2021-03-27 08:45:00 +0545    ┆ 2021-03-27 03:00:00 │
│ 2021-03-28 03:00:00 CEST      ┆ 2021-03-28 08:45:00 +0545    ┆ 2021-03-28 03:00:00 │
└───────────────────────────────┴──────────────────────────────┴─────────────────────┘
</code></pre>
<p>See the <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">list of tz database time zones</a>
for a list of what's available.</p>
<h1 id="combining-data-with-concat-and-join"><a class="header" href="#combining-data-with-concat-and-join">Combining data with <code>concat</code> and <code>join</code></a></h1>
<p>You can combine data from different <code>DataFrames</code> using:</p>
<ul>
<li><a href="howcani/combining_data/concatenating.html">the <code>concat</code> function</a> or</li>
<li><a href="howcani/combining_data/joining.html">the <code>join</code> method</a> on a <code>DataFrame</code></li>
</ul>
<h1 id="concatenation"><a class="header" href="#concatenation">Concatenation</a></h1>
<p>There are a number of ways to concatenate data from separate DataFrames:</p>
<ul>
<li>two dataframes with <strong>the same columns</strong> can be <strong>vertically</strong> concatenated to make a <strong>longer</strong> dataframe</li>
<li>two dataframes with the <strong>same number of rows</strong> and <strong>non-overlapping columns</strong> can be <strong>horizontally</strong> concatenated to make a <strong>wider</strong> dataframe</li>
<li>two dataframes with <strong>different numbers of rows and columns</strong> can be <strong>diagonally</strong> concatenated to make a dataframe which might be longer and/ or wider. Where column names overlap values will be vertically concatenated. Where column names do not overlap new rows and columns will be added. Missing values will be set as <code>null</code></li>
</ul>
<h2 id="vertical-concatenation---getting-longer"><a class="header" href="#vertical-concatenation---getting-longer">Vertical concatenation - getting longer</a></h2>
<p>In a vertical concatenation you combine all of the rows from a list of <code>DataFrames</code> into a single longer <code>DataFrame</code>.</p>
<pre><code class="language-python">df_v1 = pl.DataFrame(
    {
        &quot;a&quot;: [1],
        &quot;b&quot;: [3],
    }
)
df_v2 = pl.DataFrame(
    {
        &quot;a&quot;: [2],
        &quot;b&quot;: [4],
    }
)
df_vertical_concat = pl.concat(
    [
        df_v1,
        df_v2,
    ],
    how=&quot;vertical&quot;,
)
print(df_vertical_concat)
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 1   ┆ 3   │
│ 2   ┆ 4   │
└─────┴─────┘
</code></pre>
<p>Vertical concatenation fails when the dataframes do not have the same column names.</p>
<h2 id="horizontal-concatenation---getting-wider"><a class="header" href="#horizontal-concatenation---getting-wider">Horizontal concatenation - getting wider</a></h2>
<p>In a horizontal concatenation you combine all of the columns from a list of <code>DataFrames</code> into a single wider <code>DataFrame</code>.</p>
<pre><code class="language-python">df_h1 = pl.DataFrame(
    {
        &quot;l1&quot;: [1, 2],
        &quot;l2&quot;: [3, 4],
    }
)
df_h2 = pl.DataFrame(
    {
        &quot;r1&quot;: [5, 6],
        &quot;r2&quot;: [7, 8],
        &quot;r3&quot;: [9, 10],
    }
)
df_horizontal_concat = pl.concat(
    [
        df_h1,
        df_h2,
    ],
    how=&quot;horizontal&quot;,
print(df_horizontal_concat)
</code></pre>
<pre><code class="language-text">shape: (2, 5)
┌─────┬─────┬─────┬─────┬─────┐
│ l1  ┆ l2  ┆ r1  ┆ r2  ┆ r3  │
│ --- ┆ --- ┆ --- ┆ --- ┆ --- │
│ i64 ┆ i64 ┆ i64 ┆ i64 ┆ i64 │
╞═════╪═════╪═════╪═════╪═════╡
│ 1   ┆ 3   ┆ 5   ┆ 7   ┆ 9   │
│ 2   ┆ 4   ┆ 6   ┆ 8   ┆ 10  │
└─────┴─────┴─────┴─────┴─────┘
</code></pre>
<p>Horizontal concatenation fails when dataframes have overlapping columns or a different number of rows.</p>
<h2 id="diagonal-concatenation---getting-longer-wider-and-nullier"><a class="header" href="#diagonal-concatenation---getting-longer-wider-and-nullier">Diagonal concatenation - getting longer, wider and <code>null</code>ier</a></h2>
<p>In a diagonal concatenation you combine all of the row and columns from a list of <code>DataFrames</code> into a single longer and/or wider <code>DataFrame</code>.</p>
<pre><code class="language-python">df_d1 = pl.DataFrame(
    {
        &quot;a&quot;: [1],
        &quot;b&quot;: [3],
    }
)
df_d2 = pl.DataFrame(
    {
        &quot;a&quot;: [2],
        &quot;d&quot;: [4],
    }
)

df_diagonal_concat = pl.concat(
    [
        df_d1,
        df_d2,
    ],
    how=&quot;diagonal&quot;,
)
print(df_diagonal_concat)
</code></pre>
<pre><code class="language-text">shape: (2, 3)
┌─────┬──────┬──────┐
│ a   ┆ b    ┆ d    │
│ --- ┆ ---  ┆ ---  │
│ i64 ┆ i64  ┆ i64  │
╞═════╪══════╪══════╡
│ 1   ┆ 3    ┆ null │
│ 2   ┆ null ┆ 4    │
└─────┴──────┴──────┘
</code></pre>
<p>Diagonal concatenation generates nulls when the column names do not overlap.</p>
<p>When the dataframe shapes do not match and we have an overlapping semantic key then <a href="howcani/combining_data/joining.html">we can join the dataframes</a> instead of concatenating them.</p>
<h2 id="rechunking"><a class="header" href="#rechunking">Rechunking</a></h2>
<p>Before a concatenation we have two dataframes <code>df1</code> and <code>df2</code>. Each column in <code>df1</code> and <code>df2</code> is in one or more chunks in memory. By default, during concatenation the chunks in each column are copied to a single new chunk - this is known as <strong>rechunking</strong>. Rechunking is an expensive operation, but is often worth it because future operations will be faster.
If you do not want Polars to rechunk the concatenated <code>DataFrame</code> you specify <code>rechunk = False</code> when doing the concatenation.</p>
<h1 id="joins"><a class="header" href="#joins">Joins</a></h1>
<h2 id="join-strategies"><a class="header" href="#join-strategies">Join strategies</a></h2>
<p><code>Polars</code> supports the following join strategies by specifying the <code>strategy</code> argument:</p>
<ul>
<li><code>inner</code></li>
<li><code>left</code></li>
<li><code>outer</code></li>
<li><code>cross</code></li>
<li><code>asof</code></li>
<li><code>semi</code></li>
<li><code>anti</code></li>
</ul>
<h3 id="inner-join"><a class="header" href="#inner-join">Inner join</a></h3>
<p>An <code>inner</code> join produces a <code>DataFrame</code> that contains only the rows where the join key exists in both <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>:</p>
<pre><code class="language-python">df_customers = pl.DataFrame(
    {
        &quot;customer_id&quot;: [1, 2, 3],
        &quot;name&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],
    }
)
print(df_customers)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌─────────────┬─────────┐
│ customer_id ┆ name    │
│ ---         ┆ ---     │
│ i64         ┆ str     │
╞═════════════╪═════════╡
│ 1           ┆ Alice   │
│ 2           ┆ Bob     │
│ 3           ┆ Charlie │
└─────────────┴─────────┘
</code></pre>
<pre><code class="language-python">df_orders = pl.DataFrame(
    {
        &quot;order_id&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
        &quot;customer_id&quot;: [1, 2, 2],
        &quot;amount&quot;: [100, 200, 300],
    }
)
print(df_orders)
</code></pre>
<pre><code class="language-text">shape: (3, 3)
┌──────────┬─────────────┬────────┐
│ order_id ┆ customer_id ┆ amount │
│ ---      ┆ ---         ┆ ---    │
│ str      ┆ i64         ┆ i64    │
╞══════════╪═════════════╪════════╡
│ a        ┆ 1           ┆ 100    │
│ b        ┆ 2           ┆ 200    │
│ c        ┆ 2           ┆ 300    │
└──────────┴─────────────┴────────┘
</code></pre>
<p>To get a <code>DataFrame</code> with the orders and their associated customer we can do an <code>inner</code> join on the <code>customer_id</code> column:</p>
<pre><code class="language-python">df_inner_customer_join = df_customers.join(df_orders, on=&quot;customer_id&quot;, how=&quot;inner&quot;)
print(df_inner_join)
</code></pre>
<pre><code class="language-text">shape: (3, 4)
┌─────────────┬───────┬──────────┬────────┐
│ customer_id ┆ name  ┆ order_id ┆ amount │
│ ---         ┆ ---   ┆ ---      ┆ ---    │
│ i64         ┆ str   ┆ str      ┆ i64    │
╞═════════════╪═══════╪══════════╪════════╡
│ 1           ┆ Alice ┆ a        ┆ 100    │
│ 2           ┆ Bob   ┆ b        ┆ 200    │
│ 2           ┆ Bob   ┆ c        ┆ 300    │
└─────────────┴───────┴──────────┴────────┘
</code></pre>
<h3 id="left-join"><a class="header" href="#left-join">Left join</a></h3>
<p>The <code>left</code> join produces a <code>DataFrame</code> that contains all the rows from the left <code>DataFrame</code> and only the rows from the right <code>DataFrame</code> where the join key exists in the left <code>DataFrame</code>. If we now take the example from above and want to have a <code>DataFrame</code> with all the customers and their associated orders (regardless of whether they have placed an order or not) we can do a <code>left</code> join:</p>
<pre><code class="language-python">df_left_join = df_customers.join(df_orders, on=&quot;customer_id&quot;, how=&quot;left&quot;)
print(df_left_join)
</code></pre>
<pre><code class="language-text">shape: (4, 4)
┌─────────────┬─────────┬──────────┬────────┐
│ customer_id ┆ name    ┆ order_id ┆ amount │
│ ---         ┆ ---     ┆ ---      ┆ ---    │
│ i64         ┆ str     ┆ str      ┆ i64    │
╞═════════════╪═════════╪══════════╪════════╡
│ 1           ┆ Alice   ┆ a        ┆ 100    │
│ 2           ┆ Bob     ┆ b        ┆ 200    │
│ 2           ┆ Bob     ┆ c        ┆ 300    │
│ 3           ┆ Charlie ┆ null     ┆ null   │
└─────────────┴─────────┴──────────┴────────┘
</code></pre>
<p>Notice, that the fields for the customer with the <code>customer_id</code> of <code>3</code> are null, as there are no orders for this customer.</p>
<h3 id="outer-join"><a class="header" href="#outer-join">Outer join</a></h3>
<p>The <code>outer</code> join produces a <code>DataFrame</code> that contains all the rows from both <code>DataFrames</code>. Columns are null, if the join key does not exist in the source <code>DataFrame</code>. Doing an <code>outer</code> join on the two <code>DataFrames</code> from above produces a similar <code>DataFrame</code> to the <code>left</code> join:</p>
<pre><code class="language-python">df_outer_join = df_customers.join(df_orders, on=&quot;customer_id&quot;, how=&quot;outer&quot;)
print(df_outer_join)
</code></pre>
<pre><code class="language-text">shape: (4, 4)
┌─────────────┬─────────┬──────────┬────────┐
│ customer_id ┆ name    ┆ order_id ┆ amount │
│ ---         ┆ ---     ┆ ---      ┆ ---    │
│ i64         ┆ str     ┆ str      ┆ i64    │
╞═════════════╪═════════╪══════════╪════════╡
│ 1           ┆ Alice   ┆ a        ┆ 100    │
│ 2           ┆ Bob     ┆ b        ┆ 200    │
│ 2           ┆ Bob     ┆ c        ┆ 300    │
│ 3           ┆ Charlie ┆ null     ┆ null   │
└─────────────┴─────────┴──────────┴────────┘
</code></pre>
<h3 id="cross-join"><a class="header" href="#cross-join">Cross join</a></h3>
<p>A <code>cross</code> join is a cartesian product of the two <code>DataFrames</code>. This means that every row in the left <code>DataFrame</code> is joined with every row in the right <code>DataFrame</code>. The <code>cross</code> join is useful for creating a <code>DataFrame</code> with all possible combinations of the columns in two <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>:</p>
<pre><code class="language-python">df_colors = pl.DataFrame(
    {
        &quot;color&quot;: [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;],
    }
)
print(df_colors)
</code></pre>
<pre><code class="language-text">shape: (3, 1)
┌───────┐
│ color │
│ ---   │
│ str   │
╞═══════╡
│ red   │
│ blue  │
│ green │
└───────┘
</code></pre>
<pre><code class="language-python">df_sizes = pl.DataFrame(
    {
        &quot;size&quot;: [&quot;S&quot;, &quot;M&quot;, &quot;L&quot;],
    }
)
print(df_sizes)
</code></pre>
<pre><code class="language-text">shape: (3, 1)
┌──────┐
│ size │
│ ---  │
│ str  │
╞══════╡
│ S    │
│ M    │
│ L    │
└──────┘
</code></pre>
<p>We can now create a <code>DataFrame</code> containing all possible combinations of the colors and sizes with a <code>cross</code> join:</p>
<pre><code class="language-python">df_cross_join = df_colors.join(df_sizes, how=&quot;cross&quot;)
print(df_cross_join)
</code></pre>
<pre><code class="language-text">shape: (9, 2)
┌───────┬──────┐
│ color ┆ size │
│ ---   ┆ ---  │
│ str   ┆ str  │
╞═══════╪══════╡
│ red   ┆ S    │
│ red   ┆ M    │
│ red   ┆ L    │
│ blue  ┆ S    │
│ …     ┆ …    │
│ blue  ┆ L    │
│ green ┆ S    │
│ green ┆ M    │
│ green ┆ L    │
└───────┴──────┘
</code></pre>
<br>
<p>The <code>inner</code>, <code>left</code>, <code>outer</code> and <code>cross</code> join strategies are standard amongst dataframe libraries. We provide more details on the less familiar <code>semi</code>, <code>anti</code> and <code>asof</code> join strategies below.</p>
<h3 id="semi-join"><a class="header" href="#semi-join">Semi join</a></h3>
<p>Consider the following scenario: a car rental company has a <code>DataFrame</code> showing the cars that it owns with each car having a unique <code>id</code>.</p>
<pre><code class="language-python">df_cars = pl.DataFrame(
    {
        &quot;id&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
        &quot;make&quot;: [&quot;ford&quot;, &quot;toyota&quot;, &quot;bmw&quot;],
    }
)
print(df_cars)
</code></pre>
<pre><code class="language-text">shape: (3, 2)
┌─────┬────────┐
│ id  ┆ make   │
│ --- ┆ ---    │
│ str ┆ str    │
╞═════╪════════╡
│ a   ┆ ford   │
│ b   ┆ toyota │
│ c   ┆ bmw    │
└─────┴────────┘
</code></pre>
<p>The company has another <code>DataFrame</code> showing each repair job carried out on a vehicle.</p>
<pre><code class="language-python">df_repairs = pl.DataFrame(
    {
        &quot;id&quot;: [&quot;c&quot;, &quot;c&quot;],
        &quot;cost&quot;: [100, 200],
    }
)
print(df_repairs)
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌─────┬──────┐
│ id  ┆ cost │
│ --- ┆ ---  │
│ str ┆ i64  │
╞═════╪══════╡
│ c   ┆ 100  │
│ c   ┆ 200  │
└─────┴──────┘
</code></pre>
<p>You want to answer this question: which of the cars have had repairs carried out?</p>
<p>An inner join does not answer this question directly as it produces a <code>DataFrame</code> with multiple rows for each car that has had multiple repair jobs:</p>
<pre><code class="language-python">df_inner_join = df_cars.join(df_repairs, on=&quot;id&quot;, how=&quot;inner&quot;)
print(df_inner_join)
</code></pre>
<pre><code class="language-text">shape: (2, 3)
┌─────┬──────┬──────┐
│ id  ┆ make ┆ cost │
│ --- ┆ ---  ┆ ---  │
│ str ┆ str  ┆ i64  │
╞═════╪══════╪══════╡
│ c   ┆ bmw  ┆ 100  │
│ c   ┆ bmw  ┆ 200  │
└─────┴──────┴──────┘
</code></pre>
<p>However, a semi join produces a single row for each car that has had a repair job carried out.</p>
<pre><code class="language-python">df_semi_join = df_cars.join(df_repairs, on=&quot;id&quot;, how=&quot;semi&quot;)
print(df_semi_join)
</code></pre>
<pre><code class="language-text">shape: (1, 2)
┌─────┬──────┐
│ id  ┆ make │
│ --- ┆ ---  │
│ str ┆ str  │
╞═════╪══════╡
│ c   ┆ bmw  │
└─────┴──────┘
</code></pre>
<h3 id="anti-join"><a class="header" href="#anti-join">Anti join</a></h3>
<p>Continuing this example, an alternative question might be: which of the cars have <strong>not</strong> had a repair job carried out? An anti join produces a <code>DataFrame</code> showing all the cars from <code>df_cars</code> where the <code>id</code> is not present in the <code>df_repairs</code> <code>DataFrame</code>.</p>
<pre><code class="language-python">df_anti_join = df_cars.join(df_repairs, on=&quot;id&quot;, how=&quot;anti&quot;)
print(df_anti_join)
</code></pre>
<pre><code class="language-text">shape: (2, 2)
┌─────┬────────┐
│ id  ┆ make   │
│ --- ┆ ---    │
│ str ┆ str    │
╞═════╪════════╡
│ a   ┆ ford   │
│ b   ┆ toyota │
└─────┴────────┘
</code></pre>
<h3 id="asof-join"><a class="header" href="#asof-join">Asof join</a></h3>
<p>An <code>asof</code> join is like a left join except that we match on nearest key rather than equal keys.
In <code>Polars</code> we can do an asof join with the <code>join</code> method and specifying <code>strategy=&quot;asof&quot;</code>. However, for more flexibility we can use the <code>join_asof</code> method.</p>
<p>Consider the following scenario: a stock market broker has a <code>DataFrame</code> called <code>df_trades</code> showing transactions it has made for different stocks.</p>
<pre><code class="language-python">df_trades = pl.DataFrame(
    {
        &quot;time&quot;: [
            datetime(2020, 1, 1, 9, 1, 0),
            datetime(2020, 1, 1, 9, 1, 0),
            datetime(2020, 1, 1, 9, 3, 0),
            datetime(2020, 1, 1, 9, 6, 0),
        ],
        &quot;stock&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;],
        &quot;trade&quot;: [101, 299, 301, 500],
    }
)
print(df_trades)
</code></pre>
<pre><code class="language-text">shape: (4, 3)
┌─────────────────────┬───────┬───────┐
│ time                ┆ stock ┆ trade │
│ ---                 ┆ ---   ┆ ---   │
│ datetime[μs]        ┆ str   ┆ i64   │
╞═════════════════════╪═══════╪═══════╡
│ 2020-01-01 09:01:00 ┆ A     ┆ 101   │
│ 2020-01-01 09:01:00 ┆ B     ┆ 299   │
│ 2020-01-01 09:03:00 ┆ B     ┆ 301   │
│ 2020-01-01 09:06:00 ┆ C     ┆ 500   │
└─────────────────────┴───────┴───────┘
</code></pre>
<p>The broker has another <code>DataFrame</code> called <code>df_quotes</code> showing prices it has quoted for these stocks.</p>
<pre><code class="language-python">df_quotes = pl.DataFrame(
    {
        &quot;time&quot;: [
            datetime(2020, 1, 1, 9, 0, 0),
            datetime(2020, 1, 1, 9, 2, 0),
            datetime(2020, 1, 1, 9, 4, 0),
            datetime(2020, 1, 1, 9, 6, 0),
        ],
        &quot;stock&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;],
        &quot;quote&quot;: [100, 300, 501, 102],
    }
)
print(df_quotes)
</code></pre>
<pre><code class="language-text">shape: (4, 3)
┌─────────────────────┬───────┬───────┐
│ time                ┆ stock ┆ quote │
│ ---                 ┆ ---   ┆ ---   │
│ datetime[μs]        ┆ str   ┆ i64   │
╞═════════════════════╪═══════╪═══════╡
│ 2020-01-01 09:00:00 ┆ A     ┆ 100   │
│ 2020-01-01 09:02:00 ┆ B     ┆ 300   │
│ 2020-01-01 09:04:00 ┆ C     ┆ 501   │
│ 2020-01-01 09:06:00 ┆ A     ┆ 102   │
└─────────────────────┴───────┴───────┘
</code></pre>
<p>You want to produce a <code>DataFrame</code> showing for each trade the most recent quote provided <em>before</em> the trade. You do this with <code>join_asof</code> (using the default <code>strategy = &quot;backward&quot;</code>).
To avoid joining between trades on one stock with a quote on another you must specify an exact preliminary join on the stock column with <code>by=&quot;stock&quot;</code>.</p>
<pre><code class="language-python">df_asof_join = df_trades.join_asof(df_quotes, on=&quot;time&quot;, by=&quot;stock&quot;)
print(df_asof_join)
</code></pre>
<pre><code class="language-text">shape: (4, 4)
┌─────────────────────┬───────┬───────┬───────┐
│ time                ┆ stock ┆ trade ┆ quote │
│ ---                 ┆ ---   ┆ ---   ┆ ---   │
│ datetime[μs]        ┆ str   ┆ i64   ┆ i64   │
╞═════════════════════╪═══════╪═══════╪═══════╡
│ 2020-01-01 09:01:00 ┆ A     ┆ 101   ┆ 100   │
│ 2020-01-01 09:01:00 ┆ B     ┆ 299   ┆ null  │
│ 2020-01-01 09:03:00 ┆ B     ┆ 301   ┆ 300   │
│ 2020-01-01 09:06:00 ┆ C     ┆ 500   ┆ 501   │
└─────────────────────┴───────┴───────┴───────┘
</code></pre>
<p>If you want to make sure that only quotes within a certain time range are joined to the trades you can specify the <code>tolerance</code> argument. In this case we want to make sure that the last preceding quote is within 1 minute of the trade so we set <code>tolerance = &quot;1m&quot;</code>.</p>
<pre><code class="language-python">
print(df_asof_tolerance_join)
</code></pre>
<pre><code class="language-text">shape: (4, 4)
┌─────────────────────┬───────┬───────┬───────┐
│ time                ┆ stock ┆ trade ┆ quote │
│ ---                 ┆ ---   ┆ ---   ┆ ---   │
│ datetime[μs]        ┆ str   ┆ i64   ┆ i64   │
╞═════════════════════╪═══════╪═══════╪═══════╡
│ 2020-01-01 09:01:00 ┆ A     ┆ 101   ┆ 100   │
│ 2020-01-01 09:01:00 ┆ B     ┆ 299   ┆ null  │
│ 2020-01-01 09:03:00 ┆ B     ┆ 301   ┆ 300   │
│ 2020-01-01 09:06:00 ┆ C     ┆ 500   ┆ null  │
└─────────────────────┴───────┴───────┴───────┘
</code></pre>
<h1 id="combining-polars-with-pythons-multiprocessing"><a class="header" href="#combining-polars-with-pythons-multiprocessing">Combining Polars with Python's multiprocessing</a></h1>
<p>TLDR: if you find that using Python's built-in <code>multiprocessing</code> module together with Polars results in a Polars error about multiprocessing methods, you should make sure you are using <code>spawn</code>, not <code>fork</code>, as the starting method:</p>
<pre><code class="language-python">from multiprocessing import get_context


def my_fun(s):
    print(s)


with get_context(&quot;spawn&quot;).Pool() as pool:
    pool.map(my_fun, [&quot;input1&quot;, &quot;input2&quot;, ...])
</code></pre>
<h2 id="when-not-to-use-multiprocessing"><a class="header" href="#when-not-to-use-multiprocessing">When not to use multiprocessing</a></h2>
<p>Before we dive into the details, it is important to emphasize that Polars has been built from the start to use all your CPU cores.
It does this by executing computations which can be done in parallel in separate threads.
For example, requesting two expressions in a <code>select</code> statement can be done in parallel, with the results only being combined at the end.
Another example is aggregating a value within groups using <code>groupby().agg(&lt;expr&gt;)</code>, each group can be evaluated separately.
It is very unlikely that the <code>multiprocessing</code> module can improve your code performance in these cases.</p>
<p>See <a href="howcani/optimizations/lazy/intro.html">the optimizations section</a> for more optimizations.</p>
<h2 id="when-to-use-multiprocessing"><a class="header" href="#when-to-use-multiprocessing">When to use multiprocessing</a></h2>
<p>Although Polars is multithreaded, other libraries may be single-threaded.
When the other library is the bottleneck, and the problem at hand is parallelizable, it makes sense to use multiprocessing to speed up.</p>
<h2 id="the-problem-with-the-default-multiprocessing-config"><a class="header" href="#the-problem-with-the-default-multiprocessing-config">The problem with the default multiprocessing config</a></h2>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<p>The <a href="https://docs.python.org/3/library/multiprocessing.html">Python multiprocessing documentation</a> lists the three methods a process pool can be created:</p>
<ol>
<li>spawn</li>
<li>fork</li>
<li>forkserver</li>
</ol>
<p>The description of fork is (as of 2022-10-15):</p>
<blockquote>
<p>The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the  parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.</p>
</blockquote>
<blockquote>
<p>Available on Unix only. The default on Unix.</p>
</blockquote>
<p>The short summary is: Polars is multithreaded as to provide strong performance out-of-the-box.
Thus, it cannot be combined with <code>fork</code>.
If you are on Unix (Linux, BSD, etc), you are using <code>fork</code>, unless you explicitly override it.</p>
<p>The reason you may not have encountered this before is that pure Python code, and most Python libraries, are (mostly) single threaded.
Alternatively, you are on Windows or MacOS, on which <code>fork</code> is not even available as a method (for MacOS it was up to Python 3.7).</p>
<p>Thus one should use <code>spawn</code>, or <code>forkserver</code>, instead. <code>spawn</code> is available on all platforms and the safest choice, and hence the recommended method.</p>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<p>The problem with <code>fork</code> is in the copying of the parent's process.
Consider the example below, which is a slightly modified example posted on the <a href="https://github.com/pola-rs/polars/issues/3144">Polars issue tracker</a>:</p>
<pre><code class="language-python">import multiprocessing
import polars as pl


def test_sub_process(df: pl.DataFrame, job_id):
    df_filtered = df.filter(pl.col(&quot;a&quot;) &gt; 0)
    print(f&quot;Filtered (job_id: {job_id})&quot;, df_filtered, sep=&quot;\n&quot;)


def create_dataset():
    return pl.DataFrame({&quot;a&quot;: [0, 2, 3, 4, 5], &quot;b&quot;: [0, 4, 5, 56, 4]})


def setup():
    # some setup work
    df = create_dataset()
    df.write_parquet(&quot;/tmp/test.parquet&quot;)


def main():
    test_df = pl.read_parquet(&quot;/tmp/test.parquet&quot;)

    for i in range(0, 5):
        proc = multiprocessing.get_context(&quot;spawn&quot;).Process(target=test_sub_process, args=(test_df, i))
        proc.start()
        proc.join()

        print(f&quot;Executed sub process {i}&quot;)


if __name__ == &quot;__main__&quot;:
    setup()
    main()
</code></pre>
<p>Using <code>fork</code> as the method, instead of <code>spawn</code>, will cause a dead lock.
Please note: Polars will not even start and raise the error on multiprocessing method being set wrong, but if the check would not be there, the deadlock would exist.</p>
<p>The fork method is equivalent to calling <code>os.fork()</code>, which is a system call as defined in <a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html">the POSIX standard</a>:</p>
<blockquote>
<p>A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called.</p>
</blockquote>
<p>In contrast, <code>spawn</code> will create a completely new fresh Python interpreter, and not inherit the state of mutexes.</p>
<p>So what happens in the code example?
For reading the file with <code>pl.read_parquet</code> the file has to be locked.
Then <code>os.fork()</code> is called, copying the state of the parent process, including mutexes.
Thus all child processes will copy the file lock in an acquired state, leaving them hanging indefinitely waiting for the file lock to be released, which never happens.</p>
<p>What makes debugging these issues tricky is that <code>fork</code> can work.
Change the example to not having the call to <code>pl.read_parquet</code>:</p>
<pre><code class="language-python">from .example1 import create_dataset, test_sub_process
import multiprocessing


def main():
    test_df = create_dataset()

    for i in range(0, 5):
        proc = multiprocessing.get_context(&quot;fork&quot;).Process(target=test_sub_process, args=(test_df, i))
        proc.start()
        proc.join()

        print(f&quot;Executed sub process {i}&quot;)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>This works fine.
Therefore debugging these issues in larger code bases, i.e. not the small toy examples here, can be a real pain, as a seemingly unrelated change can break your multiprocessing code.
In general, one should therefore never use the <code>fork</code> start method with multithreaded libraries unless there are very specific requirements that cannot be met otherwise.</p>
<h3 id="pros-and-cons-of-fork"><a class="header" href="#pros-and-cons-of-fork">Pro's and cons of fork</a></h3>
<p>Based on the example, you may think, why is <code>fork</code> available in Python to start with?</p>
<p>First, probably because of historical reasons: <code>spawn</code> was added to Python in version 3.4, whilst <code>fork</code> has been part of Python from the 2.x series.</p>
<p>Second, there are several limitations for <code>spawn</code> and <code>forkserver</code> that do not apply to <code>fork</code>, in particular all arguments should be pickable.
See the <a href="https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods">Python multiprocessing docs</a> for more information.</p>
<p>Third, because it is faster to create new processes compared to <code>spawn</code>, as <code>spawn</code> is effectively <code>fork</code> + creating a brand new Python process without the locks by calling <a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/exec.html">execv</a>.
Hence the warning in the Python docs that it is slower: there is more overhead to <code>spawn</code>.
However, in almost all cases, one would like to use multiple processes to speed up computations that take multiple minutes or even hours, meaning the overhead is negligible in the grand scheme of things.
And more importantly, it actually works in combination with multithreaded libraries.</p>
<p>Fourth, <code>spawn</code> starts a new process, and therefore it requires code to be importable, in contrast to <code>fork</code>.
In particular, this means that when using <code>spawn</code> the relevant code should not be in the global scope, such as in Jupyter notebooks or in plain scripts.
Hence in the examples above, we define functions where we spawn within, and run those functions from a <code>__main__</code> clause.
This is not an issue for typical projects, but in quick experimentation in notebooks it could fail.</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<p>[1] https://docs.python.org/3/library/multiprocessing.html</p>
<p>[2] https://pythonspeed.com/articles/python-multiprocessing/</p>
<p>[3] https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html</p>
<p>[4] https://bnikolic.co.uk/blog/python/parallelism/2019/11/13/python-forkserver-preload.html</p>
<h1 id="performance"><a class="header" href="#performance">Performance</a></h1>
<p>This chapter handles some gotcha's needed to squeeze maximum performance out of <code>Polars</code>.
When used properly, <code>Polars</code> can run at blazing speeds. Take a look at the results in
<a href="https://h2oai.github.io/db-benchmark/">H2O AI database benchmark</a>.</p>
<h1 id="strings"><a class="header" href="#strings">Strings</a></h1>
<p>Understanding the memory format used by <code>Arrow</code> and <code>Polars</code> can really increase performance
of your queries. This is especially true for large string data. The figure below shows
how an <code>Arrow</code> <code>UTF8</code> array is laid out in memory.</p>
<p>The array <code>[&quot;foo&quot;, &quot;bar&quot;, &quot;ham&quot;]</code> is encoded by :</p>
<ul>
<li>a concatenated string <code>&quot;foobarham&quot;</code>,</li>
<li>an offset array indicating the start (and end) of each string <code>[0, 2, 5, 8]</code>,</li>
<li>a null bitmap, indicating null values.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/docs/arrow-string.svg" alt="" /></p>
<p>This memory structure is very cache-efficient if we are to read the string values.
Especially if we compare it to a <code>Vec&lt;String&gt;</code> (an array of heap allocated string data
in <code>Rust</code>).</p>
<p><img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/docs/pandas-string.svg" alt="" /></p>
<p>However, if we need to reorder the <code>Arrow</code> <code>UTF8</code> array, we need to swap around all the
bytes of the string values, which can become very expensive when dealing with large
strings. On the other hand for <code>Vec&lt;String&gt;</code>, we only need to swap pointers around,
which is only 8 bytes data that have to be moved with little cost.</p>
<p>Reordering a <code>DataFrame</code> embedding a large number of <code>Utf8</code> <code>Series</code> due to an operation
(filtering, joining, grouping, <em>etc.</em>) can quickly become quite expensive.</p>
<h2 id="categorical-type"><a class="header" href="#categorical-type">Categorical type</a></h2>
<p>For this reason <code>Polars</code> has a <code>CategoricalType</code>. A <code>Categorical</code> <code>Series</code> is an array
filled with <code>u32</code> values that each represent a unique string value. Thereby maintaining
cache efficiency whilst remaining cheap to move values around.</p>
<p>In the example below we demonstrate how you can cast a <code>Utf8</code> <code>Series</code> column to a
<code>Categorical</code> <code>Series</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

df[&quot;utf8-column&quot;].cast(pl.Categorical)
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

df.column(&quot;utf8-column&quot;).unwrap().cast(&amp;DataType::Categorical(None)).unwrap();
</code></pre>
</div>
<h3 id="join-multiple-dataframes-on-categorical-data"><a class="header" href="#join-multiple-dataframes-on-categorical-data">Join multiple DataFrames on Categorical data</a></h3>
<p>When two <code>DataFrames</code> need to be joined based on string data the <code>Categorical</code> data needs
to be synchronized (data in column <code>A</code> of <code>df1</code> needs to point to the same underlying
string data as column <code>B</code> in <code>df2</code>). One can do so by casting data in the <code>StringCache</code>
context manager. This will synchronize all discoverable string values for the duration of that
context manager. If you want the global string cache to exist during the whole
run, you can set <code>toggle_string_cache</code> to <code>True</code>.</p>
<div class="tabbed-blocks">
<pre><code class="language-python">import polars as pl

df1 = pl.DataFrame({&quot;a&quot;: [&quot;foo&quot;, &quot;bar&quot;, &quot;ham&quot;], &quot;b&quot;: [1, 2, 3]})
df2 = pl.DataFrame({&quot;a&quot;: [&quot;foo&quot;, &quot;spam&quot;, &quot;eggs&quot;], &quot;c&quot;: [3, 2, 2]})

with pl.StringCache():
    df1.with_columns(pl.col(&quot;a&quot;).cast(pl.Categorical))
    df2.with_columns(pl.col(&quot;a&quot;).cast(pl.Categorical))
</code></pre>
<pre><code class="language-rust noplayground">use polars::prelude::*;

fn main() {
    let words = &quot;All that glitters is not gold&quot;.split(' ').collect::&lt;Vec&lt;_&gt;&gt;();
    let df = df! (
        &quot;shakespear&quot; =&gt; &amp;words
    ).unwrap();
    println!(&quot;{df}&quot;);
}
</code></pre>
</div>
<h1 id="optimizations"><a class="header" href="#optimizations">Optimizations</a></h1>
<p>If you use <code>Polars</code>' lazy API, <code>Polars</code> will run several optimizations on your query. Some of them are executed up front,
others are determined just in time as the materialized data comes in.</p>
<p>Here is a non-complete overview of optimizations done by polars, what they do and how often they run.</p>
<table><thead><tr><th>Optimization</th><th>Explanation</th><th>runs</th></tr></thead><tbody>
<tr><td>Predicate pushdown</td><td>Applies filters as early as possible/ at scan level.</td><td>1 time</td></tr>
<tr><td>Projection pushdown</td><td>Select only the columns that are needed at the scan level.</td><td>1 time</td></tr>
<tr><td>Slice pushdown</td><td>Only load the required slice from the scan level.  Don't materialize sliced outputs (e.g. join.head(10)).</td><td>1 time</td></tr>
<tr><td>Common subplan elimination</td><td>Cache subtrees/file scans that are used by multiple subtrees in the query plan.</td><td>1 time</td></tr>
<tr><td>Simplify expressions</td><td>Various optimizations, such as constant folding and replacing expensive operations with faster alternatives.</td><td>until fixed point</td></tr>
<tr><td>Join ordering</td><td>Estimates the branches of joins that should be executed first in order to reduce memory pressure.</td><td>1 time</td></tr>
<tr><td>Type coercion</td><td>Coerce types such that operations succeed and run on minimal required memory.</td><td>until fixed point</td></tr>
<tr><td>Cardinality estimation</td><td>Estimates cardinality in order to determine optimal groupby strategy.</td><td>0/n times; dependent on query</td></tr>
</tbody></table>
<h1 id="static-schemas"><a class="header" href="#static-schemas">Static schemas</a></h1>
<p>Do you know that feeling, you are 20 mins in a ETL job and bam, your pipeline fails because we assumed to have another
data type for a column? With polars' lazy API we know the data type and column name for any node in the pipeline.</p>
<p>This is very valuable information and can be used to ensure data integrity at any node in the pipeline.</p>
<pre><code class="language-python">trip_duration = (pl.col(&quot;dropoff_datetime&quot;) - pl.col(&quot;pickup_datetime&quot;)).dt.seconds() / 3600

assert (
    pl.scan_csv(&quot;data/yellow_tripdata_2010-01.csv&quot;, parse_dates=True)
    .with_columns(trip_duration.alias(&quot;trip_duration&quot;))
    .filter(pl.col(&quot;trip_duration&quot;) &gt; 0)
    .groupby([&quot;vendor_id&quot;])
    .agg(
        [
            (pl.col(&quot;trip_distance&quot;) / pl.col(&quot;trip_duration&quot;)).mean().alias(&quot;avg_speed&quot;),
            (pl.col(&quot;tip_amount&quot;) / pl.col(&quot;passenger_count&quot;)).mean().alias(&quot;avg_tip_per_person&quot;),
        ]
    )
).schema == {&quot;vendor_id&quot;: pl.Utf8, &quot;avg_speed&quot;: pl.Float64, &quot;avg_tip_per_person&quot;: pl.Float64}
</code></pre>
<h1 id="reference-guide"><a class="header" href="#reference-guide">Reference guide</a></h1>
<p>Need to see all available methods/functions of <code>Polars</code>? We have <code>Rust</code> and <code>Python</code> references:</p>
<ul>
<li><a href="https://docs.rs/polars"><code>Rust</code> release</a></li>
<li><a href="https://pola-rs.github.io/polars/py-polars/html/reference"><code>Python</code> API</a></li>
</ul>
<h1 id="contributing"><a class="header" href="#contributing">Contributing</a></h1>
<p>See the <a href="https://github.com/pola-rs/polars/blob/master/CONTRIBUTING.md"><code>CONTRIBUTING.md</code></a> if you would like to contribute to the <code>Polars</code> project.</p>
<p>If you're new to this we recommend starting out with contributing examples to the Python API documentation. The Python API docs are generated from the docstrings of the Python wrapper located in <code>polars/py-polars</code>.</p>
<p>Here is an example <a href="https://github.com/pola-rs/polars/pull/3567/commits/5db9e335f3f2777dd1d6f80df765c6bca8f307b0">commit</a> that adds a docstring.</p>
<p>If you spot any gaps in this User Guide you can submit fixes to the <a href="https://github.com/pola-rs/polars-book"><code>pola-rs/polars-book</code></a> repo.</p>
<p>Happy hunting!</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="tabbed-code-blocks.js"></script>
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
